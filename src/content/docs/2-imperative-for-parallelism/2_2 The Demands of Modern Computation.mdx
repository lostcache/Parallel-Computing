---
title: "2.2 Modern Computational Demands"
description: "The computational challenges in science, AI, and finance that necessitate parallel processing."
---
The three walls described in the previous section—Power, Memory, and ILP—created a crisis in computer architecture by ending the era of single-core performance scaling. However, this crisis was compounded by an inconvenient reality: computational demands were not slowing down. On the contrary, emerging applications in scientific simulation, artificial intelligence, and data analytics were becoming exponentially more demanding precisely when the traditional path to performance improvement had reached its limits. The collision between these physical constraints and accelerating computational requirements left the industry with no alternative but to embrace parallelism at every level of the computing stack.

The shift away from single-core scaling has been driven by increasing computational demands in science, commerce, and artificial intelligence. Progress in many fields now requires computational capabilities beyond what single processors can provide. Parallelism has transitioned from a specialized supercomputing technique to a fundamental requirement for computational advancement.
***Scientific and Engineering Simulation***
High-Performance Computing (HPC) has long been a primary driver of parallel architectures, enabling the simulation of complex physical systems that are infeasible to study with direct experiments due to scale, speed, or safety considerations.

* **Climate Modeling:** Global climate models, which form the basis for IPCC reports, divide the Earth into a 3D grid and solve the fundamental equations of fluid dynamics for each cell. The accuracy of these models is directly related to their resolution. However, the computational cost increases substantially; **doubling the spatial resolution (e.g., from 100km to 50km grid cells) requires approximately ten times the computing power**.[^1] A single, comprehensive simulation for an IPCC assessment can take many months to complete, even on large-scale, energy-intensive supercomputers.[^2]
* **Drug Discovery:** Computer-Aided Drug Design (CADD) is now a key component in accelerating the discovery of new medicines.[^3] A key technique is
  **molecular dynamics (MD) simulation**, which models the behavior of a potential drug molecule at the atomic level. This involves calculating the forces between millions of atoms over millions of discrete time steps—an inherently parallel problem well-suited for GPUs, which can perform these calculations orders of magnitude faster than CPUs.[^4]

***Artificial Intelligence and Large Language Models***
Modern AI, particularly the training of Large Language Models (LLMs), requires substantial parallel computation resources. The training process involves processing large text corpora—often trillions of tokens—and iteratively adjusting billions or trillions of parameters.

| Model | Parameters | Training Tokens | Est. Training Compute Cost |
| :---- | :---- | :---- | :---- |
| **GPT-3** | 175 Billion | 300 Billion | \~$4.6 Million |
| **Gopher** | 280 Billion | 300 Billion | N/A |
| **Chinchilla** | 70 Billion | 1.4 Trillion | Same as Gopher |
| **GPT-4** | \>1 Trillion (est.) | \>13 Trillion (est.) | \>$100 Million |
| **Gemini Ultra** | N/A | N/A | \~$191 Million |

(Sources: [^5])
Training at this scale requires data centers equipped with tens of thousands of GPUs operating in parallel for extended periods.[^6] Research on the
**Chinchilla** model has established compute-optimal scaling laws: for a fixed computational budget, model size and the number of training tokens should be scaled equally.[^7] This finding indicates that many previous large models were undertrained relative to the available compute budget, suggesting continued growth in both data requirements and parallel computation demands.
***Big Data and Financial Computing***
Finance and big data analytics require processing large volumes of information with low latency, necessitating parallel processing architectures.

* **High-Frequency Trading (HFT):** HFT operations depend on low-latency execution. Firms employ algorithms to analyze real-time market data and execute thousands of trades per second, capitalizing on short-lived price discrepancies.[^8] These operations utilize parallel infrastructure including multi-core servers, GPUs, and FPGAs, often co-located in exchange data centers to minimize network latency.[^9]
* **Big Data Analytics:** Large-scale data analysis relies on parallel processing frameworks. MapReduce and Apache Spark are designed to partition datasets across server clusters, perform distributed operations, and aggregate results.[^10] This architecture supports search engines, recommendation systems, and fraud detection applications.

Applications ranging from climate modeling to AI training and financial computing share a common computational characteristic: they are fundamentally parallel problems. Physical limitations of single-core processors have redirected computational progress toward parallel architectures.

## References

[^1]: Q&A: How do climate models work? - Carbon Brief, accessed October 1, 2025, [https://www.carbonbrief.org/qa-how-do-climate-models-work/](https://www.carbonbrief.org/qa-how-do-climate-models-work/)
[^2]: The computational and energy cost of simulation and storage for climate science: lessons from CMIP6 - GMD, accessed October 1, 2025, [https://gmd.copernicus.org/articles/17/3081/2024/](https://gmd.copernicus.org/articles/17/3081/2024/)
[^3]: Integrated Molecular Modeling and Machine Learning for Drug ..., accessed October 1, 2025, [https://pubs.acs.org/doi/10.1021/acs.jctc.3c00814](https://pubs.acs.org/doi/10.1021/acs.jctc.3c00814)
[^4]: Best CPU, GPU, RAM for Molecular Dynamics | SabrePC Blog, accessed October 1, 2025, [https://www.sabrepc.com/blog/life-sciences/best-cpu-gpu-and-ram-for-md-workstation-server](https://www.sabrepc.com/blog/life-sciences/best-cpu-gpu-and-ram-for-md-workstation-server)
[^5]: Large Language Model Training - Research AIMultiple, accessed October 1, 2025, [https://research.aimultiple.com/large-language-model-training/](https://research.aimultiple.com/large-language-model-training/)
[^6]: What is the cost of training large language models? - CUDO Compute, accessed October 1, 2025, [https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models](https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models)
[^7]: Training Compute-Optimal Large Language Models, accessed October 1, 2025, [https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf)
[^8]: Understanding High-Frequency Trading (HFT): Basics, Mechanics, and Example, accessed October 1, 2025, [https://www.investopedia.com/terms/h/high-frequency-trading.asp](https://www.investopedia.com/terms/h/high-frequency-trading.asp)
[^9]: Parallel Computing in High Frequency Trading | PDF - Scribd, accessed October 1, 2025, [https://www.scribd.com/presentation/831078975/Parallel-Computing-in-High-Frequency-Trading](https://www.scribd.com/presentation/831078975/Parallel-Computing-in-High-Frequency-Trading)
[^10]: Parallel Computing in R for Big Data | Advanced R Programming Class Notes - Fiveable, accessed October 1, 2025, [https://fiveable.me/introduction-to-advanced-programming-in-r/unit-11](https://fiveable.me/introduction-to-advanced-programming-in-r/unit-11)