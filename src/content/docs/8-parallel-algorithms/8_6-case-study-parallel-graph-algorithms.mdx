---
title: "8.6 Case Study: Parallel Graph Algorithms"
description: "Analyzing parallel approaches to fundamental graph problems and their computational complexity."
---

Graph algorithms present a formidable challenge for parallel computing. Unlike the dense, regular data structures of matrices and arrays, graphs are fundamentally irregular, unstructured, and sparse. The performance of algorithms on these structures is often dictated not by the speed of arithmetic operations, but by the latency of memory access and network communication. This case study explores the unique difficulties of parallelizing graph algorithms and examines parallel approaches for three fundamental problems: breadth-first search (BFS), single-source shortest path (SSSP), and minimum spanning tree (MST).

### **The Unique Challenge of Irregularity in Graphs**

The difficulty in achieving high performance for parallel graph algorithms stems from several inherent characteristics 79:

* **Irregular Data Structures and Poor Memory Locality:** Graphs are often represented using pointer-based structures like adjacency lists. Traversing a graph involves "pointer chasing," which leads to random, unpredictable memory access patterns. This irregularity defeats the hardware mechanisms—such as caching and prefetching—that modern processors rely on to hide memory latency.79  
* **Load Imbalance:** The structure of real-world graphs is often highly non-uniform. For example, in social networks, some vertices (celebrities) may have millions of connections, while most have only a few. This dramatic variation in vertex degree makes it nearly impossible to statically partition the graph in a way that gives each processor an equal amount of work.79  
* **High Communication-to-Computation Ratio:** Graph algorithms are typically memory-bound rather than compute-bound. They tend to perform very few arithmetic operations for each vertex or edge they access. Consequently, the overall execution time is dominated by the cost of moving data, whether from main memory to the processor or between processors over a network.79

### **Graph Traversal: Parallel Breadth-First Search (BFS)**

Breadth-First Search (BFS) is a fundamental algorithm for exploring a graph layer by layer from a starting vertex. Its parallelization is a classic example of the level-synchronous approach.83

#### **Methodology**

The sequential BFS algorithm maintains a queue of vertices to visit. A parallel BFS algorithm replaces this single queue with the concept of a **frontier**, which is the set of all vertices at the current distance (or level) from the source. The algorithm proceeds in synchronous steps 83:

1. Initialize the frontier to contain only the source vertex.  
2. In parallel, each processor takes a subset of vertices from the current frontier.  
3. For each vertex in its subset, the processor explores its neighbors. If a neighbor has not yet been visited, it is added to a local "next frontier."  
4. A barrier synchronization occurs. The local next frontiers from all processors are combined to form the global next frontier for the subsequent step.  
5. Repeat from step 2 until the frontier is empty.

This process can be abstractly viewed as a sequence of sparse matrix-vector multiplications, where the matrix is the adjacency matrix of the graph and the vector represents the current frontier.85 The main challenge is the potential for severe load imbalance during the neighbor exploration step if some vertices in the frontier have vastly more neighbors than others. Optimization techniques focus on dynamic load balancing and reducing the cost of the global synchronization between levels.83

### **Single-Source Shortest Path (SSSP): Parallelizing Dijkstra's Algorithm**

Dijkstra's algorithm finds the shortest paths from a single source to all other vertices in a weighted graph with non-negative edge weights. However, its standard implementation is inherently sequential.86

#### **The Sequential Bottleneck**

The core of Dijkstra's algorithm is a greedy strategy: in each step, it extracts the *one* vertex from a priority queue that has the globally minimum tentative distance from the source.87 This requirement to find the single global minimum creates a dependency that prevents the straightforward parallel processing of multiple vertices.

#### **Parallel Approaches**

Parallelizing Dijkstra's requires relaxing this strict greedy constraint.

* **Delta-Stepping Algorithm:** This is a widely used parallel SSSP algorithm. Instead of processing one vertex at a time, it processes vertices in "buckets." Each bucket corresponds to a range of distances of size Δ. In each phase, all vertices in the current lowest-indexed non-empty bucket can have their "light" edges (those with weight ≤Δ) relaxed in parallel. This introduces the possibility of redundant work (a vertex's distance may be updated multiple times), but it exposes a significant amount of parallelism by allowing many vertices to be processed concurrently.89  
* **Data Parallelism:** A simpler but less scalable approach is to use data parallelism within each step of the sequential algorithm, for instance, by parallelizing the search for the minimum-distance vertex or the relaxation of edges using frameworks like OpenMP.90

### **Minimum Spanning Tree (MST)**

An MST of a weighted, undirected graph is a subgraph that connects all vertices with the minimum possible total edge weight. The parallelizability of MST algorithms depends heavily on their underlying greedy strategy.

* **Prim's Algorithm:** Prim's algorithm is very similar in structure to Dijkstra's. It grows a single MST by iteratively adding the cheapest edge that connects a vertex inside the growing tree to a vertex outside of it.91 This reliance on a single, global greedy choice makes it inherently sequential and difficult to parallelize effectively.92  
* **Kruskal's Algorithm:** Kruskal's algorithm is more amenable to parallelism. It works by sorting all edges in the graph by weight and then adding edges from the sorted list if they do not form a cycle.94 The initial edge sorting step is highly parallelizable, as demonstrated in the sorting case study. The main sequential bottleneck is the cycle detection step, which typically uses a union-find data structure. While this step is challenging to parallelize, variants like Filter-Kruskal have been developed to improve performance by partitioning edges and filtering out those that are guaranteed to be in the same component.93  
* **Boruvka's Algorithm:** This algorithm is often cited as the most naturally parallel MST algorithm. In each step, every vertex (or every existing component) simultaneously finds the cheapest edge connecting it to another vertex (or component), and all these edges are added to the MST. This process of finding local minimums and contracting components is repeated until a single tree remains. The ability for all components to make their greedy choice independently in each step exposes massive parallelism.91

The challenges in parallelizing these graph algorithms reveal a deeper pattern: the degree of parallelism is often inversely related to the "greediness" of the sequential algorithm. Algorithms like Dijkstra's and Prim's, which rely on a single, globally optimal choice at each step, create sequential bottlenecks. In contrast, algorithms like BFS, which explores all options at a given level without a greedy choice, or Boruvka's, which allows for multiple *local* greedy choices to be made simultaneously, are far more parallelizable. To parallelize a strongly greedy algorithm often requires relaxing its strict criteria, as seen in the delta-stepping approach, effectively trading some sequential algorithmic elegance for parallel performance.  
This leads to a fundamental paradigm shift for irregular problems. For regular problems like matrix multiplication, the goal of parallel algorithm design is often *static optimization*: creating a perfect, balanced decomposition of a predictable workload. For graphs, this is impossible. The most effective parallel graph algorithms are therefore designed not to eliminate irregularity, but to tolerate and adapt to it. They shift the focus from static data decomposition to *dynamic adaptation*, employing mechanisms like work-stealing, asynchronous communication, and dynamic tasking to manage an unpredictable and evolving workload efficiently.

## References

[^1]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 6, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)  
[^2]: parallel-computational-models.pdf, accessed October 6, 2025, [https://www.ijcsma.com/articles/parallel-computational-models.pdf](https://www.ijcsma.com/articles/parallel-computational-models.pdf)  
[^3]: Models for Parallel Computing: Review and Perspectives \- IDA.LiU.SE, accessed October 6, 2025, [https://www.ida.liu.se/\~chrke55/papers/modelsurvey.pdf](https://www.ida.liu.se/~chrke55/papers/modelsurvey.pdf)  
[^4]: Parallel algorithms in shared memory \- Thomas Ropars, accessed October 6, 2025, [https://tropars.github.io/downloads/lectures/PAP/pap\_3\_shared\_memory\_algos.pdf](https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf)  
[^5]: PRAM or Parallel Random Access Machines \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/](https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/)  
[^6]: COMP 633: Parallel Computing PRAM Algorithms, accessed October 6, 2025, [https://www.cs.unc.edu/\~prins/Classes/633/Readings/pram.pdf](https://www.cs.unc.edu/~prins/Classes/633/Readings/pram.pdf)  
[^7]: Parallel Random-Access Machines \- Computer Science, UWO, accessed October 6, 2025, [https://www.csd.uwo.ca/\~mmorenom/HPC-Slides/The\_PRAM\_model.pdf](https://www.csd.uwo.ca/~mmorenom/HPC-Slides/The_PRAM_model.pdf)  
[^8]: A Survey of Parallel Algorithms for Shared-Memory Machines, accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf)  
[^9]: A survey of parallel algorithms for shared-memory machines (Book) | OSTI.GOV, accessed October 6, 2025, [https://www.osti.gov/biblio/5805553](https://www.osti.gov/biblio/5805553)  
[^10]: Parallel Computation Models \- Rice University, accessed October 6, 2025, [https://www.cs.rice.edu/\~vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf](https://www.cs.rice.edu/~vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf)  
[^11]: Parallel RAM \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_RAM](https://en.wikipedia.org/wiki/Parallel_RAM)  
[^12]: Section \\\#2: PRAM models (CS838: Topics in parallel computing, CS1221, Thu, Jan 21, 1999, 8:00-9:15 am) Pavel Tvrdik \- cs.wisc.edu, accessed October 6, 2025, [https://pages.cs.wisc.edu/\~tvrdik/2/html/Section2.html](https://pages.cs.wisc.edu/~tvrdik/2/html/Section2.html)  
[^13]: Introduction to Parallel Algorithms \- Computer Engineering Group, accessed October 6, 2025, [https://www.eecg.toronto.edu/\~ece1762/hw/par.pdf](https://www.eecg.toronto.edu/~ece1762/hw/par.pdf)  
[^14]: LogP: Towards a realistic Model of Parallel Computation, accessed October 6, 2025, [https://users.cs.utah.edu/\~kirby/classes/cs6230/CullerSlides.pdf](https://users.cs.utah.edu/~kirby/classes/cs6230/CullerSlides.pdf)  
[^15]: Bulk synchronous parallel \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bulk\_synchronous\_parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel)  
[^16]: Bulk Synchronous Parallel \- HClib-Actor Documentation, accessed October 6, 2025, [https://hclib-actor.com/background/bsp/](https://hclib-actor.com/background/bsp/)  
[^17]: RAM, PRAM, and LogP models, accessed October 6, 2025, [https://www.cs.fsu.edu/\~xyuan/cis4930-cda5125/lect23\_logpbsp.ppt](https://www.cs.fsu.edu/~xyuan/cis4930-cda5125/lect23_logpbsp.ppt)  
[^18]: BSP Tutorial \- Apache Hama, accessed October 6, 2025, [https://hama.apache.org/hama\_bsp\_tutorial.html](https://hama.apache.org/hama_bsp_tutorial.html)  
[^19]: BSP model \- Bulk, accessed October 6, 2025, [https://jwbuurlage.github.io/Bulk/bsp/](https://jwbuurlage.github.io/Bulk/bsp/)  
[^20]: LogP: Towards a Realistic Model of Parallel Computation | EECS at ..., accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html)  
[^21]: LogP: Towards a realistic model of parallel computation \- Illinois Experts, accessed October 6, 2025, [https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2](https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2)  
[^22]: CS 498 Hot Topics in High Performance Computing \- Torsten Hoefler, accessed October 6, 2025, [https://htor.ethz.ch/teaching/CS498/hoefler\_cs498\_lecture\_4.pdf](https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf)  
[^23]: LogP machine \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/LogP\_machine](https://en.wikipedia.org/wiki/LogP_machine)  
[^24]: Design of Parallel and High-Performance Computing: Distributed-Memory Models and Algorithms, accessed October 6, 2025, [https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp](https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp)  
[^25]: Lecture 26: Performance Models for Distributed Memory Parallel Computing, accessed October 6, 2025, [https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf](https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf)  
[^26]: BSP vs LogP1 \- dei.unipd.it, accessed October 6, 2025, [https://www.dei.unipd.it/\~geppo/PAPERS/BSPvsLogP.pdf](https://www.dei.unipd.it/~geppo/PAPERS/BSPvsLogP.pdf)  
[^27]: (PDF) BSP vs LogP. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/221257656\_BSP\_vs\_LogP](https://www.researchgate.net/publication/221257656_BSP_vs_LogP)  
[^28]: (PDF) LogP: A Practical Model of Parallel Computation. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220420294\_LogP\_A\_Practical\_Model\_of\_Parallel\_Computation](https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation)  
[^29]: Chapter 3\. Parallel Algorithm Design Methodology, accessed October 6, 2025, [https://www.cs.hunter.cuny.edu/\~sweiss/course\_materials/csci493.65/lecture\_notes/chapter03.pdf](https://www.cs.hunter.cuny.edu/~sweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf)  
[^30]: 9.3. Parallel Design Patterns — Computer Systems Fundamentals, accessed October 6, 2025, [https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html)  
[^31]: Parallel Algorithm Analysis and Design, accessed October 6, 2025, [https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel\_Alg\_Design.pdf](https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf)  
[^32]: Data parallelism \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Data\_parallelism](https://en.wikipedia.org/wiki/Data_parallelism)  
[^33]: What Is Data Parallelism? | Pure Storage, accessed October 6, 2025, [https://www.purestorage.com/knowledge/what-is-data-parallelism.html](https://www.purestorage.com/knowledge/what-is-data-parallelism.html)  
[^34]: Parallel Algorithm \- Quick Guide \- Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/parallel\_algorithm/parallel\_algorithm\_quick\_guide.htm](https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm)  
[^35]: Data parallelism vs Task parallelism \- Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism](https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism)  
[^36]: Parallel Algorithm Design Strategies | Parallel and Distributed Computing Class Notes | Fiveable, accessed October 6, 2025, [https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O](https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O)  
[^37]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 6, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)  
[^38]: Task parallelism \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Task\_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)  
[^39]: Types of parallelism \- Arm Immortalis and Mali GPU OpenCL Developer Guide, accessed October 6, 2025, [https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism](https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism)  
[^40]: Data and Task Parallelism \- Intel, accessed October 6, 2025, [https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html](https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html)  
[^41]: Principles of Parallel Algorithm Design: Concurrency and Decomposition \- Rice University, accessed October 6, 2025, [https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf](https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf)  
[^42]: Design of Parallel Algorithms \- Physics and Astronomy, accessed October 6, 2025, [http://homepage.physics.uiowa.edu/\~ghowes/teach/phys5905/lect/NumLec13\_Design.pdf](http://homepage.physics.uiowa.edu/~ghowes/teach/phys5905/lect/NumLec13_Design.pdf)  
[^43]: What is MapReduce? \- IBM, accessed October 6, 2025, [https://www.ibm.com/think/topics/mapreduce](https://www.ibm.com/think/topics/mapreduce)  
[^44]: MapReduce 101: What It Is & How to Get Started | Talend, accessed October 6, 2025, [https://www.talend.com/resources/what-is-mapreduce/](https://www.talend.com/resources/what-is-mapreduce/)  
[^45]: An Introduction to MapReduce with Map Reduce Example \- Analytics Vidhya, accessed October 6, 2025, [https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/)  
[^46]: Map Reduce and its Phases with numerical example. \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/](https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/)  
[^47]: 4.1 MapReduce — Parallel Computing for Beginners, accessed October 6, 2025, [https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html](https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html)  
[^48]: Parallel Data Processing with Hadoop/MapReduce \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/\~tyang/class/240a17/slides/CS240TopicMapReduce.pdf](https://sites.cs.ucsb.edu/~tyang/class/240a17/slides/CS240TopicMapReduce.pdf)  
[^49]: Parallel programming: Using the Fork-Join model in Salesforce \- West Monroe, accessed October 6, 2025, [https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce](https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce)  
[^50]: CS 365: Lecture 13: Fork/Join Parallelism, accessed October 6, 2025, [https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html](https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html)  
[^51]: Fork–join model \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Fork%E2%80%93join\_model](https://en.wikipedia.org/wiki/Fork%E2%80%93join_model)  
[^52]: Introduction to the Fork/Join Framework \- Pluralsight, accessed October 6, 2025, [https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework](https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework)  
[^53]: Fork/Join \- Essential Java Classes \- Oracle Help Center, accessed October 6, 2025, [https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html](https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html)  
[^54]: Pipeline | Our Pattern Language, accessed October 6, 2025, [https://patterns.eecs.berkeley.edu/?page\_id=542](https://patterns.eecs.berkeley.edu/?page_id=542)  
[^55]: The Pipeline Design Pattern \- Examples in C\# | HackerNoon, accessed October 6, 2025, [https://hackernoon.com/the-pipeline-design-pattern-examples-in-c](https://hackernoon.com/the-pipeline-design-pattern-examples-in-c)  
[^56]: Difference between Fork/Join and Map/Reduce \- Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce](https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce)  
[^57]: Which parallel sorting algorithm has the best average case performance? \- Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance](https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance)  
[^58]: Parallel Merge Sort \- San Jose State University, accessed October 6, 2025, [https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf](https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf)  
[^59]: Parallel Merge Sort | Zaid Humayun's Blog, accessed October 6, 2025, [https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html](https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html)  
[^60]: Overview Parallel Merge Sort, accessed October 6, 2025, [https://stanford.edu/\~rezab/classes/cme323/S16/notes/Lecture04/cme323\_lec4.pdf](https://stanford.edu/~rezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf)  
[^61]: Parallel Merge Sort Algorithm. Introduction | by Rachit Vasudeva ..., accessed October 6, 2025, [https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7](https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7)  
[^62]: What is Bitonic sort? \- Educative.io, accessed October 6, 2025, [https://www.educative.io/answers/what-is-bitonic-sort](https://www.educative.io/answers/what-is-bitonic-sort)  
[^63]: Bitonic sorter \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bitonic\_sorter](https://en.wikipedia.org/wiki/Bitonic_sorter)  
[^64]: Bitonic Sort \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/bitonic-sort/](https://www.geeksforgeeks.org/dsa/bitonic-sort/)  
[^65]: Samplesort \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Samplesort](https://en.wikipedia.org/wiki/Samplesort)  
[^66]: Parallel Sample Sort using MPI, accessed October 6, 2025, [https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf](https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf)  
[^67]: 12.7.4 Quicksort or Samplesort Algorithm \- The Netlib, accessed October 6, 2025, [https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html](https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html)  
[^68]: Comparison of parallel sorting algorithms \- arXiv, accessed October 6, 2025, [https://arxiv.org/pdf/1511.03404](https://arxiv.org/pdf/1511.03404)  
[^69]: Lecture 6: Parallel Matrix Algorithms (part 3), accessed October 6, 2025, [https://www3.nd.edu/\~zxu2/acms60212-40212-S12/Lec-07-3.pdf](https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-07-3.pdf)  
[^70]: Matrix multiplication algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Matrix\_multiplication\_algorithm](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)  
[^71]: Cannon's algorithm for distributed matrix multiplication \- OpenGenus IQ, accessed October 6, 2025, [https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/](https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/)  
[^72]: PARALLEL MATRIX MULTIPLICATION: A SYSTEMATIC JOURNEY 1\. Introduction. This paper serves a number of purposes, accessed October 6, 2025, [https://www.cs.utexas.edu/\~flame/pubs/SUMMA2d3dTOMS.pdf](https://www.cs.utexas.edu/~flame/pubs/SUMMA2d3dTOMS.pdf)  
[^73]: Cannon's Algorithm, accessed October 6, 2025, [https://users.cs.utah.edu/\~hari/teaching/paralg/tutorial/05\_Cannons.html](https://users.cs.utah.edu/~hari/teaching/paralg/tutorial/05_Cannons.html)  
[^74]: CS 140 Homework 3: SUMMA Matrix Multiplication \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/\~gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf](https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf)  
[^75]: SUMMA: Scalable Universal Matrix Multiplication Algorithm \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/\~gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf](https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf)  
[^76]: Parallel and Distributed Algorithms and ... \- Canyi Lu (卢参义), accessed October 6, 2025, [https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf](https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf)  
[^77]: Matrix multiplication on multidimensional torus networks \- UC Berkeley EECS, accessed October 6, 2025, [http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf](http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf)  
[^78]: Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms \- The Netlib, accessed October 6, 2025, [https://www.netlib.org/lapack/lawnspdf/lawn248.pdf](https://www.netlib.org/lapack/lawnspdf/lawn248.pdf)  
[^79]: (PDF) Challenges in Parallel Graph Processing. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220439595\_Challenges\_in\_Parallel\_Graph\_Processing](https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing)  
[^80]: Parallel Computing Strategies for Irregular Algorithms RUPAK BlSWAS NASA Ames Research Center LEONlD OLlKER and HONGZHANG SHAN L, accessed October 6, 2025, [https://crd.lbl.gov/assets/pubs\_presos/CDS/FTG/ARSCsubmit.pdf](https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf)  
[^81]: Parallel Computing Strategies for Irregular Algorithms \- NASA Technical Reports Server, accessed October 6, 2025, [https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf](https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf)  
[^82]: Parallel Graph Algorithms \- IIT Madras, accessed October 6, 2025, [https://www.cse.iitm.ac.in/\~rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf](https://www.cse.iitm.ac.in/~rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf)  
[^83]: Parallel breadth-first search \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_breadth-first\_search](https://en.wikipedia.org/wiki/Parallel_breadth-first_search)  
[^84]: High Level Approach to Parallel BFS \- YouTube, accessed October 6, 2025, [https://www.youtube.com/watch?v=pxOL-R7gUiQ](https://www.youtube.com/watch?v=pxOL-R7gUiQ)  
[^85]: Parallel Breadth-First Search on Distributed Memory Systems \- People @EECS, accessed October 6, 2025, [https://people.eecs.berkeley.edu/\~aydin/sc11\_bfs.pdf](https://people.eecs.berkeley.edu/~aydin/sc11_bfs.pdf)  
[^86]: A Parallelization of Dijkstra's Shortest Path Algorithm \- People, accessed October 6, 2025, [https://people.mpi-inf.mpg.de/\~mehlhorn/ftp/ParallelizationDijkstra.pdf](https://people.mpi-inf.mpg.de/~mehlhorn/ftp/ParallelizationDijkstra.pdf)  
[^87]: Dijkstra's algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Dijkstra%27s\_algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)  
[^88]: Parallel Single-Source Shortest Paths \- csail, accessed October 6, 2025, [https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf](https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf)  
[^89]: Parallel single-source shortest path algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_single-source\_shortest\_path\_algorithm](https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm)  
[^90]: Parallel Dijkstra's Algorithm: SSSP in Parallel \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/](https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/)  
[^91]: Implementing Kruskal's and Prim's Algorithms: A Comprehensive Guide \- AlgoCademy, accessed October 6, 2025, [https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/](https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/)  
[^92]: Difference between Prim's and Kruskal's algorithm for MST \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/](https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/)  
[^93]: Parallel algorithms for minimum spanning trees \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_algorithms\_for\_minimum\_spanning\_trees](https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees)  
[^94]: Kruskal's algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Kruskal%27s\_algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)