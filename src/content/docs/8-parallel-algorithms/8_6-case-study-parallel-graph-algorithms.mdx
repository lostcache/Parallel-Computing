---
title: "8.6 Case Study: Parallel Graph Algorithms"
description: "Analyzing parallel approaches to fundamental graph problems and their computational complexity."
---

Graph algorithms present a significant challenge for parallel computing. Unlike the dense, regular data structures of matrices, graphs are fundamentally irregular, unstructured, and sparse. The performance of algorithms on these structures is often dictated not by arithmetic speed, but by memory access and network communication latency. This case study explores the difficulties of parallelizing graph algorithms and examines parallel approaches for three fundamental problems: breadth-first search (BFS), single-source shortest path (SSSP), and minimum spanning tree (MST).

### **The Challenge of Irregularity in Graphs**

The difficulty in achieving high performance for parallel graph algorithms stems from several inherent characteristics:[^79]

*   **Irregular Data Structures and Poor Memory Locality:** Graphs are often represented using pointer-based structures like adjacency lists. Traversing a graph involves "pointer chasing," which leads to random, unpredictable memory access patterns. This irregularity defeats hardware mechanisms—such as caching and prefetching—that modern processors rely on to hide memory latency.[^79]
*   **Load Imbalance:** The structure of real-world graphs is often highly non-uniform. For example, in social networks, some vertices may have millions of connections, while most have only a few. This variation in vertex degree makes it difficult to statically partition the graph in a way that gives each processor an equal amount of work.[^79]
*   **High Communication-to-Computation Ratio:** Graph algorithms are typically memory-bound rather than compute-bound. They perform few arithmetic operations for each vertex or edge accessed. Consequently, execution time is dominated by the cost of moving data, whether from main memory to the processor or between processors over a network.[^79]

### **Graph Traversal: Parallel Breadth-First Search (BFS)**

Breadth-First Search (BFS) is a fundamental algorithm for exploring a graph layer by layer from a starting vertex. Its parallelization is a classic example of the level-synchronous approach.[^83]

#### **Methodology**

The sequential BFS algorithm maintains a queue of vertices to visit. A parallel BFS algorithm replaces this single queue with a **frontier**, which is the set of all vertices at the current distance (or level) from the source. The algorithm proceeds in synchronous steps:[^83]

1.  Initialize the frontier to contain only the source vertex.
2.  In parallel, each processor takes a subset of vertices from the current frontier.
3.  For each vertex in its subset, the processor explores its neighbors. If a neighbor has not yet been visited, it is added to a local "next frontier."
4.  A barrier synchronization occurs. The local next frontiers from all processors are combined to form the global next frontier for the subsequent step.
5.  Repeat from step 2 until the frontier is empty.

This process can be viewed as a sequence of sparse matrix-vector multiplications, where the matrix is the graph's adjacency matrix and the vector represents the current frontier.[^85] The main challenge is the potential for severe load imbalance if some vertices in the frontier have vastly more neighbors than others. Optimization techniques focus on dynamic load balancing and reducing the cost of global synchronization between levels.[^83]

```pseudo
Algorithm: ParallelBFS(Graph G, source vertex s, p processors)
Input: Graph $G = (V, E)$, source vertex $s$, $p$ processors
Output: Distance array dist[v] for all vertices $v$

// Initialize
parallel for all vertices $v \in V$ do
    dist[v] = $\infty$
    visited[v] = false
end parallel

dist[s] = 0
current_frontier = {s}
level = 0

// Level-synchronous BFS
while current_frontier is not empty do
    next_frontier = empty set

    // Partition frontier among processors
    frontier_per_processor = |current_frontier| / $p$

    parallel for each processor $i = 0$ to $p-1$ do
        local_next_frontier = empty set

        // Each processor processes its portion of the frontier
        for each vertex $v$ in current_frontier[i] do
            // Explore neighbors of $v$
            for each neighbor $u$ of $v$ do
                // Attempt to mark $u$ as visited
                if atomic_compare_and_swap(visited[u], false, true) then
                    dist[u] = level + 1
                    local_next_frontier.add(u)
                end
            end
        end

        // Collect local next frontiers
        contribute(local_next_frontier, next_frontier)
    end parallel

    // Barrier synchronization - wait for all processors
    barrier()

    // Gather all local next frontiers into global next frontier
    current_frontier = gather(next_frontier)
    level = level + 1
end

return dist
```

Time Complexity per level: $O((|V| + |E|)/p)$
Total levels: $O(\text{diameter of graph})$
Synchronization: $O(\text{diameter})$ barriers

Key challenges:
- Load imbalance due to irregular vertex degrees
- Contention when marking vertices as visited
- Cost of global synchronization at each level

### **Single-Source Shortest Path (SSSP): Parallelizing Dijkstra's Algorithm**

Dijkstra's algorithm finds the shortest paths from a single source to all other vertices in a weighted graph with non-negative edge weights. Its standard implementation, however, is inherently sequential.[^86]

#### **The Sequential Bottleneck**

The core of Dijkstra's algorithm is a greedy strategy: in each step, it extracts the one vertex from a priority queue that has the globally minimum tentative distance from the source.[^87] This requirement to find a single global minimum creates a dependency that prevents the straightforward parallel processing of multiple vertices.

#### **Parallel Approaches**

Parallelizing Dijkstra's requires relaxing this strict greedy constraint.

*   **Delta-Stepping Algorithm:** This is a widely used parallel SSSP algorithm. Instead of processing one vertex at a time, it processes vertices in "buckets," where each bucket corresponds to a range of distances of size $\Delta$. In each phase, all "light" edges (those with weight $\leq \Delta$) connected to vertices in the current lowest-indexed non-empty bucket can be relaxed in parallel. This may introduce redundant work, but it exposes significant parallelism by allowing many vertices to be processed concurrently.[^89]
*   **Data Parallelism:** A simpler but less scalable approach is to use data parallelism within each step of the sequential algorithm, for instance, by parallelizing the search for the minimum-distance vertex or the relaxation of edges using frameworks like OpenMP.[^90]

```pseudo
Algorithm: DeltaStepping_SSSP(Graph G, source s, $\Delta$, p processors)
Input: Weighted graph $G = (V, E)$, source $s$, parameter $\Delta$, $p$ processors
Output: Shortest distances dist[v] from $s$ to all vertices $v$

// Initialize distances
parallel for all vertices $v \in V$ do
    dist[v] = $\infty$
end parallel
dist[s] = 0

// Buckets organize vertices by distance ranges $[i \cdot \Delta, (i+1) \cdot \Delta)$
buckets = array of empty sets
buckets[0].add(s)
current_bucket = 0

while exists non-empty bucket do
    // Find lowest non-empty bucket
    while buckets[current_bucket] is empty do
        current_bucket = current_bucket + 1
    end

    // Process current bucket until empty
    while buckets[current_bucket] is not empty do
        current_set = buckets[current_bucket]
        buckets[current_bucket] = empty

        // Relax light edges (weight $\leq \Delta$) in parallel
        parallel for each vertex $v$ in current_set do
            for each edge $(v, u)$ with weight$(v, u) \leq \Delta$ do
                new_dist = dist[v] + weight(v, u)
                if new_dist < dist[u] then
                    old_dist = atomic_min(dist[u], new_dist)
                    if old_dist > new_dist then
                        move u from bucket[floor(old_dist/$\Delta$)] to bucket[floor(new_dist/$\Delta$)]
                    end
                end
            end
        end parallel
        barrier()
    end

    // Relax heavy edges (weight $> \Delta$) once per bucket
    parallel for each vertex $v$ that was in current bucket do
        for each edge $(v, u)$ with weight$(v, u) > \Delta$ do
            new_dist = dist[v] + weight(v, u)
            if new_dist < dist[u] then
                old_dist = atomic_min(dist[u], new_dist)
                if old_dist > new_dist then
                    move u from bucket[floor(old_dist/$\Delta$)] to bucket[floor(new_dist/$\Delta$)]
                end
            end
        end
    end parallel
    barrier()
end

return dist
```

Time Complexity: $O((|V| + |E|/\Delta) \times \log |V|)$ expected
Parameter $\Delta$: Trade-off between parallelism and redundant work
- Small $\Delta$: More buckets, less redundant work, less parallelism
- Large $\Delta$: Fewer buckets, more redundant work, more parallelism

### **Minimum Spanning Tree (MST)**

An MST of a weighted, undirected graph is a subgraph that connects all vertices with the minimum possible total edge weight. The parallelizability of MST algorithms depends on their underlying greedy strategy.

*   **Prim's Algorithm:** Prim's algorithm is structurally similar to Dijkstra's. It grows a single MST by iteratively adding the cheapest edge that connects a vertex inside the growing tree to a vertex outside of it.[^91] This reliance on a single, global greedy choice makes it inherently sequential and difficult to parallelize effectively.[^92]
*   **Kruskal's Algorithm:** Kruskal's algorithm is more amenable to parallelism. It sorts all edges by weight and then adds edges from the sorted list if they do not form a cycle.[^94] The initial edge sorting is highly parallelizable. The main sequential bottleneck is the cycle detection step, which typically uses a union-find data structure. While this step is challenging to parallelize, variants like Filter-Kruskal have been developed to improve performance by partitioning edges and filtering out those guaranteed to be in the same component.[^93]
*   **Boruvka's Algorithm:** This algorithm is often considered the most naturally parallel MST algorithm. In each step, every vertex (or component) simultaneously finds the cheapest edge connecting it to another vertex (or component), and all these edges are added to the MST. This process of finding local minimums and contracting components is repeated until a single tree remains. The ability for all components to make independent greedy choices in each step exposes massive parallelism.[^91]

```pseudo
Algorithm: ParallelBoruvka_MST(Graph G, p processors)
Input: Weighted undirected graph $G = (V, E)$, $p$ processors
Output: Minimum Spanning Tree edges

// Initialize: each vertex is its own component
parallel for each vertex $v \in V$ do
    component[v] = v
    cheapest_edge[v] = null
end parallel

MST_edges = empty set
num_components = $|V|$

while num_components > 1 do
    // Phase 1: Each component finds its minimum-weight outgoing edge
    parallel for each vertex $v \in V$ using $p$ processors do
        // Find cheapest edge leaving v's component
        min_edge = null
        min_weight = $\infty$

        for each edge $(v, u)$ incident to $v$ do
            if component[v] $\neq$ component[u] and weight$(v, u)$ < min_weight then
                min_edge = $(v, u)$
                min_weight = weight$(v, u)$
            end
        end

        // Update component's cheapest edge atomically
        comp_id = component[v]
        atomic_update_min(cheapest_edge[comp_id], min_edge, min_weight)
    end parallel

    barrier()

    // Phase 2: Add selected edges to MST (avoiding duplicates)
    selected_edges = empty set

    parallel for each component $c$ do
        if cheapest_edge[c] $\neq$ null then
            edge = cheapest_edge[c]
            // Add edge if not already added by other endpoint
            if edge not in selected_edges then
                selected_edges.add(edge)
            end
        end
    end parallel

    MST_edges = MST_edges $\cup$ selected_edges
    barrier()

    // Phase 3: Merge components (Union-Find)
    parallel for each edge $(u, v)$ in selected_edges do
        // Union operation: merge component[u] and component[v]
        root_u = find_root(u, component)
        root_v = find_root(v, component)

        if root_u $\neq$ root_v then
            union(root_u, root_v, component)
            atomic_decrement(num_components)
        end
    end parallel

    barrier()

    // Phase 4: Path compression for efficiency
    parallel for each vertex $v$ do
        component[v] = find_root(v, component)
        cheapest_edge[component[v]] = null  // Reset for next iteration
    end parallel

    barrier()
end

return MST_edges
```

Time Complexity: $O(\log |V|)$ iterations $\times$ $O(|E|/p)$ per iteration
Parallelism: High - all vertices/components work independently each round
Key advantage: Naturally parallel, no single sequential bottleneck

The challenges in parallelizing these graph algorithms reveal a deeper pattern: the degree of parallelism is often inversely related to the "greediness" of the sequential algorithm. Algorithms like Dijkstra's and Prim's, which rely on a single, globally optimal choice at each step, create sequential bottlenecks. In contrast, algorithms like BFS, which explores all options at a given level, or Boruvka's, which allows for multiple *local* greedy choices to be made simultaneously, are far more parallelizable. To parallelize a strongly greedy algorithm often requires relaxing its strict criteria, as seen in the delta-stepping approach, trading sequential algorithmic elegance for parallel performance.

This leads to a fundamental paradigm shift for irregular problems. For regular problems like matrix multiplication, the goal of parallel algorithm design is often *static optimization*: creating a balanced decomposition of a predictable workload. For graphs, this is often impossible. The most effective parallel graph algorithms are therefore designed not to eliminate irregularity, but to tolerate and adapt to it. They shift the focus from static data decomposition to *dynamic adaptation*, employing mechanisms like work-stealing, asynchronous communication, and dynamic tasking to manage an unpredictable and evolving workload efficiently.

## References

[^1]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 6, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)  
[^2]: parallel-computational-models.pdf, accessed October 6, 2025, [https://www.ijcsma.com/articles/parallel-computational-models.pdf](https://www.ijcsma.com/articles/parallel-computational-models.pdf)  
[^3]: Models for Parallel Computing: Review and Perspectives \- IDA.LiU.SE, accessed October 6, 2025, [https://www.ida.liu.se/\~chrke55/papers/modelsurvey.pdf](https://www.ida.liu.se/~chrke55/papers/modelsurvey.pdf)  
[^4]: Parallel algorithms in shared memory \- Thomas Ropars, accessed October 6, 2025, [https://tropars.github.io/downloads/lectures/PAP/pap\_3\_shared\_memory\_algos.pdf](https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf)  
[^5]: PRAM or Parallel Random Access Machines \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/](https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/)  
[^6]: COMP 633: Parallel Computing PRAM Algorithms, accessed October 6, 2025, [https://www.cs.unc.edu/\~prins/Classes/633/Readings/pram.pdf](https://www.cs.unc.edu/~prins/Classes/633/Readings/pram.pdf)  
[^7]: Parallel Random-Access Machines \- Computer Science, UWO, accessed October 6, 2025, [https://www.csd.uwo.ca/\~mmorenom/HPC-Slides/The\_PRAM_model.pdf](https://www.csd.uwo.ca/~mmorenom/HPC-Slides/The_PRAM_model.pdf)  
[^8]: A Survey of Parallel Algorithms for Shared-Memory Machines, accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf)  
[^9]: A survey of parallel algorithms for shared-memory machines (Book) | OSTI.GOV, accessed October 6, 2025, [https://www.osti.gov/biblio/5805553](https://www.osti.gov/biblio/5805553)  
[^10]: Parallel Computation Models \- Rice University, accessed October 6, 2025, [https://www.cs.rice.edu/\~vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf](https://www.cs.rice.edu/~vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf)  
[^11]: Parallel RAM \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_RAM](https://en.wikipedia.org/wiki/Parallel_RAM)  
[^12]: Section \\\#2: PRAM models (CS838: Topics in parallel computing, CS1221, Thu, Jan 21, 1999, 8:00-9:15 am) Pavel Tvrdik \- cs.wisc.edu, accessed October 6, 2025, [https://pages.cs.wisc.edu/\~tvrdik/2/html/Section2.html](https://pages.cs.wisc.edu/~tvrdik/2/html/Section2.html)  
[^13]: Introduction to Parallel Algorithms \- Computer Engineering Group, accessed October 6, 2025, [https://www.eecg.toronto.edu/\~ece1762/hw/par.pdf](https://www.eecg.toronto.edu/~ece1762/hw/par.pdf)  
[^14]: LogP: Towards a realistic Model of Parallel Computation, accessed October 6, 2025, [https://users.cs.utah.edu/\~kirby/classes/cs6230/CullerSlides.pdf](https://users.cs.utah.edu/~kirby/classes/cs6230/CullerSlides.pdf)  
[^15]: Bulk synchronous parallel \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bulk\_synchronous\_parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel)  
[^16]: Bulk Synchronous Parallel \- HClib-Actor Documentation, accessed October 6, 2025, [https://hclib-actor.com/background/bsp/](https://hclib-actor.com/background/bsp/)  
[^17]: RAM, PRAM, and LogP models, accessed October 6, 2025, [https://www.cs.fsu.edu/\~xyuan/cis4930-cda5125/lect23\_logpbsp.ppt](https://www.cs.fsu.edu/~xyuan/cis4930-cda5125/lect23_logpbsp.ppt)  
[^18]: BSP Tutorial \- Apache Hama, accessed October 6, 2025, [https://hama.apache.org/hama\_bsp\_tutorial.html](https://hama.apache.org/hama_bsp_tutorial.html)  
[^19]: BSP model \- Bulk, accessed October 6, 2025, [https://jwbuurlage.github.io/Bulk/bsp/](https://jwbuurlage.github.io/Bulk/bsp/)  
[^20]: LogP: Towards a Realistic Model of Parallel Computation | EECS at ..., accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html)  
[^21]: LogP: Towards a realistic model of parallel computation \- Illinois Experts, accessed October 6, 2025, [https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2](https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2)  
[^22]: CS 498 Hot Topics in High Performance Computing \- Torsten Hoefler, accessed October 6, 2025, [https://htor.ethz.ch/teaching/CS498/hoefler\_cs498\_lecture\_4.pdf](https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf)  
[^23]: LogP machine \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/LogP\_machine](https://en.wikipedia.org/wiki/LogP_machine)  
[^24]: Design of Parallel and High-Performance Computing: Distributed-Memory Models and Algorithms, accessed October 6, 2025, [https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp](https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp)  
[^25]: Lecture 26: Performance Models for Distributed Memory Parallel Computing, accessed October 6, 2025, [https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf](https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf)  
[^26]: BSP vs LogP1 \- dei.unipd.it, accessed October 6, 2025, [https://www.dei.unipd.it/\~geppo/PAPERS/BSPvsLogP.pdf](https://www.dei.unipd.it/~geppo/PAPERS/BSPvsLogP.pdf)  
[^27]: (PDF) BSP vs LogP. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/221257656\_BSP\_vs\_LogP](https://www.researchgate.net/publication/221257656_BSP_vs_LogP)  
[^28]: (PDF) LogP: A Practical Model of Parallel Computation. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220420294\_LogP\_A\_Practical\_Model\_of\_Parallel\_Computation](https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation)  
[^29]: Chapter 3\. Parallel Algorithm Design Methodology, accessed October 6, 2025, [https://www.cs.hunter.cuny.edu/\~sweiss/course\_materials/csci493.65/lecture\_notes/chapter03.pdf](https://www.cs.hunter.cuny.edu/~sweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf)  
[^30]: 9.3. Parallel Design Patterns — Computer Systems Fundamentals, accessed October 6, 2025, [https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html)  
[^31]: Parallel Algorithm Analysis and Design, accessed October 6, 2025, [https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel\_Alg\_Design.pdf](https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf)  
[^32]: Data parallelism \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Data\_parallelism](https://en.wikipedia.org/wiki/Data_parallelism)  
[^33]: What Is Data Parallelism? | Pure Storage, accessed October 6, 2025, [https://www.purestorage.com/knowledge/what-is-data-parallelism.html](https://www.purestorage.com/knowledge/what-is-data-parallelism.html)  
[^34]: Parallel Algorithm \- Quick Guide \- Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/parallel\_algorithm/parallel\_algorithm\_quick\_guide.htm](https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm)  
[^35]: Data parallelism vs Task parallelism \- Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism](https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism)  
[^36]: Parallel Algorithm Design Strategies | Parallel and Distributed Computing Class Notes | Fiveable, accessed October 6, 2025, [https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O](https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O)  
[^37]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 6, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)  
[^38]: Task parallelism \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Task\_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)  
[^39]: Types of parallelism \- Arm Immortalis and Mali GPU OpenCL Developer Guide, accessed October 6, 2025, [https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism](https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism)  
[^40]: Data and Task Parallelism \- Intel, accessed October 6, 2025, [https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html](https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html)  
[^41]: Principles of Parallel Algorithm Design: Concurrency and Decomposition \- Rice University, accessed October 6, 2025, [https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf](https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf)  
[^42]: Design of Parallel Algorithms \- Physics and Astronomy, accessed October 6, 2025, [http://homepage.physics.uiowa.edu/\~ghowes/teach/phys5905/lect/NumLec13\_Design.pdf](http://homepage.physics.uiowa.edu/~ghowes/teach/phys5905/lect/NumLec13_Design.pdf)  
[^43]: What is MapReduce? \- IBM, accessed October 6, 2025, [https://www.ibm.com/think/topics/mapreduce](https://www.ibm.com/think/topics/mapreduce)  
[^44]: MapReduce 101: What It Is & How to Get Started | Talend, accessed October 6, 2025, [https://www.talend.com/resources/what-is-mapreduce/](https://www.talend.com/resources/what-is-mapreduce/)  
[^45]: An Introduction to MapReduce with Map Reduce Example \- Analytics Vidhya, accessed October 6, 2025, [https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/)  
[^46]: Map Reduce and its Phases with numerical example. \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/](https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/)  
[^47]: 4.1 MapReduce — Parallel Computing for Beginners, accessed October 6, 2025, [https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html](https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html)  
[^48]: Parallel Data Processing with Hadoop/MapReduce \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/\~tyang/class/240a17/slides/CS240TopicMapReduce.pdf](https://sites.cs.ucsb.edu/~tyang/class/240a17/slides/CS240TopicMapReduce.pdf)  
[^49]: Parallel programming: Using the Fork-Join model in Salesforce \- West Monroe, accessed October 6, 2025, [https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce](https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce)  
[^50]: CS 365: Lecture 13: Fork/Join Parallelism, accessed October 6, 2025, [https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html](https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html)  
[^51]: Fork–join model \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Fork%E2%80%93join\_model](https://en.wikipedia.org/wiki/Fork%E2%80%93join_model)  
[^52]: Introduction to the Fork/Join Framework \- Pluralsight, accessed October 6, 2025, [https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework](https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework)  
[^53]: Fork/Join \- Essential Java Classes \- Oracle Help Center, accessed October 6, 2025, [https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html](https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html)  
[^54]: Pipeline | Our Pattern Language, accessed October 6, 2025, [https://patterns.eecs.berkeley.edu/?page\_id=542](https://patterns.eecs.berkeley.edu/?page_id=542)  
[^55]: The Pipeline Design Pattern \- Examples in C\# | HackerNoon, accessed October 6, 2025, [https://hackernoon.com/the-pipeline-design-pattern-examples-in-c](https://hackernoon.com/the-pipeline-design-pattern-examples-in-c)  
[^56]: Difference between Fork/Join and Map/Reduce \- Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce](https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce)  
[^57]: Which parallel sorting algorithm has the best average case performance? \- Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance](https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance)  
[^58]: Parallel Merge Sort \- San Jose State University, accessed October 6, 2025, [https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf](https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf)  
[^59]: Parallel Merge Sort | Zaid Humayun's Blog, accessed October 6, 2025, [https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html](https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html)  
[^60]: Overview Parallel Merge Sort, accessed October 6, 2025, [https://stanford.edu/\~rezab/classes/cme323/S16/notes/Lecture04/cme323\_lec4.pdf](https://stanford.edu/~rezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf)  
[^61]: Parallel Merge Sort Algorithm. Introduction | by Rachit Vasudeva ..., accessed October 6, 2025, [https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7](https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7)  
[^62]: What is Bitonic sort? \- Educative.io, accessed October 6, 2025, [https://www.educative.io/answers/what-is-bitonic-sort](https://www.educative.io/answers/what-is-bitonic-sort)  
[^63]: Bitonic sorter \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bitonic\_sorter](https://en.wikipedia.org/wiki/Bitonic_sorter)  
[^64]: Bitonic Sort \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/bitonic-sort/](https://www.geeksforgeeks.org/dsa/bitonic-sort/)  
[^65]: Samplesort \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Samplesort](https://en.wikipedia.org/wiki/Samplesort)  
[^66]: Parallel Sample Sort using MPI, accessed October 6, 2025, [https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf](https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf)  
[^67]: 12.7.4 Quicksort or Samplesort Algorithm \- The Netlib, accessed October 6, 2025, [https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html](https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html)  
[^68]: Comparison of parallel sorting algorithms \- arXiv, accessed October 6, 2025, [https://arxiv.org/pdf/1511.03404](https://arxiv.org/pdf/1511.03404)  
[^69]: Lecture 6: Parallel Matrix Algorithms (part 3), accessed October 6, 2025, [https://www3.nd.edu/\~zxu2/acms60212-40212-S12/Lec-07-3.pdf](https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-07-3.pdf)  
[^70]: Matrix multiplication algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Matrix\_multiplication\_algorithm](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)  
[^71]: Cannon's algorithm for distributed matrix multiplication \- OpenGenus IQ, accessed October 6, 2025, [https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/](https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/)  
[^72]: PARALLEL MATRIX MULTIPLICATION: A SYSTEMATIC JOURNEY 1\. Introduction. This paper serves a number of purposes, accessed October 6, 2025, [https://www.cs.utexas.edu/\~flame/pubs/SUMMA2d3dTOMS.pdf](https://www.cs.utexas.edu/~flame/pubs/SUMMA2d3dTOMS.pdf)  
[^73]: Cannon's Algorithm, accessed October 6, 2025, [https://users.cs.utah.edu/\~hari/teaching/paralg/tutorial/05\_Cannons.html](https://users.cs.utah.edu/~hari/teaching/paralg/tutorial/05_Cannons.html)  
[^74]: CS 140 Homework 3: SUMMA Matrix Multiplication \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/\~gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf](https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf)  
[^75]: SUMMA: Scalable Universal Matrix Multiplication Algorithm \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/\~gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf](https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf)  
[^76]: Parallel and Distributed Algorithms and ... \- Canyi Lu (卢参义), accessed October 6, 2025, [https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf](https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf)  
[^77]: Matrix multiplication on multidimensional torus networks \- UC Berkeley EECS, accessed October 6, 2025, [http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf](http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf)  
[^78]: Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms \- The Netlib, accessed October 6, 2025, [https://www.netlib.org/lapack/lawnspdf/lawn248.pdf](https://www.netlib.org/lapack/lawnspdf/lawn248.pdf)  
[^79]: (PDF) Challenges in Parallel Graph Processing. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220439595\_Challenges\_in\_Parallel\_Graph\_Processing](https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing)  
[^80]: Parallel Computing Strategies for Irregular Algorithms RUPAK BlSWAS NASA Ames Research Center LEONlD OLlKER and HONGZHANG SHAN L, accessed October 6, 2025, [https://crd.lbl.gov/assets/pubs\_presos/CDS/FTG/ARSCsubmit.pdf](https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf)  
[^81]: Parallel Computing Strategies for Irregular Algorithms \- NASA Technical Reports Server, accessed October 6, 2025, [https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf](https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf)  
[^82]: Parallel Graph Algorithms \- IIT Madras, accessed October 6, 2025, [https://www.cse.iitm.ac.in/\~rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf](https://www.cse.iitm.ac.in/~rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf)  
[^83]: Parallel breadth-first search \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_breadth-first\_search](https://en.wikipedia.org/wiki/Parallel_breadth-first_search)  
[^84]: High Level Approach to Parallel BFS \- YouTube, accessed October 6, 2025, [https://www.youtube.com/watch?v=pxOL-R7gUiQ](https://www.youtube.com/watch?v=pxOL-R7gUiQ)  
[^85]: Parallel Breadth-First Search on Distributed Memory Systems \- People @EECS, accessed October 6, 2025, [https://people.eecs.berkeley.edu/\~aydin/sc11\_bfs.pdf](https://people.eecs.berkeley.edu/~aydin/sc11_bfs.pdf)  
[^86]: A Parallelization of Dijkstra's Shortest Path Algorithm \- People, accessed October 6, 2025, [https://people.mpi-inf.mpg.de/\~mehlhorn/ftp/ParallelizationDijkstra.pdf](https://people.mpi-inf.mpg.de/~mehlhorn/ftp/ParallelizationDijkstra.pdf)  
[^87]: Dijkstra's algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Dijkstra%27s\_algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)  
[^88]: Parallel Single-Source Shortest Paths \- csail, accessed October 6, 2025, [https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf](https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf)  
[^89]: Parallel single-source shortest path algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_single-source\_shortest\_path\_algorithm](https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm)  
[^90]: Parallel Dijkstra's Algorithm: SSSP in Parallel \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/](https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/)  
[^91]: Implementing Kruskal's and Prim's Algorithms: A Comprehensive Guide \- AlgoCademy, accessed October 6, 2025, [https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/](https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/)  
[^92]: Difference between Prim's and Kruskal's algorithm for MST \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/](https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/)  
[^93]: Parallel algorithms for minimum spanning trees \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel\_algorithms\_for\_minimum\_spanning\_trees](https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees)  
[^94]: Kruskal's algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Kruskal%27s\_algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)