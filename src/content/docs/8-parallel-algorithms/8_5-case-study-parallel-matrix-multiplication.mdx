---
title: "8.5 Case Study: Parallel Matrix Multiplication"
description: "Exploring parallel approaches to matrix multiplication and their scalability properties."
---

Matrix-matrix multiplication is a cornerstone of scientific and engineering computation, forming the computational core of applications in fields from linear algebra and physics simulations to machine learning. Its high computational intensity, with $O(n^3)$ arithmetic operations for $n \times n$ matrices, makes it an excellent candidate for parallelization and a standard benchmark for high-performance computing (HPC) systems.[^69] This case study examines two classic algorithms for distributed-memory parallel matrix multiplication, Cannon's algorithm and SUMMA, which employ distinct data decomposition and communication patterns to distribute the workload across a 2D grid of processors.

### **Cannon's Algorithm**

Cannon's algorithm is a memory-efficient parallel algorithm for multiplying two matrices on a 2D processor grid, typically a square grid of $p = \sqrt{p} \times \sqrt{p}$ processors with a torus (wraparound) network topology.[^71] It orchestrates the movement of data blocks in a highly structured, nearest-neighbor fashion to ensure each processor computes its share of the result matrix.

#### **Methodology**

The algorithm for computing $C = AB$ on a $\sqrt{p} \times \sqrt{p}$ grid, where processor $P(i,j)$ is responsible for computing the sub-block $C(i,j)$, proceeds in two main phases:[^71]

1.  **Initial Alignment:** Before the main computation begins, the input matrices A and B are pre-aligned.
    *   For each row $i$, the block $A(i,j)$ is circularly shifted left by $i$ positions.
    *   For each column $j$, the block $B(i,j)$ is circularly shifted up by $j$ positions.
    This initial skew ensures that after alignment, processor $P(i,j)$ holds blocks $A(i, (j+i) \bmod \sqrt{p})$ and $B((i+j) \bmod \sqrt{p}, j)$. These are the blocks needed to compute the first term in the sum for $C(i,j)$.
2.  **Compute-and-Shift Loop:** The algorithm then enters a main loop that iterates $\sqrt{p}$ times. In each step of the loop:
    *   **Compute:** Each processor $P(i,j)$ multiplies its current local blocks of A and B and adds the result to its local $C(i,j)$ block.
    *   **Shift:** Each processor shifts its block of A one position to the left (circularly) and its block of B one position up (circularly), receiving new blocks from its right and bottom neighbors, respectively.

After $\sqrt{p}$ steps, each block of A will have passed through each processor in its row, and each block of B will have passed through each processor in its column, ensuring that all necessary products for each $C(i,j)$ have been computed. The algorithm is memory-efficient because each processor only needs to store one block of A, one of B, and one of C at any given time.[^69]

<div style={{ backgroundColor: 'white', textAlign: 'center' }}>
  <img src="https://www.cs.fsu.edu/~xyuan/cop5570/lecture17_files/image018.gif" alt="Cannon's Algorithm Visualization" />
</div>
Credit: https://www.cs.fsu.edu/~xyuan/cop5570/lecture17_files/image018.gif

```pseudo
Algorithm: CannonsAlgorithm(A[$n \times n$], B[$n \times n$], $\sqrt{p} \times \sqrt{p}$ processor grid)
Input: Matrices A and B of size $n \times n$, $p = \sqrt{p} \times \sqrt{p}$ processors
Output: Matrix $C = A \times B$

// Assume n is divisible by $\sqrt{p}$
// Processor $P(i,j)$ initially holds blocks $A(i,j)$ and $B(i,j)$

block_size = $n / \sqrt{p}$

// Phase 1: Initial Alignment (Pre-skewing)
parallel for each processor $P(i, j)$ do
    // Shift A blocks: row i shifted left by i positions (circular)
    A_local = receive_from($P(i, (j+i) \bmod \sqrt{p})$)

    // Shift B blocks: column j shifted up by j positions (circular)
    B_local = receive_from($P((i+j) \bmod \sqrt{p}, j)$)

    // Initialize result block
    C_local = 0
end parallel

// Phase 2: Compute-and-Shift Loop ($\sqrt{p}$ iterations)
for step = 0 to $\sqrt{p} - 1$ do
    parallel for each processor $P(i, j)$ do
        // Compute: multiply local blocks and accumulate
        C_local = C_local + (A_local $\times$ B_local)

        // Shift A blocks: send left, receive from right
        send(A_local, to_processor=$P(i, (j-1) \bmod \sqrt{p})$)
        A_local = receive_from($P(i, (j+1) \bmod \sqrt{p})$)

        // Shift B blocks: send up, receive from below
        send(B_local, to_processor=$P((i-1) \bmod \sqrt{p}, j)$)
        B_local = receive_from($P((i+1) \bmod \sqrt{p}, j)$)
    end parallel
end

// Each processor now has its final $C(i,j)$ block
return C assembled from all C_local blocks
```

Time Complexity:
- Computation: $O(\sqrt{p})$ steps $\times$ $O((n/\sqrt{p})^3)$ = $O(n^3/p)$
- Communication: $O(\sqrt{p})$ nearest-neighbor shifts
- Memory per processor: $O(n^2/p)$

Communication pattern: Torus/wraparound grid with nearest-neighbor only

### **SUMMA (Scalable Universal Matrix Multiplication Algorithm)**

SUMMA is a more flexible and widely used algorithm that is the basis for matrix multiplication in libraries like ScaLAPACK. Instead of nearest-neighbor shifts, it uses broadcast communication and formulates the multiplication as a sequence of rank-k updates.[^72]

#### **Methodology**

SUMMA also operates on a 2D processor grid, but it does not require the grid to be square. The algorithm for $C = C + AB$ proceeds by iterating over block columns of A and block rows of B:[^74]

1.  **Outer Loop:** The algorithm iterates $k/b$ times, where $k$ is the inner dimension of the matrices and $b$ is the block size (panel width).
2.  **Broadcast and Compute:** In each iteration, say the $k$-th one:
    *   The processors owning the $k$-th block column of A (a panel of width $b$) broadcast their data **horizontally** across their respective processor rows.
    *   Simultaneously, the processors owning the $k$-th block row of B broadcast their data **vertically** down their respective processor columns.
    *   Each processor $P(i,j)$ now has the necessary blocks of A and B. It performs a local matrix multiplication of the received blocks and adds the result to its local $C(i,j)$ block.

This process is repeated for all block columns of A and block rows of B. The result matrix C remains stationary, while the relevant panels of A and B are broadcast to all processors that need them for the current rank-$k$ update.[^72]

<div style={{ backgroundColor: 'white', textAlign: 'center' }}>
  <img src="https://i.stack.imgur.com/kK7eC.png" alt="SUMMA Algorithm Visualization" />
</div>
Credit: Stack Overflow, user 'Marius'

```pseudo
Algorithm: SUMMA(A[$n \times n$], B[$n \times n$], $p_{rows} \times p_{cols}$ processor grid)
Input: Matrices A and B of size $n \times n$, processor grid $p_{rows} \times p_{cols}$
Output: Matrix $C = A \times B$

block_size = $n / \sqrt{p}$
num_panels = $n$ / block_size

// Initialize result blocks
parallel for each processor $P(i, j)$ do
    C_local[i][j] = 0
end parallel

// Iterate over panels and accumulate partial products
for k = 0 to num_panels - 1 do
    parallel for each processor $P(i, j)$ do
        // Broadcast k-th column of A horizontally across row i
        if j == owner_of_column_k then
            A_panel = A_local[i][k]
        end
        A_panel = broadcast_along_row(A_panel, row=i)

        // Broadcast k-th row of B vertically down column j
        if i == owner_of_row_k then
            B_panel = B_local[k][j]
        end
        B_panel = broadcast_along_column(B_panel, column=j)

        // Local multiplication and accumulation
        C_local[i][j] = C_local[i][j] + (A_panel $\times$ B_panel)
    end parallel
end

return C assembled from all C_local blocks
```

Example for $4 \times 4$ processors:
- Iteration 0: Broadcast column 0 of A, row 0 of B
  - $P(0,0)$ computes: $C(0,0) += A(0,0) \times B(0,0)$
  - $P(1,2)$ computes: $C(1,2) += A(1,0) \times B(0,2)$
- Iteration 1: Broadcast column 1 of A, row 1 of B
  - Continue accumulating partial products

Time Complexity:
- Computation: $O(n^3/p)$
- Communication: $O(k/b \times n^2/\sqrt{p})$ broadcasts where $k$ is matrix dimension
- Memory per processor: $O(n^2/p)$

### **Performance Analysis and Comparison**

The two algorithms embody different approaches to data orchestration. Cannon's algorithm is a "roll-roll-compute" method, where data blocks are systematically rolled through a static processor grid via nearest-neighbor communication.[^72] It is highly synchronous and spatially oriented. SUMMA is a "broadcast-broadcast-compute" method that replicates the currently needed data panels across processor rows and columns, while the result matrix C remains stationary.[^72]

*   **Communication:** Cannon's algorithm relies on $O(\sqrt{p})$ nearest-neighbor shift operations. SUMMA relies on $O(k/b)$ broadcast operations within processor rows and columns.[^77] While broadcasts may seem more expensive, they can be efficiently pipelined on modern interconnects. SUMMA's pattern can often utilize network links more effectively than the strictly 2D communication of Cannon's algorithm.[^77]
*   **Flexibility:** SUMMA is significantly more flexible. It does not require a square processor grid or a torus topology. It also handles rectangular matrices and different block sizes, which has made it the de facto standard in production linear algebra libraries.[^72]
*   **Memory:** Both are "2D" algorithms, meaning they distribute a single copy of the matrices over a 2D processor grid and have a memory footprint of $O(n^2/p)$ per processor. Under this constraint, their communication costs are asymptotically optimal.[^77]

This analysis leads to a deeper principle in modern HPC: trading memory for communication. The existence of "2.5D" and "3D" matrix multiplication algorithms demonstrates this trade-off. A 3D algorithm, for instance, might arrange $p$ processors in a $p^{1/3} \times p^{1/3} \times p^{1/3}$ cube and replicate the input matrices $p^{1/3}$ times along the third dimension. This extra memory usage allows much of the required data to be local, dramatically reducing the volume of data that must be communicated over the network. Using $c$ copies of the data can reduce the communication bandwidth cost by a factor of $c^{1/2}$ and the latency cost by $c^{3/2}$.[^78] This shows that the notion of an "optimal" algorithm is not absolute but is contingent on available resources. While 2D algorithms like Cannon's and SUMMA are optimal under minimal memory constraints, communication-avoiding algorithms like the 2.5D/3D variants can achieve superior performance if additional memory is available.

## References

[^1]: Introduction to Parallel Computing Tutorial – | HPC @ LLNL, accessed October 6, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)  
[^2]: parallel-computational-models.pdf, accessed October 6, 2025, [https://www.ijcsma.com/articles/parallel-computational-models.pdf](https://www.ijcsma.com/articles/parallel-computational-models.pdf)  
[^3]: Models for Parallel Computing: Review and Perspectives – IDA.LiU.SE, accessed October 6, 2025, [https://www.ida.liu.se/‎chrke55/papers/modelsurvey.pdf](https://www.ida.liu.se/%E2%80%8Echrke55/papers/modelsurvey.pdf)  
[^4]: Parallel algorithms in shared memory – Thomas Ropars, accessed October 6, 2025, [https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf](https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf)  
[^5]: PRAM or Parallel Random Access Machines – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/](https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/)  
[^6]: COMP 633: Parallel Computing PRAM Algorithms, accessed October 6, 2025, [https://www.cs.unc.edu/‎prins/Classes/633/Readings/pram.pdf](https://www.cs.unc.edu/%E2%80%8Eprins/Classes/633/Readings/pram.pdf)  
[^7]: Parallel Random-Access Machines – Computer Science, UWO, accessed October 6, 2025, [https://www.csd.uwo.ca/‎mmorenom/HPC-Slides/The_PRAM_model.pdf](https://www.csd.uwo.ca/%E2%80%8Emmorenom/HPC-Slides/The_PRAM_model.pdf)  
[^8]: A Survey of Parallel Algorithms for Shared-Memory Machines, accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf)  
[^9]: A survey of parallel algorithms for shared-memory machines (Book) | OSTI.GOV, accessed October 6, 2025, [https://www.osti.gov/biblio/5805553](https://www.osti.gov/biblio/5805553)  
[^10]: Parallel Computation Models – Rice University, accessed October 6, 2025, [https://www.cs.rice.edu/‎vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf](https://www.cs.rice.edu/%E2%80%8Evs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf)  
[^11]: Parallel RAM – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_RAM](https://en.wikipedia.org/wiki/Parallel_RAM)  
[^12]: Section \#2: PRAM models (CS838: Topics in parallel computing, CS1221, Thu, Jan 21, 1999, 8:00-9:15 am) Pavel Tvrdik – cs.wisc.edu, accessed October 6, 2025, [https://pages.cs.wisc.edu/‎tvrdik/2/html/Section2.html](https://pages.cs.wisc.edu/%E2%80%8Etvrdik/2/html/Section2.html)  
[^13]: Introduction to Parallel Algorithms – Computer Engineering Group, accessed October 6, 2025, [https://www.eecg.toronto.edu/‎ece1762/hw/par.pdf](https://www.eecg.toronto.edu/%E2%80%8Eece1762/hw/par.pdf)  
[^14]: LogP: Towards a realistic Model of Parallel Computation, accessed October 6, 2025, [https://users.cs.utah.edu/‎kirby/classes/cs6230/CullerSlides.pdf](https://users.cs.utah.edu/%E2%80%8Ekirby/classes/cs6230/CullerSlides.pdf)  
[^15]: Bulk synchronous parallel – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bulk_synchronous_parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel)  
[^16]: Bulk Synchronous Parallel – HClib-Actor Documentation, accessed October 6, 2025, [https://hclib-actor.com/background/bsp/](https://hclib-actor.com/background/bsp/)  
[^17]: RAM, PRAM, and LogP models, accessed October 6, 2025, [https://www.cs.fsu.edu/‎xyuan/cis4930-cda5125/lect23_logpbsp.ppt](https://www.cs.fsu.edu/%E2%80%8Exyuan/cis4930-cda5125/lect23_logpbsp.ppt)  
[^18]: BSP Tutorial – Apache Hama, accessed October 6, 2025, [https://hama.apache.org/hama_bsp_tutorial.html](https://hama.apache.org/hama_bsp_tutorial.html)  
[^19]: BSP model – Bulk, accessed October 6, 2025, [https://jwbuurlage.github.io/Bulk/bsp/](https://jwbuurlage.github.io/Bulk/bsp/)  
[^20]: LogP: Towards a Realistic Model of Parallel Computation | EECS at ..., accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html)  
[^21]: LogP: Towards a realistic model of parallel computation – Illinois Experts, accessed October 6, 2025, [https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2](https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2)  
[^22]: CS 498 Hot Topics in High Performance Computing – Torsten Hoefler, accessed October 6, 2025, [https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf](https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf)  
[^23]: LogP machine – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/LogP_machine](https://en.wikipedia.org/wiki/LogP_machine)  
[^24]: Design of Parallel and High-Performance Computing: Distributed-Memory Models and Algorithms, accessed October 6, 2025, [https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp](https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp)  
[^25]: Lecture 26: Performance Models for Distributed Memory Parallel Computing, accessed October 6, 2025, [https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf](https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf)  
[^26]: BSP vs LogP1 – dei.unipd.it, accessed October 6, 2025, [https://www.dei.unipd.it/‎geppo/PAPERS/BSPvsLogP.pdf](https://www.dei.unipd.it/%E2%80%8Egeppo/PAPERS/BSPvsLogP.pdf)  
[^27]: (PDF) BSP vs LogP. – ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/221257656_BSP_vs_LogP](https://www.researchgate.net/publication/221257656_BSP_vs_LogP)  
[^28]: (PDF) LogP: A Practical Model of Parallel Computation. – ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation](https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation)  
[^29]: Chapter 3. Parallel Algorithm Design Methodology, accessed October 6, 2025, [https://www.cs.hunter.cuny.edu/‎sweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf](https://www.cs.hunter.cuny.edu/%E2%80%8Esweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf)  
[^30]: 9.3. Parallel Design Patterns — Computer Systems Fundamentals, accessed October 6, 2025, [https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html)  
[^31]: Parallel Algorithm Analysis and Design, accessed October 6, 2025, [https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf](https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf)  
[^32]: Data parallelism – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Data_parallelism](https://en.wikipedia.org/wiki/Data_parallelism)  
[^33]: What Is Data Parallelism? | Pure Storage, accessed October 6, 2025, [https://www.purestorage.com/knowledge/what-is-data-parallelism.html](https://www.purestorage.com/knowledge/what-is-data-parallelism.html)  
[^34]: Parallel Algorithm – Quick Guide – Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm](https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm)  
[^35]: Data parallelism vs Task parallelism – Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism](https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism)  
[^36]: Parallel Algorithm Design Strategies | Parallel and Distributed Computing Class Notes | Fiveable, accessed October 6, 2025, [https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O](https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O)  
[^37]: Data Parallel, Task Parallel, and Agent Actor Architectures – bytewax, accessed October 6, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)  
[^38]: Task parallelism – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Task_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)  
[^39]: Types of parallelism – Arm Immortalis and Mali GPU OpenCL Developer Guide, accessed October 6, 2025, [https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism](https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism)  
[^40]: Data and Task Parallelism – Intel, accessed October 6, 2025, [https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html](https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html)  
[^41]: Principles of Parallel Algorithm Design: Concurrency and Decomposition – Rice University, accessed October 6, 2025, [https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf](https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf)  
[^42]: Design of Parallel Algorithms – Physics and Astronomy, accessed October 6, 2025, [http://homepage.physics.uiowa.edu/‎ghowes/teach/phys5905/lect/NumLec13_Design.pdf](http://homepage.physics.uiowa.edu/%E2%80%8Eghowes/teach/phys5905/lect/NumLec13_Design.pdf)  
[^43]: What is MapReduce? – IBM, accessed October 6, 2025, [https://www.ibm.com/think/topics/mapreduce](https://www.ibm.com/think/topics/mapreduce)  
[^44]: MapReduce 101: What It Is & How to Get Started | Talend, accessed October 6, 2025, [https://www.talend.com/resources/what-is-mapreduce/](https://www.talend.com/resources/what-is-mapreduce/)  
[^45]: An Introduction to MapReduce with Map Reduce Example – Analytics Vidhya, accessed October 6, 2025, [https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/)  
[^46]: Map Reduce and its Phases with numerical example. – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/](https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/)  
[^47]: 4.1 MapReduce — Parallel Computing for Beginners, accessed October 6, 2025, [https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html](https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html)  
[^48]: Parallel Data Processing with Hadoop/MapReduce – UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/‎tyang/class/240a17/slides/CS240TopicMapReduce.pdf](https://sites.cs.ucsb.edu/%E2%80%8Etyang/class/240a17/slides/CS240TopicMapReduce.pdf)  
[^49]: Parallel programming: Using the Fork-Join model in Salesforce – West Monroe, accessed October 6, 2025, [https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce](https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce)  
[^50]: CS 365: Lecture 13: Fork/Join Parallelism, accessed October 6, 2025, [https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html](https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html)  
[^51]: Fork–join model – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Fork%E2%80%93join_model](https://en.wikipedia.org/wiki/Fork%E2%80%93join_model)  
[^52]: Introduction to the Fork/Join Framework – Pluralsight, accessed October 6, 2025, [https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework](https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework)  
[^53]: Fork/Join – Essential Java Classes – Oracle Help Center, accessed October 6, 2025, [https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html](https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html)  
[^54]: Pipeline | Our Pattern Language, accessed October 6, 2025, [https://patterns.eecs.berkeley.edu/?page_id=542](https://patterns.eecs.berkeley.edu/?page_id=542)  
[^55]: The Pipeline Design Pattern – Examples in C# | HackerNoon, accessed October 6, 2025, [https://hackernoon.com/the-pipeline-design-pattern-examples-in-c](https://hackernoon.com/the-pipeline-design-pattern-examples-in-c)  
[^56]: Difference between Fork/Join and Map/Reduce – Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce](https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce)  
[^57]: Which parallel sorting algorithm has the best average case performance? – Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance](https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance)  
[^58]: Parallel Merge Sort – San Jose State University, accessed October 6, 2025, [https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf](https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf)  
[^59]: Parallel Merge Sort | Zaid Humayun's Blog, accessed October 6, 2025, [https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html](https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html)  
[^60]: Overview Parallel Merge Sort, accessed October 6, 2025, [https://stanford.edu/‎rezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf](https://stanford.edu/%E2%80%8Erezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf)  
[^61]: Parallel Merge Sort Algorithm. Introduction | by Rachit Vasudeva ..., accessed October 6, 2025, [https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7](https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7)  
[^62]: What is Bitonic sort? – Educative.io, accessed October 6, 2025, [https://www.educative.io/answers/what-is-bitonic-sort](https://www.educative.io/answers/what-is-bitonic-sort)  
[^63]: Bitonic sorter – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bitonic_sorter](https://en.wikipedia.org/wiki/Bitonic_sorter)  
[^64]: Bitonic Sort – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/bitonic-sort/](https://www.geeksforgeeks.org/dsa/bitonic-sort/)  
[^65]: Samplesort – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Samplesort](https://en.wikipedia.org/wiki/Samplesort)  
[^66]: Parallel Sample Sort using MPI, accessed October 6, 2025, [https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf](https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf)  
[^67]: 12.7.4 Quicksort or Samplesort Algorithm – The Netlib, accessed October 6, 2025, [https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html](https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html)  
[^68]: Comparison of parallel sorting algorithms – arXiv, accessed October 6, 2025, [https://arxiv.org/pdf/1511.03404](https://arxiv.org/pdf/1511.03404)  
[^69]: Lecture 6: Parallel Matrix Algorithms (part 3), accessed October 6, 2025, [https://www3.nd.edu/‎ zxu2/acms60212-40212-S12/Lec-07-3.pdf](https://www3.nd.edu/%E2%80%8E zxu2/acms60212-40212-S12/Lec-07-3.pdf)  
[^70]: Matrix multiplication algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)  
[^71]: Cannon's algorithm for distributed matrix multiplication – OpenGenus IQ, accessed October 6, 2025, [https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/](https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/)  
[^72]: PARALLEL MATRIX MULTIPLICATION: A SYSTEMATIC JOURNEY 1. Introduction. This paper serves a number of purposes, accessed October 6, 2025, [https://www.cs.utexas.edu/‎flame/pubs/SUMMA2d3dTOMS.pdf](https://www.cs.utexas.edu/%E2%80%8Eflame/pubs/SUMMA2d3dTOMS.pdf)  
[^73]: Cannon's Algorithm, accessed October 6, 2025, [https://users.cs.utah.edu/‎hari/teaching/paralg/tutorial/05_Cannons.html](https://users.cs.utah.edu/%E2%80%8Ehari/teaching/paralg/tutorial/05_Cannons.html)  
[^74]: CS 140 Homework 3: SUMMA Matrix Multiplication – UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/‎gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf](https://sites.cs.ucsb.edu/%E2%80%8Egilbert/cs140/old/cs140Win2009/assignments/hw3.pdf)  
[^75]: SUMMA: Scalable Universal Matrix Multiplication Algorithm – UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/‎gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf](https://sites.cs.ucsb.edu/%E2%80%8Egilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf)  
[^76]: Parallel and Distributed Algorithms and ... – Canyi Lu (卢参义), accessed October 6, 2025, [https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf](https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf)  
[^77]: Matrix multiplication on multidimensional torus networks – UC Berkeley EECS, accessed October 6, 2025, [http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf](http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf)  
[^78]: Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms – The Netlib, accessed October 6, 2025, [https://www.netlib.org/lapack/lawnspdf/lawn248.pdf](https://www.netlib.org/lapack/lawnspdf/lawn248.pdf)  
[^79]: (PDF) Challenges in Parallel Graph Processing. – ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing](https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing)  
[^80]: Parallel Computing Strategies for Irregular Algorithms RUPAK BlSWAS NASA Ames Research Center LEONlD OLlKER and HONGZHANG SHAN L, accessed October 6, 2025, [https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf](https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf)  
[^81]: Parallel Computing Strategies for Irregular Algorithms – NASA Technical Reports Server, accessed October 6, 2025, [https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf](https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf)  
[^82]: Parallel Graph Algorithms – IIT Madras, accessed October 6, 2025, [https://www.cse.iitm.ac.in/‎rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf](https://www.cse.iitm.ac.in/%E2%80%8Erupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf)  
[^83]: Parallel breadth-first search – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_breadth-first_search](https://en.wikipedia.org/wiki/Parallel_breadth-first_search)  
[^84]: High Level Approach to Parallel BFS – YouTube, accessed October 6, 2025, [https://www.youtube.com/watch?v=pxOL-R7gUiQ](https://www.youtube.com/watch?v=pxOL-R7gUiQ)  
[^85]: Parallel Breadth-First Search on Distributed Memory Systems – People @EECS, accessed October 6, 2025, [https://people.eecs.berkeley.edu/‎aydin/sc11_bfs.pdf](https://people.eecs.berkeley.edu/%E2%80%8Eaydin/sc11_bfs.pdf)  
[^86]: A Parallelization of Dijkstra's Shortest Path Algorithm – People, accessed October 6, 2025, [https://people.mpi-inf.mpg.de/‎mehlhorn/ftp/ParallelizationDijkstra.pdf](https://people.mpi-inf.mpg.de/%E2%80%8Emehlhorn/ftp/ParallelizationDijkstra.pdf)  
[^87]: Dijkstra's algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)  
[^88]: Parallel Single-Source Shortest Paths – csail, accessed October 6, 2025, [https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf](https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf)  
[^89]: Parallel single-source shortest path algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm](https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm)  
[^90]: Parallel Dijkstra's Algorithm: SSSP in Parallel – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/](https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/)  
[^91]: Implementing Kruskal's and Prim's Algorithms: A Comprehensive Guide – AlgoCademy, accessed October 6, 2025, [https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/](https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/)  
[^92]: Difference between Prim's and Kruskal's algorithm for MST – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/](https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/)  
[^93]: Parallel algorithms for minimum spanning trees – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees](https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees)  
[^94]: Kruskal's algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Kruskal%27s_algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)