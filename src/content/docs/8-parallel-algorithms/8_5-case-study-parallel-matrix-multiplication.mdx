---
title: "8.5 Case Study: Parallel Matrix Multiplication"
description: "Exploring parallel approaches to matrix multiplication and their scalability properties."
---

Matrix-matrix multiplication is a cornerstone of scientific and engineering computation, forming the computational core of applications in fields from linear algebra and physics simulations to machine learning. Its high computational intensity, with O(n3) arithmetic operations for n×n matrices, makes it an excellent candidate for parallelization and a standard benchmark for high-performance computing (HPC) systems.69 This case study examines two classic algorithms for distributed-memory parallel matrix multiplication, Cannon's algorithm and SUMMA, which employ distinct data decomposition and communication patterns to distribute the workload across a 2D grid of processors.

### **Cannon's Algorithm**

Cannon's algorithm is a memory-efficient parallel algorithm designed for multiplying two matrices on a 2D processor grid, typically a square grid of p=p​×p​ processors with a torus (wraparound) network topology.71 It orchestrates the movement of data blocks in a highly structured, nearest-neighbor fashion to ensure each processor computes its share of the result matrix.

#### **Methodology**

The algorithm for computing C=AB on a p​×p​ grid, where processor Pi,j​ is responsible for computing the sub-block Ci,j​, proceeds in two main phases 71:

1. **Initial Alignment:** Before the main computation begins, the input matrices A and B are pre-aligned.  
   * For each row i, the block Ai,j​ is circularly shifted left by i positions.  
   * For each column j, the block Bi,j​ is circularly shifted up by j positions.  
     This initial skew ensures that after alignment, processor Pi,j​ holds blocks Ai,(j+i)%p​​ and B(i+j)%p​,j​. These are precisely the blocks needed to compute the first term in the sum for Ci,j​.  
2. **Compute-and-Shift Loop:** The algorithm then enters a main loop that iterates p​ times. In each step of the loop:  
   * **Compute:** Each processor Pi,j​ multiplies its current local blocks of A and B and adds the result to its local Ci,j​ block.  
   * **Shift:** Each processor shifts its block of A one position to the left (circularly) and its block of B one position up (circularly), receiving new blocks from its right and bottom neighbors, respectively.

After p​ steps, each block of A will have passed through each processor in its row, and each block of B will have passed through each processor in its column, ensuring that all necessary products for each Ci,j​ have been computed. The algorithm is memory-efficient because each processor only needs to store one block of A, one block of B, and one block of C at any given time.69

### **SUMMA (Scalable Universal Matrix Multiplication Algorithm)**

SUMMA is a more flexible and widely used algorithm that forms the basis for matrix multiplication in libraries like ScaLAPACK. Instead of nearest-neighbor shifts, it uses broadcast communication and formulates the multiplication as a sequence of rank-k updates.72

#### **Methodology**

SUMMA also operates on a 2D processor grid, but it does not require the grid to be square. The algorithm for C=C+AB proceeds by iterating over block columns of A and block rows of B 74:

1. **Outer Loop:** The algorithm iterates k/b times, where k is the inner dimension of the matrices and b is the block size (panel width).  
2. **Broadcast and Compute:** In each iteration, say the kth one:  
   * The processors owning the kth block column of A (a panel of width b) broadcast their data **horizontally** across their respective processor rows.  
   * Simultaneously, the processors owning the kth block row of B broadcast their data **vertically** down their respective processor columns.  
   * Each processor Pi,j​ now has the necessary blocks of A and B. It performs a local matrix multiplication of the received blocks and adds the result to its local Ci,j​ block.

This process is repeated for all block columns of A and block rows of B. The key idea is that the result matrix C remains stationary, while the relevant panels of A and B are broadcast to all processors that need them for the current rank-k update.72

### **Performance Analysis and Comparison**

The two algorithms embody different philosophies of data orchestration. Cannon's algorithm is a "roll-roll-compute" method, where data blocks are systematically rolled through a static processor grid via nearest-neighbor communication.72 It is highly synchronous and spatially oriented. SUMMA, in contrast, is a "broadcast-broadcast-compute" method. Its philosophy is to replicate the currently needed data panels across processor rows and columns, while the result matrix C remains stationary.72 This makes SUMMA more temporally oriented, focusing on broadcasting the data required for the current time step.

* **Communication:** Cannon's algorithm relies on O(p​) nearest-neighbor shift operations. SUMMA relies on O(k/b) broadcast operations within processor rows and columns.77 While broadcasts may seem more expensive, they can be efficiently pipelined on modern interconnects. SUMMA's pattern can often utilize network links more effectively than the strictly 2D communication of Cannon's algorithm.77  
* **Flexibility:** SUMMA is significantly more flexible. It does not require a square processor grid, nor does it depend on a torus topology. It also naturally handles rectangular matrices and different block sizes, which is why it has become the de facto standard in production linear algebra libraries.72  
* **Memory:** Both are considered "2D" algorithms, meaning they distribute a single copy of the matrices over a 2D processor grid and have a memory footprint of O(n2/p) per processor. Under this constraint, their communication costs are asymptotically optimal.77

This analysis, however, opens the door to a deeper principle in modern HPC: trading memory for communication. The existence of "2.5D" and "3D" matrix multiplication algorithms demonstrates this trade-off explicitly. A 3D algorithm, for instance, might arrange p processors in a p1/3×p1/3×p1/3 cube and replicate the input matrices p1/3 times along the third dimension. This extra memory usage allows much of the required data to be available locally, dramatically reducing the volume of data that must be communicated over the network. It has been shown that using c copies of the data can reduce the communication bandwidth cost by a factor of c1/2 and the latency cost by c3/2.78 This reveals that the notion of an "optimal" algorithm is not absolute; it is contingent on the available resources. While 2D algorithms like Cannon's and SUMMA are optimal under minimal memory constraints, communication-avoiding algorithms like the 2.5D/3D variants can achieve superior performance if additional memory is available.

## References

[^1]: Introduction to Parallel Computing Tutorial – | HPC @ LLNL, accessed October 6, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)  
[^2]: parallel-computational-models.pdf, accessed October 6, 2025, [https://www.ijcsma.com/articles/parallel-computational-models.pdf](https://www.ijcsma.com/articles/parallel-computational-models.pdf)  
[^3]: Models for Parallel Computing: Review and Perspectives – IDA.LiU.SE, accessed October 6, 2025, [https://www.ida.liu.se/‎chrke55/papers/modelsurvey.pdf](https://www.ida.liu.se/%E2%80%8Echrke55/papers/modelsurvey.pdf)  
[^4]: Parallel algorithms in shared memory – Thomas Ropars, accessed October 6, 2025, [https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf](https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf)  
[^5]: PRAM or Parallel Random Access Machines – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/](https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/)  
[^6]: COMP 633: Parallel Computing PRAM Algorithms, accessed October 6, 2025, [https://www.cs.unc.edu/‎prins/Classes/633/Readings/pram.pdf](https://www.cs.unc.edu/%E2%80%8Eprins/Classes/633/Readings/pram.pdf)  
[^7]: Parallel Random-Access Machines – Computer Science, UWO, accessed October 6, 2025, [https://www.csd.uwo.ca/‎mmorenom/HPC-Slides/The_PRAM_model.pdf](https://www.csd.uwo.ca/%E2%80%8Emmorenom/HPC-Slides/The_PRAM_model.pdf)  
[^8]: A Survey of Parallel Algorithms for Shared-Memory Machines, accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf)  
[^9]: A survey of parallel algorithms for shared-memory machines (Book) | OSTI.GOV, accessed October 6, 2025, [https://www.osti.gov/biblio/5805553](https://www.osti.gov/biblio/5805553)  
[^10]: Parallel Computation Models – Rice University, accessed October 6, 2025, [https://www.cs.rice.edu/‎vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf](https://www.cs.rice.edu/%E2%80%8Evs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf)  
[^11]: Parallel RAM – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_RAM](https://en.wikipedia.org/wiki/Parallel_RAM)  
[^12]: Section \#2: PRAM models (CS838: Topics in parallel computing, CS1221, Thu, Jan 21, 1999, 8:00-9:15 am) Pavel Tvrdik – cs.wisc.edu, accessed October 6, 2025, [https://pages.cs.wisc.edu/‎tvrdik/2/html/Section2.html](https://pages.cs.wisc.edu/%E2%80%8Etvrdik/2/html/Section2.html)  
[^13]: Introduction to Parallel Algorithms – Computer Engineering Group, accessed October 6, 2025, [https://www.eecg.toronto.edu/‎ece1762/hw/par.pdf](https://www.eecg.toronto.edu/%E2%80%8Eece1762/hw/par.pdf)  
[^14]: LogP: Towards a realistic Model of Parallel Computation, accessed October 6, 2025, [https://users.cs.utah.edu/‎kirby/classes/cs6230/CullerSlides.pdf](https://users.cs.utah.edu/%E2%80%8Ekirby/classes/cs6230/CullerSlides.pdf)  
[^15]: Bulk synchronous parallel – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bulk_synchronous_parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel)  
[^16]: Bulk Synchronous Parallel – HClib-Actor Documentation, accessed October 6, 2025, [https://hclib-actor.com/background/bsp/](https://hclib-actor.com/background/bsp/)  
[^17]: RAM, PRAM, and LogP models, accessed October 6, 2025, [https://www.cs.fsu.edu/‎xyuan/cis4930-cda5125/lect23_logpbsp.ppt](https://www.cs.fsu.edu/%E2%80%8Exyuan/cis4930-cda5125/lect23_logpbsp.ppt)  
[^18]: BSP Tutorial – Apache Hama, accessed October 6, 2025, [https://hama.apache.org/hama_bsp_tutorial.html](https://hama.apache.org/hama_bsp_tutorial.html)  
[^19]: BSP model – Bulk, accessed October 6, 2025, [https://jwbuurlage.github.io/Bulk/bsp/](https://jwbuurlage.github.io/Bulk/bsp/)  
[^20]: LogP: Towards a Realistic Model of Parallel Computation | EECS at ..., accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html)  
[^21]: LogP: Towards a realistic model of parallel computation – Illinois Experts, accessed October 6, 2025, [https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2](https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2)  
[^22]: CS 498 Hot Topics in High Performance Computing – Torsten Hoefler, accessed October 6, 2025, [https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf](https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf)  
[^23]: LogP machine – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/LogP_machine](https://en.wikipedia.org/wiki/LogP_machine)  
[^24]: Design of Parallel and High-Performance Computing: Distributed-Memory Models and Algorithms, accessed October 6, 2025, [https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp](https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp)  
[^25]: Lecture 26: Performance Models for Distributed Memory Parallel Computing, accessed October 6, 2025, [https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf](https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf)  
[^26]: BSP vs LogP1 – dei.unipd.it, accessed October 6, 2025, [https://www.dei.unipd.it/‎geppo/PAPERS/BSPvsLogP.pdf](https://www.dei.unipd.it/%E2%80%8Egeppo/PAPERS/BSPvsLogP.pdf)  
[^27]: (PDF) BSP vs LogP. – ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/221257656_BSP_vs_LogP](https://www.researchgate.net/publication/221257656_BSP_vs_LogP)  
[^28]: (PDF) LogP: A Practical Model of Parallel Computation. – ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation](https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation)  
[^29]: Chapter 3. Parallel Algorithm Design Methodology, accessed October 6, 2025, [https://www.cs.hunter.cuny.edu/‎sweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf](https://www.cs.hunter.cuny.edu/%E2%80%8Esweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf)  
[^30]: 9.3. Parallel Design Patterns — Computer Systems Fundamentals, accessed October 6, 2025, [https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html)  
[^31]: Parallel Algorithm Analysis and Design, accessed October 6, 2025, [https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf](https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf)  
[^32]: Data parallelism – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Data_parallelism](https://en.wikipedia.org/wiki/Data_parallelism)  
[^33]: What Is Data Parallelism? | Pure Storage, accessed October 6, 2025, [https://www.purestorage.com/knowledge/what-is-data-parallelism.html](https://www.purestorage.com/knowledge/what-is-data-parallelism.html)  
[^34]: Parallel Algorithm – Quick Guide – Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm](https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm)  
[^35]: Data parallelism vs Task parallelism – Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism](https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism)  
[^36]: Parallel Algorithm Design Strategies | Parallel and Distributed Computing Class Notes | Fiveable, accessed October 6, 2025, [https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O](https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O)  
[^37]: Data Parallel, Task Parallel, and Agent Actor Architectures – bytewax, accessed October 6, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)  
[^38]: Task parallelism – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Task_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)  
[^39]: Types of parallelism – Arm Immortalis and Mali GPU OpenCL Developer Guide, accessed October 6, 2025, [https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism](https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism)  
[^40]: Data and Task Parallelism – Intel, accessed October 6, 2025, [https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html](https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html)  
[^41]: Principles of Parallel Algorithm Design: Concurrency and Decomposition – Rice University, accessed October 6, 2025, [https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf](https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf)  
[^42]: Design of Parallel Algorithms – Physics and Astronomy, accessed October 6, 2025, [http://homepage.physics.uiowa.edu/‎ghowes/teach/phys5905/lect/NumLec13_Design.pdf](http://homepage.physics.uiowa.edu/%E2%80%8Eghowes/teach/phys5905/lect/NumLec13_Design.pdf)  
[^43]: What is MapReduce? – IBM, accessed October 6, 2025, [https://www.ibm.com/think/topics/mapreduce](https://www.ibm.com/think/topics/mapreduce)  
[^44]: MapReduce 101: What It Is & How to Get Started | Talend, accessed October 6, 2025, [https://www.talend.com/resources/what-is-mapreduce/](https://www.talend.com/resources/what-is-mapreduce/)  
[^45]: An Introduction to MapReduce with Map Reduce Example – Analytics Vidhya, accessed October 6, 2025, [https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/)  
[^46]: Map Reduce and its Phases with numerical example. – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/](https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/)  
[^47]: 4.1 MapReduce — Parallel Computing for Beginners, accessed October 6, 2025, [https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html](https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html)  
[^48]: Parallel Data Processing with Hadoop/MapReduce – UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/‎tyang/class/240a17/slides/CS240TopicMapReduce.pdf](https://sites.cs.ucsb.edu/%E2%80%8Etyang/class/240a17/slides/CS240TopicMapReduce.pdf)  
[^49]: Parallel programming: Using the Fork-Join model in Salesforce – West Monroe, accessed October 6, 2025, [https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce](https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce)  
[^50]: CS 365: Lecture 13: Fork/Join Parallelism, accessed October 6, 2025, [https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html](https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html)  
[^51]: Fork–join model – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Fork%E2%80%93join_model](https://en.wikipedia.org/wiki/Fork%E2%80%93join_model)  
[^52]: Introduction to the Fork/Join Framework – Pluralsight, accessed October 6, 2025, [https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework](https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework)  
[^53]: Fork/Join – Essential Java Classes – Oracle Help Center, accessed October 6, 2025, [https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html](https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html)  
[^54]: Pipeline | Our Pattern Language, accessed October 6, 2025, [https://patterns.eecs.berkeley.edu/?page_id=542](https://patterns.eecs.berkeley.edu/?page_id=542)  
[^55]: The Pipeline Design Pattern – Examples in C# | HackerNoon, accessed October 6, 2025, [https://hackernoon.com/the-pipeline-design-pattern-examples-in-c](https://hackernoon.com/the-pipeline-design-pattern-examples-in-c)  
[^56]: Difference between Fork/Join and Map/Reduce – Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce](https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce)  
[^57]: Which parallel sorting algorithm has the best average case performance? – Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance](https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance)  
[^58]: Parallel Merge Sort – San Jose State University, accessed October 6, 2025, [https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf](https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf)  
[^59]: Parallel Merge Sort | Zaid Humayun's Blog, accessed October 6, 2025, [https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html](https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html)  
[^60]: Overview Parallel Merge Sort, accessed October 6, 2025, [https://stanford.edu/‎rezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf](https://stanford.edu/%E2%80%8Erezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf)  
[^61]: Parallel Merge Sort Algorithm. Introduction | by Rachit Vasudeva ..., accessed October 6, 2025, [https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7](https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7)  
[^62]: What is Bitonic sort? – Educative.io, accessed October 6, 2025, [https://www.educative.io/answers/what-is-bitonic-sort](https://www.educative.io/answers/what-is-bitonic-sort)  
[^63]: Bitonic sorter – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bitonic_sorter](https://en.wikipedia.org/wiki/Bitonic_sorter)  
[^64]: Bitonic Sort – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/bitonic-sort/](https://www.geeksforgeeks.org/dsa/bitonic-sort/)  
[^65]: Samplesort – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Samplesort](https://en.wikipedia.org/wiki/Samplesort)  
[^66]: Parallel Sample Sort using MPI, accessed October 6, 2025, [https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf](https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf)  
[^67]: 12.7.4 Quicksort or Samplesort Algorithm – The Netlib, accessed October 6, 2025, [https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html](https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html)  
[^68]: Comparison of parallel sorting algorithms – arXiv, accessed October 6, 2025, [https://arxiv.org/pdf/1511.03404](https://arxiv.org/pdf/1511.03404)  
[^69]: Lecture 6: Parallel Matrix Algorithms (part 3), accessed October 6, 2025, [https://www3.nd.edu/‎ zxu2/acms60212-40212-S12/Lec-07-3.pdf](https://www3.nd.edu/%E2%80%8E zxu2/acms60212-40212-S12/Lec-07-3.pdf)  
[^70]: Matrix multiplication algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)  
[^71]: Cannon's algorithm for distributed matrix multiplication – OpenGenus IQ, accessed October 6, 2025, [https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/](https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/)  
[^72]: PARALLEL MATRIX MULTIPLICATION: A SYSTEMATIC JOURNEY 1. Introduction. This paper serves a number of purposes, accessed October 6, 2025, [https://www.cs.utexas.edu/‎flame/pubs/SUMMA2d3dTOMS.pdf](https://www.cs.utexas.edu/%E2%80%8Eflame/pubs/SUMMA2d3dTOMS.pdf)  
[^73]: Cannon's Algorithm, accessed October 6, 2025, [https://users.cs.utah.edu/‎hari/teaching/paralg/tutorial/05_Cannons.html](https://users.cs.utah.edu/%E2%80%8Ehari/teaching/paralg/tutorial/05_Cannons.html)  
[^74]: CS 140 Homework 3: SUMMA Matrix Multiplication – UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/‎gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf](https://sites.cs.ucsb.edu/%E2%80%8Egilbert/cs140/old/cs140Win2009/assignments/hw3.pdf)  
[^75]: SUMMA: Scalable Universal Matrix Multiplication Algorithm – UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/‎gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf](https://sites.cs.ucsb.edu/%E2%80%8Egilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf)  
[^76]: Parallel and Distributed Algorithms and ... – Canyi Lu (卢参义), accessed October 6, 2025, [https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf](https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf)  
[^77]: Matrix multiplication on multidimensional torus networks – UC Berkeley EECS, accessed October 6, 2025, [http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf](http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf)  
[^78]: Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms – The Netlib, accessed October 6, 2025, [https://www.netlib.org/lapack/lawnspdf/lawn248.pdf](https://www.netlib.org/lapack/lawnspdf/lawn248.pdf)  
[^79]: (PDF) Challenges in Parallel Graph Processing. – ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing](https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing)  
[^80]: Parallel Computing Strategies for Irregular Algorithms RUPAK BlSWAS NASA Ames Research Center LEONlD OLlKER and HONGZHANG SHAN L, accessed October 6, 2025, [https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf](https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf)  
[^81]: Parallel Computing Strategies for Irregular Algorithms – NASA Technical Reports Server, accessed October 6, 2025, [https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf](https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf)  
[^82]: Parallel Graph Algorithms – IIT Madras, accessed October 6, 2025, [https://www.cse.iitm.ac.in/‎rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf](https://www.cse.iitm.ac.in/%E2%80%8Erupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf)  
[^83]: Parallel breadth-first search – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_breadth-first_search](https://en.wikipedia.org/wiki/Parallel_breadth-first_search)  
[^84]: High Level Approach to Parallel BFS – YouTube, accessed October 6, 2025, [https://www.youtube.com/watch?v=pxOL-R7gUiQ](https://www.youtube.com/watch?v=pxOL-R7gUiQ)  
[^85]: Parallel Breadth-First Search on Distributed Memory Systems – People @EECS, accessed October 6, 2025, [https://people.eecs.berkeley.edu/‎aydin/sc11_bfs.pdf](https://people.eecs.berkeley.edu/%E2%80%8Eaydin/sc11_bfs.pdf)  
[^86]: A Parallelization of Dijkstra's Shortest Path Algorithm – People, accessed October 6, 2025, [https://people.mpi-inf.mpg.de/‎mehlhorn/ftp/ParallelizationDijkstra.pdf](https://people.mpi-inf.mpg.de/%E2%80%8Emehlhorn/ftp/ParallelizationDijkstra.pdf)  
[^87]: Dijkstra's algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)  
[^88]: Parallel Single-Source Shortest Paths – csail, accessed October 6, 2025, [https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf](https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf)  
[^89]: Parallel single-source shortest path algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm](https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm)  
[^90]: Parallel Dijkstra's Algorithm: SSSP in Parallel – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/](https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/)  
[^91]: Implementing Kruskal's and Prim's Algorithms: A Comprehensive Guide – AlgoCademy, accessed October 6, 2025, [https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/](https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/)  
[^92]: Difference between Prim's and Kruskal's algorithm for MST – GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/](https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/)  
[^93]: Parallel algorithms for minimum spanning trees – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees](https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees)  
[^94]: Kruskal's algorithm – Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Kruskal%27s_algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)