---
title: "8.4 Case Study: Parallel Sorting Algorithms"
description: "An in-depth analysis of parallel sorting techniques and their performance characteristics."
---

Sorting is a fundamental problem in computer science, serving not only as a critical component in countless applications but also as a canonical benchmark for evaluating the performance of parallel algorithms and architectures. The goal of parallel sorting is to significantly reduce the time required to sort a large dataset by distributing the work across multiple processors. While the best sequential comparison-based sorting algorithms have a time complexity of O(nlogn), the ideal parallel algorithm with p processors aims for a time complexity of O(pnlogn​).57 This case study examines three distinct parallel sorting algorithms—Parallel Merge Sort, Bitonic Sort, and Sample Sort—each illustrating a different design strategy and highlighting the trade-offs between computation, communication, and architectural suitability.

### **Parallel Merge Sort**

Merge sort's inherent divide-and-conquer structure makes it a natural candidate for parallelization using the Fork/Join pattern.58 The algorithm can be parallelized at two key stages: the recursive subdivision and the merging process.

#### **Methodology**

The most straightforward parallelization of merge sort involves executing the two recursive calls in parallel. Given an array to sort, the algorithm splits it into two halves. One processor (or thread) can recursively sort the left half while another simultaneously sorts the right half.59 This process continues until the sub-arrays are small enough to be sorted sequentially. The primary challenge then becomes the **parallel merge** step, where the two sorted sub-arrays must be combined into a single sorted array.
A simple sequential merge is a bottleneck. A more advanced parallel merge algorithm works by having each of the p available processors take an element from one of the sorted sub-arrays (say, A) and use binary search to find its correct position in the other sub-array (B). The final position of the element in the merged output is the sum of its indices in A and B. By performing this for all elements in parallel, the merge step can be executed efficiently.60 A scalable implementation for distributed memory often involves each of the p processors sorting a local chunk of n/p elements, followed by a tree-based merging phase where pairs of processors merge their sorted lists in successive rounds.61

### **Bitonic Sort**

Bitonic sort, developed by Ken Batcher, is a parallel sorting algorithm based on the concept of a sorting network.62 A key feature of this algorithm is that the sequence of comparisons is fixed and does not depend on the input data, making it a **data-oblivious** algorithm. This property makes it exceptionally well-suited for implementation in hardware and on highly parallel, synchronous architectures like GPUs.63

#### **Methodology**

The algorithm operates in two main phases, requiring the input size n to be a power of two:

1. **Building a Bitonic Sequence:** The algorithm first transforms the unsorted input into a *bitonic sequence*. A bitonic sequence is one that first monotonically increases and then monotonically decreases, or a circular shift of such a sequence. This is done by recursively sorting smaller sub-arrays, half in ascending order and half in descending order, and then concatenating them.
2. **Bitonic Merging:** The bitonic sequence is then sorted using a *bitonic merge* operation. This operation recursively compares and swaps elements at a distance of n/2, then n/4, and so on, until all elements are sorted. For a bitonic sequence a, comparing and swapping a\[i\] with a\[i + n/2\] for all i\<n/2 results in two smaller bitonic sequences, where all elements in the first half are smaller than all elements in the second half. These two halves are then sorted recursively in parallel.

The parallel time complexity of bitonic sort is O(log2n) using O(n) processors.63 While it performs more total comparisons than algorithms like merge sort, its highly regular and data-independent communication pattern allows for very efficient implementation on SIMD-style architectures where processors execute instructions in lock-step.64

### **Sample Sort**

Sample sort is a powerful parallel sorting algorithm designed primarily for distributed-memory systems. It is a generalization of quicksort that uses multiple pivots, called *splitters*, to partition the data into many buckets, addressing the critical challenge of load balancing that plagues simple parallel quicksort implementations.65

#### **Methodology**

The algorithm proceeds in several distinct phases 65:

1. **Sampling:** Each of the p processors randomly selects a small number of samples from its local data. The number of samples is often chosen using an *oversampling factor* k, such that each processor selects k(p−1) samples.
2. **Splitter Selection:** The samples from all processors are gathered to one processor (or all processors). These samples are sorted, and p−1 splitters are chosen from the sorted list at regular intervals. These splitters are intended to represent the overall distribution of the input data.
3. **Data Partitioning (Bucketing):** The p−1 splitters are broadcast to all processors. Each processor then uses these splitters to partition its local data into p buckets. Bucket j on processor i contains all local elements that fall between splitter j−1 and splitter j.
4. **Data Redistribution:** A global all-to-all communication phase occurs. Each processor i sends its j-th bucket to processor j. After this step, processor j holds all elements from the entire dataset that belong in the j-th bucket.
5. **Local Sort:** Each processor sorts the data in its final bucket using a fast sequential sorting algorithm.
6. **Concatenation:** Since the splitters divide the entire key range, the sorted data across the processors is now globally sorted. The concatenation of the sorted buckets from processor 0 to p−1 forms the final sorted array.

The success of sample sort hinges on the quality of the splitters. By oversampling, the algorithm increases the probability that the splitters will partition the data into nearly equal-sized buckets, thus ensuring a balanced workload in the final local sort phase, which is typically the most computationally expensive part of the algorithm.65 This upfront investment in sampling and communication to achieve load balance is what makes sample sort highly scalable on distributed systems.
The choice among these algorithms reveals a fundamental trade-off between generality and architectural specialization. Parallel merge sort is a general-purpose algorithm, representing an intuitive parallelization of a classic sequential method that performs well on shared-memory systems. In contrast, bitonic sort and sample sort are highly specialized. Bitonic sort's data-oblivious, regular structure is optimized for the synchronous, massively parallel nature of GPUs and custom hardware.63 Sample sort is tailored for distributed-memory clusters, with its entire design—sampling, partitioning, and a single all-to-all communication phase—engineered to overcome high network latency and the critical need for load balancing across independent nodes.65 This demonstrates that in parallel computing, there is no universally "best" algorithm; the optimal choice is a function of the target hardware.
Furthermore, this case study highlights a crucial shift in thinking from sequential to parallel design. While sequential sorting algorithms focus on minimizing the number of comparisons, the primary challenge in scalable parallel sorting is often not computation but rather communication and load balancing. A simple parallel quicksort can fail catastrophically if a poor pivot choice sends most of the data to one processor, leaving others idle and creating a sequential bottleneck.67 Sample sort's design is a direct acknowledgment of this problem; its complex sampling and splitter selection phases are an upfront cost paid specifically to ensure a balanced distribution of work, thereby minimizing processor idle time and maximizing parallel efficiency.65

| Feature | Parallel Merge Sort | Bitonic Sort | Sample Sort |
| :---- | :---- | :---- | :---- |
| **Core Idea** | Recursive "divide and conquer" where sub-problems are sorted in parallel and then merged. | A data-oblivious sorting network that builds and merges bitonic sequences. | A generalization of quicksort that uses multiple "splitters" to partition data into buckets for load balancing. |
| **Parallelism Strategy** | Task Parallelism (Fork/Join). | Data Parallelism (SIMD). | Data Parallelism with coordinated phases. |
| **Time Complexity (Ideal)** | O(logn) with O(n) processors. | O(log2n) with O(n) processors. | O(logn) with O(n) processors. |
| **Key Challenge** | Efficiently parallelizing the merge step. | Higher number of total comparisons; requires input size to be a power of two. | Selecting good splitters to ensure load balance; high communication cost in redistribution phase. |
| **Best Suited Architecture** | Shared-memory multi-core systems. | SIMD architectures, GPUs, FPGAs, and hardware sorting networks. | Distributed-memory clusters and supercomputers. |

**Table 8.4.1: Comparative Analysis of Parallel Sorting Algorithms.** This table summarizes the core concepts, performance characteristics, and ideal architectural contexts for Parallel Merge Sort, Bitonic Sort, and Sample Sort, illustrating the diverse strategies for tackling this fundamental problem.59

## References

[^1]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 6, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)
[^2]: parallel-computational-models.pdf, accessed October 6, 2025, [https://www.ijcsma.com/articles/parallel-computational-models.pdf](https://www.ijcsma.com/articles/parallel-computational-models.pdf)
[^3]: Models for Parallel Computing: Review and Perspectives \- IDA.LiU.SE, accessed October 6, 2025, [https://www.ida.liu.se/~chrke55/papers/modelsurvey.pdf](https://www.ida.liu.se/~chrke55/papers/modelsurvey.pdf)
[^4]: Parallel algorithms in shared memory \- Thomas Ropars, accessed October 6, 2025, [https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf](https://tropars.github.io/downloads/lectures/PAP/pap_3_shared_memory_algos.pdf)
[^5]: PRAM or Parallel Random Access Machines \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/](https://www.geeksforgeeks.org/computer-organization-architecture/pram-or-parallel-random-access-machines/)
[^6]: COMP 633: Parallel Computing PRAM Algorithms, accessed October 6, 2025, [https://www.cs.unc.edu/~prins/Classes/633/Readings/pram.pdf](https://www.cs.unc.edu/~prins/Classes/633/Readings/pram.pdf)
[^7]: Parallel Random-Access Machines \- Computer Science, UWO, accessed October 6, 2025, [https://www.csd.uwo.ca/~mmorenom/HPC-Slides/The_PRAM_model.pdf](https://www.csd.uwo.ca/~mmorenom/HPC-Slides/The_PRAM_model.pdf)
[^8]: A Survey of Parallel Algorithms for Shared-Memory Machines, accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1988/CSD-88-408.pdf)
[^9]: A survey of parallel algorithms for shared-memory machines (Book) | OSTI.GOV, accessed October 6, 2025, [https://www.osti.gov/biblio/5805553](https://www.osti.gov/biblio/5805553)
[^10]: Parallel Computation Models \- Rice University, accessed October 6, 2025, [https://www.cs.rice.edu/~vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf](https://www.cs.rice.edu/~vs3/comp422/lecture-notes/comp422-lec20-s08-v1.pdf)
[^11]: Parallel RAM \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_RAM](https://en.wikipedia.org/wiki/Parallel_RAM)
[^12]: Section \#2: PRAM models (CS838: Topics in parallel computing, CS1221, Thu, Jan 21, 1999, 8:00-9:15 am) Pavel Tvrdik \- cs.wisc.edu, accessed October 6, 2025, [https://pages.cs.wisc.edu/~tvrdik/2/html/Section2.html](https://pages.cs.wisc.edu/~tvrdik/2/html/Section2.html)
[^13]: Introduction to Parallel Algorithms \- Computer Engineering Group, accessed October 6, 2025, [https://www.eecg.toronto.edu/~ece1762/hw/par.pdf](https://www.eecg.toronto.edu/~ece1762/hw/par.pdf)
[^14]: LogP: Towards a realistic Model of Parallel Computation, accessed October 6, 2025, [https://users.cs.utah.edu/~kirby/classes/cs6230/CullerSlides.pdf](https://users.cs.utah.edu/~kirby/classes/cs6230/CullerSlides.pdf)
[^15]: Bulk synchronous parallel \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bulk_synchronous_parallel](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel)
[^16]: Bulk Synchronous Parallel \- HClib-Actor Documentation, accessed October 6, 2025, [https://hclib-actor.com/background/bsp/](https://hclib-actor.com/background/bsp/)
[^17]: RAM, PRAM, and LogP models, accessed October 6, 2025, [https://www.cs.fsu.edu/~xyuan/cis4930-cda5125/lect23_logpbsp.ppt](https://www.cs.fsu.edu/~xyuan/cis4930-cda5125/lect23_logpbsp.ppt)
[^18]: BSP Tutorial \- Apache Hama, accessed October 6, 2025, [https://hama.apache.org/hama_bsp_tutorial.html](https://hama.apache.org/hama_bsp_tutorial.html)
[^19]: BSP model \- Bulk, accessed October 6, 2025, [https://jwbuurlage.github.io/Bulk/bsp/](https://jwbuurlage.github.io/Bulk/bsp/)
[^20]: LogP: Towards a Realistic Model of Parallel Computation | EECS at ..., accessed October 6, 2025, [https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html](https://www2.eecs.berkeley.edu/Pubs/TechRpts/1992/6262.html)
[^21]: LogP: Towards a realistic model of parallel computation \- Illinois Experts, accessed October 6, 2025, [https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2](https://experts.illinois.edu/en/publications/logp-towards-a-realistic-model-of-parallel-computation-2)
[^22]: CS 498 Hot Topics in High Performance Computing \- Torsten Hoefler, accessed October 6, 2025, [https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf](https://htor.ethz.ch/teaching/CS498/hoefler_cs498_lecture_4.pdf)
[^23]: LogP machine \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/LogP_machine](https://en.wikipedia.org/wiki/LogP_machine)
[^24]: Design of Parallel and High-Performance Computing: Distributed-Memory Models and Algorithms, accessed October 6, 2025, [https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp](https://spcl.inf.ethz.ch/Teaching/2015-dphpc/lecture/lecture12-loggp)
[^25]: Lecture 26: Performance Models for Distributed Memory Parallel Computing, accessed October 6, 2025, [https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf](https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture26.pdf)
[^26]: BSP vs LogP1 \- dei.unipd.it, accessed October 6, 2025, [https://www.dei.unipd.it/~geppo/PAPERS/BSPvsLogP.pdf](https://www.dei.unipd.it/~geppo/PAPERS/BSPvsLogP.pdf)
[^27]: (PDF) BSP vs LogP. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/221257656_BSP_vs_LogP](https://www.researchgate.net/publication/221257656_BSP_vs_LogP)
[^28]: (PDF) LogP: A Practical Model of Parallel Computation. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation](https://www.researchgate.net/publication/220420294_LogP_A_Practical_Model_of_Parallel_Computation)
[^29]: Chapter 3. Parallel Algorithm Design Methodology, accessed October 6, 2025, [https://www.cs.hunter.cuny.edu/~sweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf](https://www.cs.hunter.cuny.edu/~sweiss/course_materials/csci493.65/lecture_notes/chapter03.pdf)
[^30]: 9.3. Parallel Design Patterns — Computer Systems Fundamentals, accessed October 6, 2025, [https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/ParallelDesign.html)
[^31]: Parallel Algorithm Analysis and Design, accessed October 6, 2025, [https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf](https://www.math-cs.gordon.edu/courses/cps343/presentations/Parallel_Alg_Design.pdf)
[^32]: Data parallelism \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Data_parallelism](https://en.wikipedia.org/wiki/Data_parallelism)
[^33]: What Is Data Parallelism? | Pure Storage, accessed October 6, 2025, [https://www.purestorage.com/knowledge/what-is-data-parallelism.html](https://www.purestorage.com/knowledge/what-is-data-parallelism.html)
[^34]: Parallel Algorithm \- Quick Guide \- Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm](https://www.tutorialspoint.com/parallel_algorithm/parallel_algorithm_quick_guide.htm)
[^35]: Data parallelism vs Task parallelism \- Tutorials Point, accessed October 6, 2025, [https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism](https://www.tutorialspoint.com/data-parallelism-vs-task-parallelism)
[^36]: Parallel Algorithm Design Strategies | Parallel and Distributed Computing Class Notes | Fiveable, accessed October 6, 2025, [https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O](https://fiveable.me/parallel-and-distributed-computing/unit-6/parallel-algorithm-design-strategies/study-guide/B9NPGnrWtPEbON5O)
[^37]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 6, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)
[^38]: Task parallelism \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Task_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)
[^39]: Types of parallelism \- Arm Immortalis and Mali GPU OpenCL Developer Guide, accessed October 6, 2025, [https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism](https://developer.arm.com/documentation/101574/latest/Parallel-processing-concepts/Types-of-parallelism)
[^40]: Data and Task Parallelism \- Intel, accessed October 6, 2025, [https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html](https://www.intel.com/content/www/us/en/docs/advisor/user-guide/2023-2/data-and-task-parallelism.html)
[^41]: Principles of Parallel Algorithm Design: Concurrency and Decomposition \- Rice University, accessed October 6, 2025, [https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf](https://www.clear.rice.edu/comp422/lecture-notes/comp422-534-2020-Lecture2-ConcurrencyDecomposition.pdf)
[^42]: Design of Parallel Algorithms \- Physics and Astronomy, accessed October 6, 2025, [http://homepage.physics.uiowa.edu/~ghowes/teach/phys5905/lect/NumLec13_Design.pdf](http://homepage.physics.uiowa.edu/~ghowes/teach/phys5905/lect/NumLec13_Design.pdf)
[^43]: What is MapReduce? \- IBM, accessed October 6, 2025, [https://www.ibm.com/think/topics/mapreduce](https://www.ibm.com/think/topics/mapreduce)
[^44]: MapReduce 101: What It Is & How to Get Started | Talend, accessed October 6, 2025, [https://www.talend.com/resources/what-is-mapreduce/](https://www.talend.com/resources/what-is-mapreduce/)
[^45]: An Introduction to MapReduce with Map Reduce Example \- Analytics Vidhya, accessed October 6, 2025, [https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-mapreduce-with-a-word-count-example/)
[^46]: Map Reduce and its Phases with numerical example. \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/](https://www.geeksforgeeks.org/data-science/mapreduce-understanding-with-real-life-example/)
[^47]: 4.1 MapReduce — Parallel Computing for Beginners, accessed October 6, 2025, [https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html](https://www.learnpdc.org/PDCBeginners/6-furtherAvenues/mapreduce.html)
[^48]: Parallel Data Processing with Hadoop/MapReduce \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/~tyang/class/240a17/slides/CS240TopicMapReduce.pdf](https://sites.cs.ucsb.edu/~tyang/class/240a17/slides/CS240TopicMapReduce.pdf)
[^49]: Parallel programming: Using the Fork-Join model in Salesforce \- West Monroe, accessed October 6, 2025, [https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce](https://www.westmonroe.com/insights/parallel-programming-using-the-fork-Join-model-in-salesforce)
[^50]: CS 365: Lecture 13: Fork/Join Parallelism, accessed October 6, 2025, [https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html](https://ycpcs.github.io/cs365-spring2017/lectures/lecture13.html)
[^51]: Fork–join model \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Fork%E2%80%93join_model](https://en.wikipedia.org/wiki/Fork%E2%80%93join_model)
[^52]: Introduction to the Fork/Join Framework \- Pluralsight, accessed October 6, 2025, [https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework](https://www.pluralsight.com/resources/blog/guides/introduction-to-the-fork-join-framework)
[^53]: Fork/Join \- Essential Java Classes \- Oracle Help Center, accessed October 6, 2025, [https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html](https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html)
[^54]: Pipeline | Our Pattern Language, accessed October 6, 2025, [https://patterns.eecs.berkeley.edu/?page_id=542](https://patterns.eecs.berkeley.edu/?page_id=542)
[^55]: The Pipeline Design Pattern \- Examples in C# | HackerNoon, accessed October 6, 2025, [https://hackernoon.com/the-pipeline-design-pattern-examples-in-c](https://hackernoon.com/the-pipeline-design-pattern-examples-in-c)
[^56]: Difference between Fork/Join and Map/Reduce \- Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce](https://stackoverflow.com/questions/2538224/difference-between-fork-join-and-map-reduce)
[^57]: Which parallel sorting algorithm has the best average case performance? \- Stack Overflow, accessed October 6, 2025, [https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance](https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance)
[^58]: Parallel Merge Sort \- San Jose State University, accessed October 6, 2025, [https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf](https://www.sjsu.edu/people/robert.chun/courses/cs159/s3/T.pdf)
[^59]: Parallel Merge Sort | Zaid Humayun's Blog, accessed October 6, 2025, [https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html](https://redixhumayun.github.io/systems/2023/12/29/parallel-merge-sort.html)
[^60]: Overview Parallel Merge Sort, accessed October 6, 2025, [https://stanford.edu/~rezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf](https://stanford.edu/~rezab/classes/cme323/S16/notes/Lecture04/cme323_lec4.pdf)
[^61]: Parallel Merge Sort Algorithm. Introduction | by Rachit Vasudeva ..., accessed October 6, 2025, [https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7](https://rachitvasudeva.medium.com/parallel-merge-sort-algorithm-e8175ab60e7)
[^62]: What is Bitonic sort? \- Educative.io, accessed October 6, 2025, [https://www.educative.io/answers/what-is-bitonic-sort](https://www.educative.io/answers/what-is-bitonic-sort)
[^63]: Bitonic sorter \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Bitonic_sorter](https://en.wikipedia.org/wiki/Bitonic_sorter)
[^64]: Bitonic Sort \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/bitonic-sort/](https://www.geeksforgeeks.org/dsa/bitonic-sort/)
[^65]: Samplesort \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Samplesort](https://en.wikipedia.org/wiki/Samplesort)
[^66]: Parallel Sample Sort using MPI, accessed October 6, 2025, [https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf](https://cse.buffalo.edu/faculty/miller/Courses/CSE702/Nicolas-Barrios-Fall-2021.pdf)
[^67]: 12.7.4 Quicksort or Samplesort Algorithm \- The Netlib, accessed October 6, 2025, [https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html](https://www.netlib.org/utk/lsi/pcwLSI/text/node302.html)
[^68]: Comparison of parallel sorting algorithms \- arXiv, accessed October 6, 2025, [https://arxiv.org/pdf/1511.03404](https://arxiv.org/pdf/1511.03404)
[^69]: Lecture 6: Parallel Matrix Algorithms (part 3), accessed October 6, 2025, [https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-07-3.pdf](https://www3.nd.edu/~zxu2/acms60212-40212-S12/Lec-07-3.pdf)
[^70]: Matrix multiplication algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)
[^71]: Cannon's algorithm for distributed matrix multiplication \- OpenGenus IQ, accessed October 6, 2025, [https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/](https://iq.opengenus.org/cannon-algorithm-distributed-matrix-multiplication/)
[^72]: PARALLEL MATRIX MULTIPLICATION: A SYSTEMATIC JOURNEY 1. Introduction. This paper serves a number of purposes, accessed October 6, 2025, [https://www.cs.utexas.edu/~flame/pubs/SUMMA2d3dTOMS.pdf](https://www.cs.utexas.edu/~flame/pubs/SUMMA2d3dTOMS.pdf)
[^73]: Cannon's Algorithm, accessed October 6, 2025, [https://users.cs.utah.edu/~hari/teaching/paralg/tutorial/05_Cannons.html](https://users.cs.utah.edu/~hari/teaching/paralg/tutorial/05_Cannons.html)
[^74]: CS 140 Homework 3: SUMMA Matrix Multiplication \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf](https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/hw3.pdf)
[^75]: SUMMA: Scalable Universal Matrix Multiplication Algorithm \- UCSB Computer Science, accessed October 6, 2025, [https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf](https://sites.cs.ucsb.edu/~gilbert/cs140/old/cs140Win2009/assignments/lawn96-SUMMA.pdf)
[^76]: Parallel and Distributed Algorithms and ... \- Canyi Lu (卢参义), accessed October 6, 2025, [https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf](https://canyilu.github.io/teaching/appd-fall-2016/tp4/tp4.pdf)
[^77]: Matrix multiplication on multidimensional torus networks \- UC Berkeley EECS, accessed October 6, 2025, [http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf](http://eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-28.pdf)
[^78]: Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms \- The Netlib, accessed October 6, 2025, [https://www.netlib.org/lapack/lawnspdf/lawn248.pdf](https://www.netlib.org/lapack/lawnspdf/lawn248.pdf)
[^79]: (PDF) Challenges in Parallel Graph Processing. \- ResearchGate, accessed October 6, 2025, [https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing](https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing)
[^80]: Parallel Computing Strategies for Irregular Algorithms RUPAK BlSWAS NASA Ames Research Center LEONlD OLlKER and HONGZHANG SHAN L, accessed October 6, 2025, [https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf](https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/ARSCsubmit.pdf)
[^81]: Parallel Computing Strategies for Irregular Algorithms \- NASA Technical Reports Server, accessed October 6, 2025, [https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf](https://ntrs.nasa.gov/api/citations/20020090950/downloads/20020090950.pdf)
[^82]: Parallel Graph Algorithms \- IIT Madras, accessed October 6, 2025, [https://www.cse.iitm.ac.in/~rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf](https://www.cse.iitm.ac.in/~rupesh/teaching/gpu/jan25/9-casestudy-graphs.pdf)
[^83]: Parallel breadth-first search \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_breadth-first_search](https://en.wikipedia.org/wiki/Parallel_breadth-first_search)
[^84]: High Level Approach to Parallel BFS \- YouTube, accessed October 6, 2025, [https://www.youtube.com/watch?v=pxOL-R7gUiQ](https://www.youtube.com/watch?v=pxOL-R7gUiQ)
[^85]: Parallel Breadth-First Search on Distributed Memory Systems \- People @EECS, accessed October 6, 2025, [https://people.eecs.berkeley.edu/~aydin/sc11_bfs.pdf](https://people.eecs.berkeley.edu/~aydin/sc11_bfs.pdf)
[^86]: A Parallelization of Dijkstra's Shortest Path Algorithm \- People, accessed October 6, 2025, [https://people.mpi-inf.mpg.de/~mehlhorn/ftp/ParallelizationDijkstra.pdf](https://people.mpi-inf.mpg.de/~mehlhorn/ftp/ParallelizationDijkstra.pdf)
[^87]: Dijkstra's algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)
[^88]: Parallel Single-Source Shortest Paths \- csail, accessed October 6, 2025, [https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf](https://courses.csail.mit.edu/6.884/spring10/projects/kelleyk-neboat-paper.pdf)
[^89]: Parallel single-source shortest path algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm](https://en.wikipedia.org/wiki/Parallel_single-source_shortest_path_algorithm)
[^90]: Parallel Dijkstra's Algorithm: SSSP in Parallel \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/](https://www.geeksforgeeks.org/dsa/parallel-dijkstras-algorithm-sssp-in-parallel/)
[^91]: Implementing Kruskal's and Prim's Algorithms: A Comprehensive Guide \- AlgoCademy, accessed October 6, 2025, [https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/](https://algocademy.com/blog/implementing-kruskals-and-prims-algorithms-a-comprehensive-guide/)
[^92]: Difference between Prim's and Kruskal's algorithm for MST \- GeeksforGeeks, accessed October 6, 2025, [https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/](https://www.geeksforgeeks.org/dsa/difference-between-prims-and-kruskals-algorithm-for-mst/)
[^93]: Parallel algorithms for minimum spanning trees \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees](https://en.wikipedia.org/wiki/Parallel_algorithms_for_minimum_spanning_trees)
[^94]: Kruskal's algorithm \- Wikipedia, accessed October 6, 2025, [https://en.wikipedia.org/wiki/Kruskal%27s_algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)
