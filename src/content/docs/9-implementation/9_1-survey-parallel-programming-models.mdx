---
title: "9.1 Survey of Parallel Programming Models and Libraries"
description: "An overview of the major frameworks and paradigms for implementing parallel algorithms."
---
The field of parallel programming encompasses a diverse range of models, languages, and libraries. These have evolved over several decades to adapt to the changing landscape of computer architecture.[^1] To select the appropriate tool for a given task, one must understand the fundamental principles that differentiate these models. These distinctions are primarily driven by two fundamental dichotomies: how the underlying hardware organizes memory and how computational work is decomposed.[^4]

### **9.1.1 Foundational Dichotomies: Memory and Work Decomposition**

The design of a parallel programming model is significantly influenced by the machine's memory system architecture. This characteristic determines how processing elements communicate and shapes the programmer's approach to problem-solving.

#### **Shared vs. Distributed Memory Architectures**

Parallel computer architectures are broadly classified into two families based on how processors access memory.[^2]

*   **Shared Memory:** In a shared-memory architecture, all processors or cores access a single, global address space.[^3] Any processor can read from or write to any memory location. This model simplifies programming because communication happens implicitly through shared variables.[^5] Modern multi-core CPUs in desktops, laptops, and servers are common examples of shared-memory systems.[^7] However, this simplicity introduces challenges. The main difficulty is managing concurrent access to shared data to prevent **race conditions**, where the computation's outcome depends on the unpredictable timing of operations by different threads. To ensure correctness, programmers must use synchronization mechanisms such as locks, semaphores, or atomic operations to protect critical code sections.[^6] Additionally, as the number of processors grows, the shared memory bus can become a performance bottleneck, limiting system scalability.[^2]
*   **Distributed Memory:** In a distributed-memory architecture, each processor has its own private, local memory that other processors cannot directly access.[^3] These systems comprise multiple independent nodes, each with its own processor and memory, connected by a network.[^5] Communication occurs explicitly through **message passing**, where one processor sends data in a message to another processor, which then explicitly receives it.[^3] This model is more complex to program because the developer must manage all data partitioning, distribution, and communication.[^3] However, it offers high scalability; adding more nodes increases both computational power and total memory capacity. This architecture is prevalent in large-scale systems, including high-performance computing (HPC) clusters and supercomputers.[^10]
*   **Hybrid Models:** To bridge the gap between these two extremes, hybrid models have been developed. **Distributed Shared Memory (DSM)** systems create the illusion of a single shared address space over physically distributed memory, thereby concealing the underlying message passing from the programmer.[^3] Although this simplifies programming, it does not remove the performance penalty of remote memory access; communication latency is hidden but remains present.[^3] Another approach is the **Partitioned Global Address Space (PGAS)** model, which exposes a global address space that is logically partitioned. Each partition has an "affinity" to a specific processor, allowing programmers to consider data locality while still using shared-memory-style reads and writes.[^6]

The selection between a shared or distributed memory model is a fundamental decision that shapes the entire programming paradigm. A shared-memory program focuses on synchronization, whereas a distributed-memory program emphasizes data movement. Migrating an application from one model to the other necessitates a complete re-evaluation of the algorithm's data structures and communication patterns.

#### **Data Parallelism vs. Task Parallelism**

The second fundamental dichotomy concerns how a problem is decomposed into parallel units of work.[^4]

*   **Data Parallelism:** This model involves distributing a large dataset across multiple processing units and applying the same operation to each data subset concurrently.[^4] Parallelism originates from the data rather than the operations. This pattern is common and highly scalable, especially suitable for regularly structured data such as arrays and matrices.[^6] Examples include vector processing in scientific simulations, applying the same filter to every pixel in image processing, and training deep learning models on large data batches.[^11] Architecturally, this aligns well with Single Instruction, Multiple Data (SIMD) or Single Program, Multiple Data (SPMD) execution models.[^13]
*   **Task Parallelism:** This model, also known as function or control parallelism, involves distributing distinct tasks (functions, code blocks) across different processors for concurrent execution.[^11] These tasks may operate on the same or different data and can be independent or part of a complex workflow with dependencies.[^13] Pipelining is a common example, where a data stream progresses through a series of stages, with each stage performing a unique task.[^13] Task parallelism offers greater flexibility for irregular or heterogeneous problems but poses significant load balancing challenges due to potentially varied task execution times.[^11] This model aligns with the Multiple Instruction, Multiple Data (MIMD) architectural classification.[^4]

In practice, few real-world applications are exclusively data-parallel or task-parallel. Most applications exhibit a combination of both forms of parallelism at various levels of their program structure.[^13] For example, a climate simulation might employ data parallelism to update the state of grid cells within a time step (applying the same operation to different data) and task parallelism to concurrently execute separate models for the atmosphere and the ocean (different tasks).

### **9.1.2 Models for Shared Memory Systems**

Programming models for shared-memory systems utilize the global address space, primarily offering mechanisms to create concurrent threads of execution and manage their access to shared data. The development of these models demonstrates an ongoing effort to increase abstraction, thereby protecting the programmer from the complexities of direct thread management while maintaining high performance.

#### **Directive-Based Parallelism: OpenMP and the Fork-Join Model**

Open Multi-Processing (OpenMP) is the primary standard for high-level, directive-based parallel programming on shared-memory systems.[^16] OpenMP is a specification, not a library, implemented by compiler vendors (e.g., GCC, Intel C++ Compiler). It enables programmers to incrementally parallelize serial C, C++, and Fortran code using special preprocessor directives, known as pragmas.[^18]
OpenMP's core is its **fork-join execution model**.[^18] An OpenMP program starts as a single *master thread*. When this master thread encounters a *parallel region* (defined by an OpenMP directive), it "forks," creating a team of parallel *worker threads*. All threads in the team then execute the code within this region concurrently. At the end of the parallel region, the worker threads "join" back with the master thread, which resumes serial execution until another parallel region is met.[^18]
The most common use of OpenMP is for loop parallelization. By adding the directive `#pragma omp parallel for` before a `for` loop, the programmer instructs the compiler to automatically distribute the loop's iterations among available threads.[^18] This simplicity is a key advantage of OpenMP, enabling developers to achieve significant performance improvements on multi-core processors with minimal code changes, thus making it fundamental to HPC.[^7] The model also offers various clauses to manage data scope (e.g., private, shared), perform reductions (e.g., `reduction(+:sum)`), and control scheduling.[^16]

#### **Explicit Threading: POSIX Threads (Pthreads)**

In contrast to OpenMP, POSIX Threads (Pthreads) represent a lower level of abstraction. Pthreads is a low-level C programming API, standardized by the IEEE, offering direct and explicit control over threads.[^7] This library-based approach requires including the `<pthread.h>` header and linking against the Pthreads library.[^9]
With Pthreads, the programmer manages all aspects of the thread lifecycle.[^9] New threads are created using the `pthread_create()` function, which specifies the function the new thread will execute.[^20] The main program can await a thread's completion using `pthread_join()`.[^9] Since all threads within a process share the same memory space, programmers must manually manage synchronization to prevent data races. Pthreads offers a suite of synchronization primitives, with the mutual exclusion lock, or *mutex* (`pthread_mutex_t`), being the most fundamental. A thread must acquire a lock using `pthread_mutex_lock()` before entering a critical section that accesses shared data and release it with `pthread_mutex_unlock()` upon exit, ensuring exclusive access to the critical section.[^9]
This model provides extensive control and flexibility, but at the cost of significant programming complexity. Developers must identify and manage all potential data races, synchronization, and manual work partitioning among threads. Consequently, while Pthreads is powerful for system-level programming, it is often too cumbersome for general application development, where higher-level models are typically preferred.[^7]

#### **Task-Based Parallelism: Intel Threading Building Blocks (oneTBB)**

Intel's Threading Building Blocks (now oneAPI Threading Building Blocks, or oneTBB) presents a modern C++ approach that balances the high-level simplicity of OpenMP with the explicit control of Pthreads.[^22] TBB is a C++ template library that abstracts raw threads, encouraging programmers to conceptualize work in terms of *tasks*.[^24]
TBB's philosophy promotes data-parallel programming using high-level parallel algorithms that operate on data collections, similar to the C++ Standard Template Library (STL).[^26] For instance, instead of manually creating threads for loop parallelization, a developer can use TBB's `tbb::parallel_for` algorithm, providing it with a range of indices and a C++ lambda function or function object that defines the work for each iteration.[^24]
The efficacy of TBB stems from its sophisticated runtime scheduler.[^27] In contrast to the often static scheduling of basic OpenMP, TBB utilizes a dynamic **work-stealing** scheduler to achieve automatic load balancing.[^23] In this model, each thread maintains its own task queue. When a thread becomes idle, it attempts to "steal" a task from the queue of another busy thread.[^23] This mechanism effectively manages applications with irregular or unpredictable task execution times, ensuring continuous processor core utilization without requiring manual programmer intervention.[^24] The combination of high-level C++ abstractions and an intelligent runtime positions TBB as a robust tool for developing complex, scalable parallel applications on shared-memory systems.

### **9.1.3 Models for Distributed Memory Systems**

When computation extends beyond a single machine to clusters of interconnected nodes, the shared-memory assumption becomes invalid. Programming models for these systems must address the physical reality of distributed memory, prioritizing data locality and explicit communication.

#### **Message Passing: The MPI Standard**

The Message Passing Interface (MPI) is the established standard for programming distributed-memory parallel computers, ranging from small clusters to the largest supercomputers globally.[^5] MPI is a specification for a library of functions, not a language itself, that can be invoked from C, C++, and Fortran to manage parallel processes and their communication.[^32]
The fundamental concepts of MPI are clear.[^32] An MPI program initiates as a collection of independent processes, frequently executing on distinct physical nodes. These processes are organized into a communication group, termed a **communicator**, with `MPI_COMM_WORLD` being the default, encompassing all processes. Within a communicator, each process receives a unique integer identifier, its **rank**, starting from 0.[^32]
Communication in MPI is explicit, relying on messages sent and received between processes identified by their ranks.[^32] MPI defines a comprehensive set of communication operations, categorized into two main types:

1.  **Point-to-Point Communication:** These operations involve a single sender and a single receiver. The most basic are blocking sends and receives, `MPI_Send()` and `MPI_Recv()`. When a process calls `MPI_Send()`, it packages data into a message and transmits it to a destination rank. The `MPI_Recv()` call blocks until a message from a source rank arrives and is placed into a specified buffer.[^31]
2.  **Collective Communication:** These operations involve all processes within a communicator and efficiently implement common parallel patterns. Examples include `MPI_Bcast()` (broadcast), where one process sends identical data to all other processes; `MPI_Scatter()`, where one process distributes distinct portions of an array to all other processes; `MPI_Gather()`, which is the inverse of `MPI_Scatter()`; and `MPI_Reduce()`, which combines data from all processes into a single result (e.g., computing a global sum) using a specified operation.[^31]

By requiring explicit management of data placement and inter-process communication, MPI offers maximum control over parallel execution in a distributed system. Although this approach is more complex than shared-memory models, its portability and scalability have established it as a fundamental tool in high-performance computing.[^32]

### **9.1.4 Models for Heterogeneous and Accelerator-Based Systems**

The emergence of specialized hardware accelerators, particularly Graphics Processing Units (GPUs), has added a new dimension to parallel programming. These devices provide significant computational power but have distinct architectures and memory systems, requiring specialized programming models to manage the complex interaction between a host CPU and an accelerator device.

#### **GPU Computing: NVIDIA's CUDA and the OpenCL Standard**

The evolution of GPUs from fixed-function graphics pipelines to fully programmable parallel processors for general-purpose computing (GPGPU) marked a significant turning point in HPC.[^7]

*   **CUDA (Compute Unified Device Architecture):** Developed by NVIDIA, CUDA is a proprietary parallel computing platform and programming model for its GPUs.[^34] It extends C++ with language extensions and a runtime library. The CUDA programming model is explicitly heterogeneous, based on the interaction between a **host** (CPU) and one or more **devices** (GPUs).[^36] The host and device possess separate memory spaces (system RAM and GPU VRAM, respectively), and the programmer must explicitly manage data transfers between them using functions such as `cudaMemcpy()`.[^37] Parallel computations for the GPU are written as special functions called **kernels**, designated by the `__global__` specifier. The host launches a kernel on the device using a specific syntax (`kernel_name<<<...>>>`), which specifies the number of parallel threads to execute.[^37] This model provides fine-grained control over the GPU's massive parallelism, establishing CUDA as a leading technology in fields like deep learning and scientific simulation.[^7]
*   **OpenCL (Open Computing Language):** In response to the demand for a vendor-neutral standard for heterogeneous computing, the Khronos Group developed OpenCL.[^40] OpenCL is an open, royalty-free standard for writing programs that execute on various platforms, including CPUs, GPUs, Digital Signal Processors (DSPs), and Field-Programmable Gate Arrays (FPGAs).[^40] Similar to CUDA, OpenCL uses a host-device model with explicit memory management and kernel execution.[^42] A host program defines a *context*, manages *command queues*, and submits kernels for execution on compute devices.[^43] The kernels are typically written in a C99-based language dialect known as OpenCL C.[^42] The main advantage of OpenCL is its portability; theoretically, an OpenCL program can run on any compliant hardware from any vendor.[^40] However, this portability can result in reduced performance, as the code must be more generic and may not fully leverage vendor-specific hardware features as effectively as a proprietary model like CUDA.[^41]

#### **Modern C++ Abstractions: SYCL**

The complexity of managing separate host and device codebases in CUDA and OpenCL prompted the development of higher-level abstractions. SYCL (pronounced "sickle") is a royalty-free, cross-platform abstraction layer from the Khronos Group that allows code for heterogeneous processors to be written in a **single-source** style using standard C++.[^46]
SYCL is not a standalone language but a C++ template library that operates on top of a backend such as OpenCL or CUDA.[^46] Its primary innovation is enabling both host and device code to be written in the same source file, using modern C++ features like classes, templates, and lambda functions.[^46] The programmer submits work to a device through a queue object. The work, including the kernel and its data dependencies, is encapsulated within a *command group* defined inside a lambda function.[^47] The SYCL runtime automatically manages the kernel's execution on the target device and handles the required data transfers, which are described using buffer and accessor objects.[^46] This single-source approach enhances programmability and code maintainability for complex heterogeneous systems, representing a significant step in abstracting hardware complexity from the developer.[^46]

### **9.1.5 Comparative Analysis of Programming Models**

The choice of a parallel programming model requires balancing trade-offs between performance, programmability, portability, and the target hardware. The evolution from low-level models such as Pthreads, which provide maximum control at the cost of high complexity, to high-level abstractions like SYCL, which prioritize programmer productivity and portability, directly addresses the increasing complexity of parallel hardware.[^7] This progression is a necessary adaptation to make parallel programming more accessible. The optimal model is context-dependent, determined by the application's specific requirements, the available hardware, and development resources. The following table summarizes the key models discussed, outlining their defining characteristics and typical use cases.

| Model | Primary Memory Model | Parallelism Type | Abstraction Level | Primary Use Case | Programmability |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Pthreads** | Shared | Task | Low | Fine-grained thread control, systems programming | Low |
| **OpenMP** | Shared | Data / Task | High | Incremental parallelization of loops on multi-core CPUs | High |
| **oneTBB** | Shared | Data / Task | High | C++ task-based parallelism with dynamic load balancing | High |
| **MPI** | Distributed | Data / Task | Low | Large-scale cluster and supercomputer programming | Low |
| **CUDA** | Host-Device | Data | Medium | High-performance computing on NVIDIA GPUs | Medium |
| **OpenCL** | Host-Device | Data / Task | Medium | Portable parallel programming for heterogeneous accelerators | Medium |
| **SYCL** | Host-Device | Data / Task | High | Single-source C++ for portable heterogeneous programming | High |

## References
[^1]: Models for Parallel Computing : Review and Perspectives \- Semantic Scholar, accessed October 7, 2025, [https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29](https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29)
[^2]: Shared Versus Distributed Memory Multiprocessors \- ECMWF, accessed October 7, 2025, [https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf](https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf)
[^3]: Distributed memory \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Distributed\_memory](https://en.wikipedia.org/wiki/Distributed_memory)
[^4]: Insights on Parallel Programming Model \- Advanced Millennium Technologies, accessed October 7, 2025, [https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/](https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/)
[^5]: How Parallel Processing Shaped Modern Computing \- CelerData, accessed October 7, 2025, [https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing](https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing)
[^6]: Parallel programming model \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_programming\_model](https://en.wikipedia.org/wiki/Parallel_programming_model)
[^7]: The Evolution of Parallel Programming | by Tiwariabhinav | Medium, accessed October 7, 2025, [https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88](https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88)
[^8]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 7, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)
[^9]: Multithreaded Programming (POSIX pthreads Tutorial) \- randu.org, accessed October 7, 2025, [https://randu.org/tutorials/threads/](https://randu.org/tutorials/threads/)
[^10]: Parallel Computing at a Glance, accessed October 7, 2025, [http://www.buyya.com/microkernel/chap1.pdf](http://www.buyya.com/microkernel/chap1.pdf)
[^11]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 7, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)
[^12]: Scalable Parallel Processing: Architectural Models, Real-Time Programming, and Performance Evaluation \- MDPI, accessed October 7, 2025, [https://www.mdpi.com/2673-4591/104/1/60](https://www.mdpi.com/2673-4591/104/1/60)
[^13]: Task parallelism \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Task\_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)
[^14]: Parallel computing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_computing](https://en.wikipedia.org/wiki/Parallel_computing)
[^15]: What is a good example for task parallelismn in data processing? \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing](https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing)
[^16]: Using OpenMP with C — Research Computing University of ..., accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html](https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html)
[^17]: C++ Examples of Parallel Programming with OpenMP, accessed October 7, 2025, [https://people.math.sc.edu/burkardt/cpp\_src/openmp/openmp.html](https://people.math.sc.edu/burkardt/cpp_src/openmp/openmp.html)
[^18]: An introduction to OpenMP \- University College London, accessed October 7, 2025, [https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02\_intro\_openmp.html](https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02_intro_openmp.html)
[^19]: OpenMP Tutorial \- Scalable Parallel Computing Lab, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf](https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf)
[^20]: C++ Tutorial: Multi-Threaded Programming \- C++ Class Thread for Pthreads \- 2020 \- BogoToBogo, accessed October 7, 2025, [https://www.bogotobogo.com/cplusplus/multithreading\_pthread.php](https://www.bogotobogo.com/cplusplus/multithreading_pthread.php)
[^21]: Linux Tutorial: POSIX Threads, accessed October 7, 2025, [https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html](https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html)
[^22]: Getting Started with Intel® Threading Building Blocks (Intel® TBB), accessed October 7, 2025, [https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html](https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html)
[^23]: Threading Building Blocks \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Threading\_Building\_Blocks](https://en.wikipedia.org/wiki/Threading_Building_Blocks)
[^24]: Introduction to the Intel Threading Building Blocks — mcs572 0.7.8 documentation, accessed October 7, 2025, [http://homepages.math.uic.edu/\~jan/mcs572f16/mcs572notes/lec11.html](http://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec11.html)
[^25]: Threading Building Blocks \- Tools \- EBRAINS, accessed October 7, 2025, [https://www.ebrains.eu/tools/threading-building-blocks](https://www.ebrains.eu/tools/threading-building-blocks)
[^26]: TBB Tutorial \- cs.wisc.edu, accessed October 7, 2025, [https://pages.cs.wisc.edu/\~gibson/tbbTutorial.html](https://pages.cs.wisc.edu/~gibson/tbbTutorial.html)
[^27]: Introducing Intel Tbb \- Agenda INFN, accessed October 7, 2025, [https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb\_Introduction.pdf](https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb_Introduction.pdf)
[^28]: Intel(R) Threading Building Blocks Getting Started Guide, accessed October 7, 2025, [http://www.physics.ntua.gr/\~konstant/HetCluster/intel12.1/tbb/Getting\_Started.pdf](http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/tbb/Getting_Started.pdf)
[^29]: Work stealing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Work\_stealing](https://en.wikipedia.org/wiki/Work_stealing)
[^30]: Towards Efficient HPC: Exploring Overlap Strategies Using MPI Non ..., accessed October 7, 2025, [https://www.mdpi.com/2227-7390/13/11/1848](https://www.mdpi.com/2227-7390/13/11/1848)
[^31]: Tutorials · MPI Tutorial, accessed October 7, 2025, [https://mpitutorial.com/tutorials/](https://mpitutorial.com/tutorials/)
[^32]: MPI Tutorial – Part 1, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf](https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf)
[^33]: Using MPI with C \- CU Research Computing User Guide, accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/MPI-C.html](https://curc.readthedocs.io/en/latest/programming/MPI-C.html)
[^34]: (PDF) Review of Architecture and Model for Parallel Programming \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/372822306\_Review\_of\_Architecture\_and\_Model\_for\_Parallel\_Programming](https://www.researchgate.net/publication/372822306_Review_of_Architecture_and_Model_for_Parallel_Programming)
[^35]: mikeroyal/CUDA-Guide \- GitHub, accessed October 7, 2025, [https://github.com/mikeroyal/CUDA-Guide](https://github.com/mikeroyal/CUDA-Guide)
[^36]: CUDA C++ Programming Guide | NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/pdf/CUDA\_C\_Programming\_Guide.pdf](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
[^37]: CUDA Programming Guide: An Overview | by Zia Babar | Aug, 2025 \- Medium, accessed October 7, 2025, [https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8](https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8)
[^38]: Matrix Multiplication in CUDA \- Harshit Kumar, accessed October 7, 2025, [https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda](https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda)
[^39]: What is NVIDIA CUDA? What can it do for Deep Learning? \- Exxact Corporation, accessed October 7, 2025, [https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning](https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning)
[^40]: OpenCL \- The Open Standard for Parallel Programming of ..., accessed October 7, 2025, [https://www.khronos.org/opencl/](https://www.khronos.org/opencl/)
[^41]: An introduction to OpenCL \- Purdue Engineering, accessed October 7, 2025, [https://engineering.purdue.edu/\~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf](https://engineering.purdue.edu/~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf)
[^42]: OpenCL \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/OpenCL](https://en.wikipedia.org/wiki/OpenCL)
[^43]: OpenCL execution model \- Arm Developer, accessed October 7, 2025, [https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model](https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model)
[^44]: The OpenCL™ C Specification, accessed October 7, 2025, [https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL\_C.pdf](https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL_C.pdf)
[^45]: The OpenCL Specification \- IC-Unicamp, accessed October 7, 2025, [https://www.ic.unicamp.br/\~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf](https://www.ic.unicamp.br/~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf)
[^46]: Heterogeneous programming with SYCL documentation, accessed October 7, 2025, [https://enccs.github.io/sycl-workshop/](https://enccs.github.io/sycl-workshop/)
[^47]: Getting Started \- SYCL.tech, accessed October 7, 2025, [https://sycl.tech/getting-started/](https://sycl.tech/getting-started/)
[^48]: Compiler for Parallel Machines. Parallel computing is an approach ..., accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf)
[^49]: The Nexus Task-parallel Runtime System, accessed October 7, 2025, [https://marketing.globuscs.info/production/strapi/uploads/india\_paper\_ps\_40c2982c84.pdf](https://marketing.globuscs.info/production/strapi/uploads/india_paper_ps_40c2982c84.pdf)
[^50]: medium.com, accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf\#:\~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf#:~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.)
[^51]: en.wikipedia.org, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Automatic\_parallelization\_tool\#:\~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization\&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.](https://en.wikipedia.org/wiki/Automatic_parallelization_tool#:~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.)
[^52]: Auto-parallelization Overview, accessed October 7, 2025, [https://www.cita.utoronto.ca/\~merz/intel\_c10b/main\_cls/mergedProjects/optaps\_cls/common/optaps\_qpar\_par.htm](https://www.cita.utoronto.ca/~merz/intel_c10b/main_cls/mergedProjects/optaps_cls/common/optaps_qpar_par.htm)
[^53]: Compilers for Parallel Machines: A User-Friendly Guide | by Pranay Junghare | Medium, accessed October 7, 2025, [https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6](https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6)
[^54]: Pragma directives for OpenMP parallelization \- IBM, accessed October 7, 2025, [https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization)
[^55]: Directive Format \- OpenMP, accessed October 7, 2025, [https://www.openmp.org/spec-html/5.1/openmpse9.html](https://www.openmp.org/spec-html/5.1/openmpse9.html)
[^56]: What is parallel computing? \- IBM, accessed October 7, 2025, [https://www.ibm.com/think/topics/parallel-computing](https://www.ibm.com/think/topics/parallel-computing)
[^57]: Parallel Programming Models, Languages and Compilers \- csenotes, accessed October 7, 2025, [https://csenotes.github.io/pdf/mod5\_aca.pdf](https://csenotes.github.io/pdf/mod5_aca.pdf)
[^58]: (PDF) A Comparative Study and Evaluation of Parallel Programming ..., accessed October 7, 2025, [https://www.researchgate.net/publication/255791855\_A\_Comparative\_Study\_and\_Evaluation\_of\_Parallel\_Programming\_Models\_for\_Shared-Memory\_Parallel\_Architectures](https://www.researchgate.net/publication/255791855_A_Comparative_Study_and_Evaluation_of_Parallel_Programming_Models_for_Shared-Memory_Parallel_Architectures)
[^59]: Scheduling Multithreaded Computations by Work Stealing, accessed October 7, 2025, [https://www.csd.uwo.ca/\~mmorenom/CS433-CS9624/Resources/Scheduling\_multithreaded\_computations\_by\_work\_stealing.pdf](https://www.csd.uwo.ca/~mmorenom/CS433-CS9624/Resources/Scheduling_multithreaded_computations_by_work_stealing.pdf)
[^60]: Parallel Programming With OpenMP In C++ \- Matrix Multiplication ..., accessed October 7, 2025, [https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/](https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/)
[^61]: 5.2 Matrix Multiplication — Parallel Computing for Beginners \- Free Hands-On Materials for Learning PDC, accessed October 7, 2025, [https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html](https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html)
[^62]: Parallelized Blocked Matrix Multiplication using OpenMP | by Charith Pietersz \- Medium, accessed October 7, 2025, [https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47](https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47)
[^63]: OpenMP and cores/threads \- c++ \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/9292191/openmp-and-cores-threads](https://stackoverflow.com/questions/9292191/openmp-and-cores-threads)
[^64]: OpenMP thread mapping to physical cores \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores](https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores)
[^65]: Processor Affinity for OpenMP and MPI » ADMIN Magazine, accessed October 7, 2025, [https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI](https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI)
[^66]: How do I control threads and CPUs on Ookami using OpenMP? \- Stony Brook University, accessed October 7, 2025, [https://www.stonybrook.edu/commcms/ookami/support/faq/thread\_binding](https://www.stonybrook.edu/commcms/ookami/support/faq/thread_binding)
[^67]: Multicore processors and cache coherence | Intro to Computer Architecture Class Notes, accessed October 7, 2025, [https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH](https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH)
[^68]: Cache Coherence: How the MESI Protocol Keeps Multi-Core CPUs ..., accessed October 7, 2025, [https://dev.to/sachin\_tolay\_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j](https://dev.to/sachin_tolay_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j)
[^69]: Non-uniform memory access \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Non-uniform\_memory\_access](https://en.wikipedia.org/wiki/Non-uniform_memory_access)
[^70]: Improving Parallel System Performance with a NUMA-aware Load Balancer \- IDEALS, accessed October 7, 2025, [https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1](https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1)
[^71]: Mysteries of NUMA Memory Management Revealed \- Red Hat, accessed October 7, 2025, [https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed](https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed)
[^72]: CUDA C++ Programming Guide \- NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#hardware-implementation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)
[^73]: Matrix Multiplication in CUDA — A Simple Guide | by Charitha Saumya | Analytics Vidhya, accessed October 7, 2025, [https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab](https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab)
[^74]: MatrixMul sample \- CUDA Programming and Performance \- NVIDIA Developer Forums, accessed October 7, 2025, [https://forums.developer.nvidia.com/t/matrixmul-sample/58025](https://forums.developer.nvidia.com/t/matrixmul-sample/58025)