---
title: "9.1 Survey of Parallel Programming Models and Libraries"
description: "An overview of the major frameworks and paradigms for implementing parallel algorithms."
---
The field of parallel programming encompasses a diverse range of models, languages, and libraries. These have evolved over several decades to adapt to the changing landscape of computer architecture.[^1] To select the appropriate tool for a given task, one must understand the fundamental principles that differentiate these models. These distinctions are primarily driven by two fundamental dichotomies: how the underlying hardware organizes memory and how computational work is decomposed.[^4]

### **9.1.1 Foundational Dichotomies: Memory and Work Decomposition**

The design of a parallel programming model is significantly influenced by the machine's memory system architecture. This characteristic determines how processing elements communicate and shapes the programmer's approach to problem-solving.

#### **Shared vs. Distributed Memory Architectures**

Parallel computer architectures are broadly classified into two families based on how processors access memory.[^2]

| Architecture Type | Memory Organization | Communication Method | Key Advantages | Key Challenges | Examples |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Shared Memory** | Single global address space accessible by all processors[^3] | Implicit through shared variables[^5] | Simpler programming model; implicit communication | Race conditions; synchronization overhead; limited scalability due to memory bus contention[^2][^6] | Multi-core CPUs, desktops, laptops, servers[^7] |
| **Distributed Memory** | Each processor has private local memory[^3] | Explicit message passing[^3] | High scalability; adding nodes increases compute power and memory | Complex programming; must manage data partitioning, distribution, and communication[^3] | HPC clusters, supercomputers[^10] |
| **Distributed Shared Memory (DSM)** | Illusion of shared address space over distributed hardware[^3] | Appears shared but uses message passing underneath | Simpler than pure distributed memory | Hidden but present communication latency[^3] | Research systems, some commercial platforms |
| **PGAS (Partitioned Global Address Space)** | Logically partitioned global address space with processor affinity[^6] | Shared-memory-style reads/writes with locality awareness | Balance of programmability and performance; locality control | Requires careful data placement | UPC, Chapel, X10 languages |

> **The Fundamental Trade-off:** Shared-memory programs focus on **synchronization** (preventing race conditions through locks, semaphores, and atomic operations), while distributed-memory programs emphasize **data movement** (managing explicit communication between processors). Migrating an application from one model to the other necessitates complete re-evaluation of the algorithm's data structures and communication patterns.[^3][^5][^6]

#### **Data Parallelism vs. Task Parallelism**

The second fundamental dichotomy concerns how a problem is decomposed into parallel units of work.[^4]

| Parallelism Type | Definition | Source of Parallelism | Best Suited For | Common Examples | Architectural Alignment |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Data Parallelism** | Distribute large dataset across multiple processing units; apply same operation to each subset[^4] | The data, not the operations | Regularly structured data (arrays, matrices)[^6] | Vector processing in simulations; image filters applied to pixels; deep learning training on batches[^11] | SIMD, SPMD[^13] |
| **Task Parallelism** | Distribute distinct tasks across different processors for concurrent execution[^11] | Different operations/functions | Irregular or heterogeneous problems; workflows with dependencies[^13] | Pipelining; concurrent atmosphere and ocean models in climate simulation | MIMD[^4] |

> **Real-World Hybrid Approach:** Few applications are exclusively data-parallel or task-parallel. Most exhibit a combination of both at various program levels. A climate simulation might employ data parallelism to update grid cells within a time step (same operation, different data) and task parallelism to concurrently execute separate atmosphere and ocean models (different tasks).[^13]

### **9.1.2 Models for Shared Memory Systems**

Programming models for shared-memory systems utilize the global address space, primarily offering mechanisms to create concurrent threads of execution and manage their access to shared data. The development of these models demonstrates an ongoing effort to increase abstraction, thereby protecting the programmer from the complexities of direct thread management while maintaining high performance.

#### **Directive-Based Parallelism: OpenMP and the Fork-Join Model**

Open Multi-Processing (OpenMP) is the primary standard for high-level, directive-based parallel programming on shared-memory systems.[^16] OpenMP is a specification, not a library, implemented by compiler vendors (e.g., GCC, Intel C++ Compiler). It enables programmers to incrementally parallelize serial C, C++, and Fortran code using special preprocessor directives, known as pragmas.[^18]

> **The Fork-Join Execution Model:** An OpenMP program starts as a single *master thread*. When encountering a *parallel region*, it "forks," creating a team of parallel *worker threads*. All threads execute the region concurrently. At the end, worker threads "join" back with the master thread, which resumes serial execution until the next parallel region.[^18]

**Key Features of OpenMP:**

1. **Loop Parallelization:** Adding `#pragma omp parallel for` before a loop automatically distributes iterations among available threads[^18]
2. **Data Scope Control:** Clauses manage variable visibility (e.g., `private`, `shared`)[^16]
3. **Reduction Operations:** Built-in support for common reductions (e.g., `reduction(+:sum)`)[^16]
4. **Scheduling Control:** Various policies for iteration distribution among threads[^16]

> **OpenMP's Key Advantage:** Incremental parallelization—developers can achieve significant performance improvements on multi-core processors with minimal code changes, making it fundamental to HPC.[^7][^18]

#### **Explicit Threading: POSIX Threads (Pthreads)**

In contrast to OpenMP, POSIX Threads (Pthreads) represent a lower level of abstraction. Pthreads is a low-level C programming API, standardized by the IEEE, offering direct and explicit control over threads.[^7] This library-based approach requires including the `<pthread.h>` header and linking against the Pthreads library.[^9]
With Pthreads, the programmer manages all aspects of the thread lifecycle.[^9] New threads are created using the `pthread_create()` function, which specifies the function the new thread will execute.[^20] The main program can await a thread's completion using `pthread_join()`.[^9] Since all threads within a process share the same memory space, programmers must manually manage synchronization to prevent data races. Pthreads offers a suite of synchronization primitives, with the mutual exclusion lock, or *mutex* (`pthread_mutex_t`), being the most fundamental. A thread must acquire a lock using `pthread_mutex_lock()` before entering a critical section that accesses shared data and release it with `pthread_mutex_unlock()` upon exit, ensuring exclusive access to the critical section.[^9]
This model provides extensive control and flexibility, but at the cost of significant programming complexity. Developers must identify and manage all potential data races, synchronization, and manual work partitioning among threads. Consequently, while Pthreads is powerful for system-level programming, it is often too cumbersome for general application development, where higher-level models are typically preferred.[^7]

#### **Task-Based Parallelism: Intel Threading Building Blocks (oneTBB)**

Intel's Threading Building Blocks (now oneAPI Threading Building Blocks, or oneTBB) presents a modern C++ approach that balances the high-level simplicity of OpenMP with the explicit control of Pthreads.[^22] TBB is a C++ template library that abstracts raw threads, encouraging programmers to conceptualize work in terms of *tasks*.[^24]
TBB's philosophy promotes data-parallel programming using high-level parallel algorithms that operate on data collections, similar to the C++ Standard Template Library (STL).[^26] For instance, instead of manually creating threads for loop parallelization, a developer can use TBB's `tbb::parallel_for` algorithm, providing it with a range of indices and a C++ lambda function or function object that defines the work for each iteration.[^24]
The efficacy of TBB stems from its sophisticated runtime scheduler.[^27] In contrast to the often static scheduling of basic OpenMP, TBB utilizes a dynamic **work-stealing** scheduler to achieve automatic load balancing.[^23] In this model, each thread maintains its own task queue. When a thread becomes idle, it attempts to "steal" a task from the queue of another busy thread.[^23] This mechanism effectively manages applications with irregular or unpredictable task execution times, ensuring continuous processor core utilization without requiring manual programmer intervention.[^24] The combination of high-level C++ abstractions and an intelligent runtime positions TBB as a robust tool for developing complex, scalable parallel applications on shared-memory systems.

### **9.1.3 Models for Distributed Memory Systems**

When computation extends beyond a single machine to clusters of interconnected nodes, the shared-memory assumption becomes invalid. Programming models for these systems must address the physical reality of distributed memory, prioritizing data locality and explicit communication.

#### **Message Passing: The MPI Standard**

The Message Passing Interface (MPI) is the established standard for programming distributed-memory parallel computers, ranging from small clusters to the largest supercomputers globally.[^5] MPI is a specification for a library of functions, not a language itself, that can be invoked from C, C++, and Fortran to manage parallel processes and their communication.[^32]

> **MPI Fundamental Concepts:** An MPI program consists of independent processes, often on distinct physical nodes. Processes are organized into a **communicator** (default: `MPI_COMM_WORLD`). Each process has a unique integer **rank** (starting from 0) within its communicator.[^32]

Communication in MPI is explicit, relying on messages sent and received between processes identified by their ranks.[^32]

#### **MPI Communication Patterns**

| Type | Operations | Description | Use Case |
| :--- | :--- | :--- | :--- |
| **Point-to-Point** | `MPI_Send()`, `MPI_Recv()` | Single sender, single receiver. Sender packages data and transmits to destination rank; receiver blocks until message arrives[^31] | Direct data exchange between specific processes |
| **Broadcast** | `MPI_Bcast()` | One process sends identical data to all other processes[^31] | Distributing global parameters or configuration |
| **Scatter** | `MPI_Scatter()` | One process distributes distinct portions of an array to all processes[^31] | Partitioning data for parallel processing |
| **Gather** | `MPI_Gather()` | Inverse of Scatter—collects distinct data from all processes to one[^31] | Collecting results after parallel computation |
| **Reduction** | `MPI_Reduce()` | Combines data from all processes into single result using specified operation (e.g., sum, max)[^31] | Computing global aggregates like totals or maximums |

> **MPI's Design Philosophy:** By requiring explicit management of data placement and inter-process communication, MPI offers maximum control over parallel execution in distributed systems. Though more complex than shared-memory models, its portability and scalability have made it fundamental to high-performance computing.[^32]

### **9.1.4 Models for Heterogeneous and Accelerator-Based Systems**

The emergence of specialized hardware accelerators, particularly Graphics Processing Units (GPUs), has added a new dimension to parallel programming. These devices provide significant computational power but have distinct architectures and memory systems, requiring specialized programming models to manage the complex interaction between a host CPU and an accelerator device.

#### **GPU Computing: NVIDIA's CUDA and the OpenCL Standard**

The evolution of GPUs from fixed-function graphics pipelines to fully programmable parallel processors for general-purpose computing (GPGPU) marked a significant turning point in HPC.[^7]

**CUDA (Compute Unified Device Architecture)**

Developed by NVIDIA, CUDA is a proprietary parallel computing platform and programming model for its GPUs.[^34] It extends C++ with language extensions and a runtime library.

> **CUDA's Heterogeneous Model:** Explicitly based on interaction between a **host** (CPU) and **devices** (GPUs). Host and device have separate memory spaces (system RAM and GPU VRAM), requiring explicit data transfers via `cudaMemcpy()`. Parallel computations are written as **kernels** (designated by `__global__`), launched from the host with syntax specifying parallel thread count.[^36][^37]

| Component | Role | Key Operations |
| :--- | :--- | :--- |
| **Host (CPU)** | Orchestrates computation; manages data transfers | Allocate GPU memory, transfer data, launch kernels |
| **Device (GPU)** | Executes massively parallel computations | Run thousands of threads executing kernel code |
| **Kernel** | GPU function executed by many threads in parallel | Designated by `__global__`; launched with `<<<...>>>` syntax[^37] |

**OpenCL (Open Computing Language)**

In response to demand for a vendor-neutral standard, the Khronos Group developed OpenCL.[^40]

> **OpenCL's Portability Promise:** Open, royalty-free standard for writing programs that execute on various platforms (CPUs, GPUs, DSPs, FPGAs). Theoretically, an OpenCL program can run on any compliant hardware from any vendor.[^40][^42]

| Feature | CUDA | OpenCL |
| :--- | :--- | :--- |
| **Vendor** | NVIDIA (proprietary) | Khronos Group (open standard) |
| **Hardware Support** | NVIDIA GPUs only | CPUs, GPUs, DSPs, FPGAs from multiple vendors[^40] |
| **Programming Model** | Host-device with explicit memory management | Host-device with context, command queues, kernels[^42][^43] |
| **Kernel Language** | CUDA C/C++ | OpenCL C (C99-based dialect)[^42] |
| **Key Advantage** | Highly optimized for NVIDIA hardware; mature ecosystem[^7] | True cross-platform portability[^40] |
| **Trade-off** | Vendor lock-in | May sacrifice performance for genericity[^41] |

#### **Modern C++ Abstractions: SYCL**

The complexity of managing separate host and device codebases in CUDA and OpenCL prompted the development of higher-level abstractions. SYCL (pronounced "sickle") is a royalty-free, cross-platform abstraction layer from the Khronos Group that allows code for heterogeneous processors to be written in a **single-source** style using standard C++.[^46]

> **SYCL's Single-Source Innovation:** Both host and device code in the same source file, using modern C++ features (classes, templates, lambda functions). The SYCL runtime automatically manages kernel execution and data transfers through buffer and accessor objects, abstracting hardware complexity from the developer.[^46][^47]

**SYCL Architecture:**

| Component | Function |
| :--- | :--- |
| **Queue Object** | Submits work to compute devices[^47] |
| **Command Group** | Encapsulates kernel and data dependencies (defined in lambda)[^47] |
| **Buffer & Accessor Objects** | Describe and manage data transfers automatically[^46] |
| **Backend** | Operates on top of OpenCL, CUDA, or other compute APIs[^46] |

This single-source approach enhances programmability and code maintainability for complex heterogeneous systems, representing a significant step in abstracting hardware complexity from the developer.[^46]

### **9.1.5 Comparative Analysis of Programming Models**

The choice of a parallel programming model requires balancing trade-offs between performance, programmability, portability, and the target hardware. The evolution from low-level models such as Pthreads, which provide maximum control at the cost of high complexity, to high-level abstractions like SYCL, which prioritize programmer productivity and portability, directly addresses the increasing complexity of parallel hardware.[^7] This progression is a necessary adaptation to make parallel programming more accessible. The optimal model is context-dependent, determined by the application's specific requirements, the available hardware, and development resources. The following table summarizes the key models discussed, outlining their defining characteristics and typical use cases.

| Model | Primary Memory Model | Parallelism Type | Abstraction Level | Primary Use Case | Programmability |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Pthreads** | Shared | Task | Low | Fine-grained thread control, systems programming | Low |
| **OpenMP** | Shared | Data / Task | High | Incremental parallelization of loops on multi-core CPUs | High |
| **oneTBB** | Shared | Data / Task | High | C++ task-based parallelism with dynamic load balancing | High |
| **MPI** | Distributed | Data / Task | Low | Large-scale cluster and supercomputer programming | Low |
| **CUDA** | Host-Device | Data | Medium | High-performance computing on NVIDIA GPUs | Medium |
| **OpenCL** | Host-Device | Data / Task | Medium | Portable parallel programming for heterogeneous accelerators | Medium |
| **SYCL** | Host-Device | Data / Task | High | Single-source C++ for portable heterogeneous programming | High |

## References
[^1]: Models for Parallel Computing : Review and Perspectives \- Semantic Scholar, accessed October 7, 2025, [https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29](https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29)
[^2]: Shared Versus Distributed Memory Multiprocessors \- ECMWF, accessed October 7, 2025, [https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf](https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf)
[^3]: Distributed memory \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Distributed\_memory](https://en.wikipedia.org/wiki/Distributed_memory)
[^4]: Insights on Parallel Programming Model \- Advanced Millennium Technologies, accessed October 7, 2025, [https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/](https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/)
[^5]: How Parallel Processing Shaped Modern Computing \- CelerData, accessed October 7, 2025, [https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing](https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing)
[^6]: Parallel programming model \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_programming\_model](https://en.wikipedia.org/wiki/Parallel_programming_model)
[^7]: The Evolution of Parallel Programming | by Tiwariabhinav | Medium, accessed October 7, 2025, [https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88](https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88)
[^8]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 7, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)
[^9]: Multithreaded Programming (POSIX pthreads Tutorial) \- randu.org, accessed October 7, 2025, [https://randu.org/tutorials/threads/](https://randu.org/tutorials/threads/)
[^10]: Parallel Computing at a Glance, accessed October 7, 2025, [http://www.buyya.com/microkernel/chap1.pdf](http://www.buyya.com/microkernel/chap1.pdf)
[^11]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 7, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)
[^12]: Scalable Parallel Processing: Architectural Models, Real-Time Programming, and Performance Evaluation \- MDPI, accessed October 7, 2025, [https://www.mdpi.com/2673-4591/104/1/60](https://www.mdpi.com/2673-4591/104/1/60)
[^13]: Task parallelism \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Task\_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)
[^14]: Parallel computing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_computing](https://en.wikipedia.org/wiki/Parallel_computing)
[^15]: What is a good example for task parallelismn in data processing? \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing](https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing)
[^16]: Using OpenMP with C — Research Computing University of ..., accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html](https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html)
[^17]: C++ Examples of Parallel Programming with OpenMP, accessed October 7, 2025, [https://people.math.sc.edu/burkardt/cpp\_src/openmp/openmp.html](https://people.math.sc.edu/burkardt/cpp_src/openmp/openmp.html)
[^18]: An introduction to OpenMP \- University College London, accessed October 7, 2025, [https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02\_intro\_openmp.html](https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02_intro_openmp.html)
[^19]: OpenMP Tutorial \- Scalable Parallel Computing Lab, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf](https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf)
[^20]: C++ Tutorial: Multi-Threaded Programming \- C++ Class Thread for Pthreads \- 2020 \- BogoToBogo, accessed October 7, 2025, [https://www.bogotobogo.com/cplusplus/multithreading\_pthread.php](https://www.bogotobogo.com/cplusplus/multithreading_pthread.php)
[^21]: Linux Tutorial: POSIX Threads, accessed October 7, 2025, [https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html](https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html)
[^22]: Getting Started with Intel® Threading Building Blocks (Intel® TBB), accessed October 7, 2025, [https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html](https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html)
[^23]: Threading Building Blocks \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Threading\_Building\_Blocks](https://en.wikipedia.org/wiki/Threading_Building_Blocks)
[^24]: Introduction to the Intel Threading Building Blocks — mcs572 0.7.8 documentation, accessed October 7, 2025, [http://homepages.math.uic.edu/\~jan/mcs572f16/mcs572notes/lec11.html](http://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec11.html)
[^25]: Threading Building Blocks \- Tools \- EBRAINS, accessed October 7, 2025, [https://www.ebrains.eu/tools/threading-building-blocks](https://www.ebrains.eu/tools/threading-building-blocks)
[^26]: TBB Tutorial \- cs.wisc.edu, accessed October 7, 2025, [https://pages.cs.wisc.edu/\~gibson/tbbTutorial.html](https://pages.cs.wisc.edu/~gibson/tbbTutorial.html)
[^27]: Introducing Intel Tbb \- Agenda INFN, accessed October 7, 2025, [https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb\_Introduction.pdf](https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb_Introduction.pdf)
[^28]: Intel(R) Threading Building Blocks Getting Started Guide, accessed October 7, 2025, [http://www.physics.ntua.gr/\~konstant/HetCluster/intel12.1/tbb/Getting\_Started.pdf](http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/tbb/Getting_Started.pdf)
[^29]: Work stealing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Work\_stealing](https://en.wikipedia.org/wiki/Work_stealing)
[^30]: Towards Efficient HPC: Exploring Overlap Strategies Using MPI Non ..., accessed October 7, 2025, [https://www.mdpi.com/2227-7390/13/11/1848](https://www.mdpi.com/2227-7390/13/11/1848)
[^31]: Tutorials · MPI Tutorial, accessed October 7, 2025, [https://mpitutorial.com/tutorials/](https://mpitutorial.com/tutorials/)
[^32]: MPI Tutorial – Part 1, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf](https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf)
[^33]: Using MPI with C \- CU Research Computing User Guide, accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/MPI-C.html](https://curc.readthedocs.io/en/latest/programming/MPI-C.html)
[^34]: (PDF) Review of Architecture and Model for Parallel Programming \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/372822306\_Review\_of\_Architecture\_and\_Model\_for\_Parallel\_Programming](https://www.researchgate.net/publication/372822306_Review_of_Architecture_and_Model_for_Parallel_Programming)
[^35]: mikeroyal/CUDA-Guide \- GitHub, accessed October 7, 2025, [https://github.com/mikeroyal/CUDA-Guide](https://github.com/mikeroyal/CUDA-Guide)
[^36]: CUDA C++ Programming Guide | NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/pdf/CUDA\_C\_Programming\_Guide.pdf](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
[^37]: CUDA Programming Guide: An Overview | by Zia Babar | Aug, 2025 \- Medium, accessed October 7, 2025, [https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8](https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8)
[^38]: Matrix Multiplication in CUDA \- Harshit Kumar, accessed October 7, 2025, [https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda](https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda)
[^39]: What is NVIDIA CUDA? What can it do for Deep Learning? \- Exxact Corporation, accessed October 7, 2025, [https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning](https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning)
[^40]: OpenCL \- The Open Standard for Parallel Programming of ..., accessed October 7, 2025, [https://www.khronos.org/opencl/](https://www.khronos.org/opencl/)
[^41]: An introduction to OpenCL \- Purdue Engineering, accessed October 7, 2025, [https://engineering.purdue.edu/\~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf](https://engineering.purdue.edu/~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf)
[^42]: OpenCL \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/OpenCL](https://en.wikipedia.org/wiki/OpenCL)
[^43]: OpenCL execution model \- Arm Developer, accessed October 7, 2025, [https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model](https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model)
[^44]: The OpenCL™ C Specification, accessed October 7, 2025, [https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL\_C.pdf](https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL_C.pdf)
[^45]: The OpenCL Specification \- IC-Unicamp, accessed October 7, 2025, [https://www.ic.unicamp.br/\~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf](https://www.ic.unicamp.br/~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf)
[^46]: Heterogeneous programming with SYCL documentation, accessed October 7, 2025, [https://enccs.github.io/sycl-workshop/](https://enccs.github.io/sycl-workshop/)
[^47]: Getting Started \- SYCL.tech, accessed October 7, 2025, [https://sycl.tech/getting-started/](https://sycl.tech/getting-started/)
[^48]: Compiler for Parallel Machines. Parallel computing is an approach ..., accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf)
[^49]: The Nexus Task-parallel Runtime System, accessed October 7, 2025, [https://marketing.globuscs.info/production/strapi/uploads/india\_paper\_ps\_40c2982c84.pdf](https://marketing.globuscs.info/production/strapi/uploads/india_paper_ps_40c2982c84.pdf)
[^50]: medium.com, accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf\#:\~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf#:~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.)
[^51]: en.wikipedia.org, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Automatic\_parallelization\_tool\#:\~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization\&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.](https://en.wikipedia.org/wiki/Automatic_parallelization_tool#:~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.)
[^52]: Auto-parallelization Overview, accessed October 7, 2025, [https://www.cita.utoronto.ca/\~merz/intel\_c10b/main\_cls/mergedProjects/optaps\_cls/common/optaps\_qpar\_par.htm](https://www.cita.utoronto.ca/~merz/intel_c10b/main_cls/mergedProjects/optaps_cls/common/optaps_qpar_par.htm)
[^53]: Compilers for Parallel Machines: A User-Friendly Guide | by Pranay Junghare | Medium, accessed October 7, 2025, [https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6](https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6)
[^54]: Pragma directives for OpenMP parallelization \- IBM, accessed October 7, 2025, [https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization)
[^55]: Directive Format \- OpenMP, accessed October 7, 2025, [https://www.openmp.org/spec-html/5.1/openmpse9.html](https://www.openmp.org/spec-html/5.1/openmpse9.html)
[^56]: What is parallel computing? \- IBM, accessed October 7, 2025, [https://www.ibm.com/think/topics/parallel-computing](https://www.ibm.com/think/topics/parallel-computing)
[^57]: Parallel Programming Models, Languages and Compilers \- csenotes, accessed October 7, 2025, [https://csenotes.github.io/pdf/mod5\_aca.pdf](https://csenotes.github.io/pdf/mod5_aca.pdf)
[^58]: (PDF) A Comparative Study and Evaluation of Parallel Programming ..., accessed October 7, 2025, [https://www.researchgate.net/publication/255791855\_A\_Comparative\_Study\_and\_Evaluation\_of\_Parallel\_Programming\_Models\_for\_Shared-Memory\_Parallel\_Architectures](https://www.researchgate.net/publication/255791855_A_Comparative_Study_and_Evaluation_of_Parallel_Programming_Models_for_Shared-Memory_Parallel_Architectures)
[^59]: Scheduling Multithreaded Computations by Work Stealing, accessed October 7, 2025, [https://www.csd.uwo.ca/\~mmorenom/CS433-CS9624/Resources/Scheduling\_multithreaded\_computations\_by\_work\_stealing.pdf](https://www.csd.uwo.ca/~mmorenom/CS433-CS9624/Resources/Scheduling_multithreaded_computations_by_work_stealing.pdf)
[^60]: Parallel Programming With OpenMP In C++ \- Matrix Multiplication ..., accessed October 7, 2025, [https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/](https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/)
[^61]: 5.2 Matrix Multiplication — Parallel Computing for Beginners \- Free Hands-On Materials for Learning PDC, accessed October 7, 2025, [https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html](https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html)
[^62]: Parallelized Blocked Matrix Multiplication using OpenMP | by Charith Pietersz \- Medium, accessed October 7, 2025, [https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47](https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47)
[^63]: OpenMP and cores/threads \- c++ \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/9292191/openmp-and-cores-threads](https://stackoverflow.com/questions/9292191/openmp-and-cores-threads)
[^64]: OpenMP thread mapping to physical cores \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores](https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores)
[^65]: Processor Affinity for OpenMP and MPI » ADMIN Magazine, accessed October 7, 2025, [https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI](https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI)
[^66]: How do I control threads and CPUs on Ookami using OpenMP? \- Stony Brook University, accessed October 7, 2025, [https://www.stonybrook.edu/commcms/ookami/support/faq/thread\_binding](https://www.stonybrook.edu/commcms/ookami/support/faq/thread_binding)
[^67]: Multicore processors and cache coherence | Intro to Computer Architecture Class Notes, accessed October 7, 2025, [https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH](https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH)
[^68]: Cache Coherence: How the MESI Protocol Keeps Multi-Core CPUs ..., accessed October 7, 2025, [https://dev.to/sachin\_tolay\_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j](https://dev.to/sachin_tolay_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j)
[^69]: Non-uniform memory access \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Non-uniform\_memory\_access](https://en.wikipedia.org/wiki/Non-uniform_memory_access)
[^70]: Improving Parallel System Performance with a NUMA-aware Load Balancer \- IDEALS, accessed October 7, 2025, [https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1](https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1)
[^71]: Mysteries of NUMA Memory Management Revealed \- Red Hat, accessed October 7, 2025, [https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed](https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed)
[^72]: CUDA C++ Programming Guide \- NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#hardware-implementation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)
[^73]: Matrix Multiplication in CUDA — A Simple Guide | by Charitha Saumya | Analytics Vidhya, accessed October 7, 2025, [https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab](https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab)
[^74]: MatrixMul sample \- CUDA Programming and Performance \- NVIDIA Developer Forums, accessed October 7, 2025, [https://forums.developer.nvidia.com/t/matrixmul-sample/58025](https://forums.developer.nvidia.com/t/matrixmul-sample/58025)