---
title: "9.3 Mapping Constructs to Hardware: An Example"
description: "A detailed example of how high-level parallel constructs are mapped to underlying hardware architectures."
---
The theoretical concepts of programming models, compilers, and runtimes become more concrete when tracing a specific algorithm's path from source code to execution on physical hardware. By examining how a standard parallel problem—matrix multiplication—is implemented on two distinct architectures (a multi-core CPU and a many-core GPU), we can highlight the significant influence of hardware design on software strategy. The most efficient parallelization approach is not an inherent property of the algorithm but is determined by the architectural characteristics of the target machine.

### **9.3.1 The Canonical Problem: Matrix Multiplication**

The multiplication of two matrices, $C = A \times B$, is a fundamental operation in linear algebra and is widely used in scientific and engineering domains.[^60] The computation of each element $C_{i,j}$ involves the dot product of the $i$-th row of matrix A and the $j$-th column of matrix B:[^61]

$$C_{i,j} = \sum_{k=0}^{N-1} A_{i,k} \times B_{k,j}$$

> **Embarrassingly Parallel:** The calculation for each element $C_{i,j}$ is independent of the calculation for any other element. This property makes matrix multiplication an ideal test case for demonstrating parallel implementation techniques.[^61]

**Key Characteristics:**

| Property | Value | Implication |
| :--- | :--- | :--- |
| **Computational Complexity** | $O(N^3)$ for $N \times N$ matrices[^62] | Computationally intensive; significant parallelization benefit |
| **Data Independence** | Each $C_{i,j}$ calculated independently | Perfect for parallel execution |
| **Parallelization Potential** | "Embarrassingly parallel"[^61] | Multiple viable parallelization strategies across architectures |

### **9.3.2 Mapping to a Multi-Core CPU with OpenMP**

For a standard shared-memory multi-core CPU, OpenMP offers a direct and efficient method for parallelization.

#### **The Code**

A standard C++ implementation of matrix multiplication uses three nested loops. Parallelizing this with OpenMP is straightforward: a single directive, `#pragma omp parallel for`, is placed before the outermost loop. This instructs the compiler to distribute the iterations of the `i` loop—which correspond to the rows of the output matrix C—among a team of parallel threads.[^60]

C++

```cpp
// C, A, and B are N x N matrices
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        double sum = 0.0;
        for (int k = 0; k < N; k++) {
            sum += A[i][k] * B[k][j];
        }
        C[i][j] = sum;
    }
}
```

#### **Logical-to-Physical Mapping**

The process from this pragma to hardware execution requires precise collaboration between the compiler, the OpenMP runtime, and the operating system:

| Stage | Component | Action | Result |
| :--- | :--- | :--- | :--- |
| **1. Compile-Time** | Compiler | Transforms code upon encountering `#pragma omp parallel for`[^54] | Generates function for loop body; inserts OpenMP runtime calls |
| **2. Runtime Initialization** | OpenMP Runtime | Creates team of software threads at program execution[^18] | Typically one thread per available CPU core |
| **3. Thread-to-Core Mapping** | OS Scheduler + OpenMP Runtime | Maps software threads to physical CPU cores using affinity policies[^64] | Threads bound to specific cores based on policy |

> **Thread Affinity:** The mapping of software threads to physical CPU cores, critical for performance. Programmers control this via environment variables: `OMP_PLACES` defines available hardware resources (cores, sockets); `OMP_PROC_BIND` specifies placement policy.[^64][^65]

**Thread Affinity Policies:**

| Policy | Behavior | Best For | Example Use Case |
| :--- | :--- | :--- | :--- |
| **close** | Pack threads onto cores within a single socket before using another[^65] | Applications with high data sharing | Threads frequently access same cache lines |
| **spread** | Distribute threads as widely as possible across all sockets and cores[^65] | Applications with independent threads | Maximize aggregate memory bandwidth |
| **master** | Threads colocate near the master thread | NUMA-aware applications | Minimize remote memory access |

#### **Hardware Considerations in Execution**

Once threads are executing on cores, their performance is determined by the complex behavior of the CPU's memory subsystem.

> **The Memory Abstraction Gap:** The programmer's view of memory as a simple, flat, shared space is a useful abstraction, but it is built upon a complex physical reality involving caches, coherence protocols, and non-uniform memory access.

**Cache Coherence**

Each CPU core has private L1 and L2 caches to minimize memory access latency.[^67] When multiple threads on different cores compute rows of matrix C, they all write to a shared data structure, introducing a **cache coherence** problem.

> **The Coherence Problem:** A core's private cache may hold a "stale" copy of a memory location that has been updated by another core. Multi-core CPUs implement hardware cache coherence protocols (typically **MESI**: Modified, Exclusive, Shared, Invalid) to maintain consistency.[^67][^68]

**MESI Protocol Operation:**

1. Caches "snoop" on a shared bus
2. When one core writes to a cache line, it broadcasts an invalidation signal
3. Other cores with that line mark their copies as invalid
4. Next access by another core causes cache miss → fetch updated version

> **False Sharing:** A performance hazard where threads write to nearby memory locations on the same cache line, causing excessive invalidation traffic despite no logical data sharing.[^67]

**Non-Uniform Memory Access (NUMA)**

In multi-socket systems, shared memory is physically distributed.

| Memory Type | Access Latency | Scenario |
| :--- | :--- | :--- |
| **Local Memory** | Low (fast) | Core accesses memory bank on its own socket[^69] |
| **Remote Memory** | High (slow) | Core accesses memory bank on different socket[^69] |

> **NUMA Impact:** If a thread on Socket 0 computes a row of C whose data is allocated in Socket 1's memory, every access incurs the higher latency of traversing the inter-socket interconnect. NUMA-aware scheduling (e.g., `OMP_PROC_BIND=close`) co-locates threads and data on the same NUMA node, minimizing remote access penalties.[^69][^70]

### **9.3.3 Mapping to a GPU with CUDA**

Mapping the same matrix multiplication problem to a GPU demonstrates a completely different architectural philosophy and, as a result, a different parallelization strategy.

#### **The Code**

A CUDA implementation includes two parts: host code that runs on the CPU and a kernel that runs on the GPU. The host is responsible for allocating memory on both the host and the device, transferring the input matrices A and B to the GPU, launching the kernel, and transferring the resulting matrix C back from the GPU.[^37]

```cpp
#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>

__global__ void matrixMulKernel(float* A, float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

int main() {
    const int N = 1024;
    size_t size = N * N * sizeof(float);

    float *h_A = (float*)malloc(size);
    float *h_B = (float*)malloc(size);
    float *h_C = (float*)malloc(size);

    for (int i = 0; i < N * N; i++) {
        h_A[i] = 1.0f;
        h_B[i] = 2.0f;
    }

    float *d_A, *d_B, *d_C;
    cudaMalloc((void**)&d_A, size);
    cudaMalloc((void**)&d_B, size);
    cudaMalloc((void**)&d_C, size);

    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks((N + 15) / 16, (N + 15) / 16);
    matrixMulKernel<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);

    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    printf("Result: C[0][0] = %.2f\n", h_C[0]);

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
    free(h_A);
    free(h_B);
    free(h_C);

    return 0;
}
```

**Compilation:**
```bash
nvcc -o matrix_mul matrix_mul.cu
./matrix_mul
```

#### **Logical-to-Physical Mapping**

The CUDA mapping is hierarchical and explicitly defined by the programmer, which translates directly to GPU hardware structures:[^72]

| Hierarchy Level | Logical Construct | Physical Hardware | Mapping Details |
| :--- | :--- | :--- | :--- |
| **1. Grid** | Grid of Thread Blocks | Entire GPU | Programmer organizes computation into 2D/3D grid; each thread computes one output element[^38] |
| **2. Block** | Thread Block | Streaming Multiprocessor (SM) | CUDA runtime distributes blocks to SMs; multiple blocks can reside on one SM[^37] |
| **3. Warp** | Group of 32 Threads | Warp Scheduler + CUDA Cores | Hardware groups threads into warps; SM's Warp Scheduler issues instructions[^37] |

> **Single Instruction, Multiple Thread (SIMT):** All 32 threads in a warp execute the same instruction simultaneously but on different data. If a warp waits for memory, the scheduler instantly switches to another resident warp, hiding latency and keeping computational units busy.[^37]

**CUDA Execution Hierarchy:**

```
Grid (entire GPU)
└── Thread Blocks (mapped to SMs)
    └── Warps (32 threads, scheduled by SM)
        └── Individual Threads (execute on CUDA cores)
```

**Key GPU Architecture Concepts:**

| Component | Role | Count/Capability |
| :--- | :--- | :--- |
| **Streaming Multiprocessor (SM)** | Core processing unit of GPU[^37] | Multiple per GPU; runs multiple blocks concurrently |
| **Warp** | Hardware thread grouping[^37] | 32 threads; execute in lockstep (SIMT) |
| **Warp Scheduler** | Selects warps for execution[^37] | Hides memory latency via rapid context switching |
| **CUDA Core** | Arithmetic logic unit | Hundreds per SM; execute thread instructions |

#### **Hardware Considerations in Execution**

The GPU's performance is closely tied to its explicit and non-uniform memory hierarchy.

> **CPU vs. GPU Memory Philosophy:** The CPU attempts to hide memory complexity through automatic caching. The GPU exposes its memory hierarchy to the programmer, who must manage it explicitly to achieve high performance.[^36][^37]

**GPU Memory Hierarchy:**

| Memory Type | Scope | Speed | Size | Programmer Control | Typical Use |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Registers** | Per-thread private | Fastest | Very small | Automatic (compiler-managed) | Loop variables, temporaries |
| **Shared Memory** | Per-block shared | Very fast (on-chip) | Small (KB per SM) | Explicit | Cooperative data sharing; programmer-managed cache[^37] |
| **Global Memory** | All threads | Slow (high latency) | Large (GB DRAM)[^36] | Explicit | Input/output data; main data storage |
| **Constant Memory** | Read-only, all threads | Fast (cached) | Small | Explicit | Read-only lookup tables |
| **Texture Memory** | Read-only, all threads | Fast (cached, optimized for 2D locality) | Large | Explicit | Image processing |

> **The Primary GPU Bottleneck:** Global memory access. The naive kernel performs numerous accesses to slow global memory, making it highly inefficient. The solution is **tiling**.[^38]

**Optimization via Tiling:**

1. **Problem:** Each thread performs many slow global memory accesses
2. **Solution:** Thread block cooperatively loads sub-matrices (tiles) of A and B from global memory into fast shared memory
3. **Benefit:** Threads compute using data exclusively from shared memory
4. **Result:** Dramatically reduced global memory traffic; maximized data reuse in fast memory[^38]

> **Effective CUDA Programming:** Structure algorithms to maximize computation on data in registers and shared memory while minimizing global memory access. This optimization strategy defines the difference between naive and high-performance GPU code.[^38]

### **9.3.4 Synthesis: Contrasting CPU and GPU Mapping**

The parallelization of matrix multiplication on a CPU versus a GPU highlights fundamental differences in their architectural philosophies.

> **Architectural Optimization Goals:** The CPU is optimized for **low-latency execution** of a few complex tasks. The GPU is optimized for **high-throughput execution** of many simple tasks. This fundamental difference drives entirely different mapping strategies, memory considerations, and optimization goals.

**Key Architectural Contrasts:**

| Dimension | Multi-Core CPU | Many-Core GPU |
| :--- | :--- | :--- |
| **Unit of Parallelism** | Heavyweight thread (OS/runtime-managed); small number mapped to powerful cores | Lightweight thread (hardware-managed in warps); thousands of threads |
| **Scheduling Approach** | Complex software-based (OS + runtime); sophisticated preemption and work-stealing for fairness/load balancing | Simple hardware-based warp scheduler; rapid context switching to hide memory latency |
| **Memory Philosophy** | Hardware complexity (large caches, coherence protocols) provides illusion of uniform, coherent shared memory | Exposes physical memory hierarchy; requires explicit programmer management for performance |
| **Optimization Focus** | Maximize cache hits; ensure NUMA-local access; minimize synchronization overhead | Maximize shared memory use; minimize global memory access; maximize warp occupancy |
| **Parallelism Granularity** | Coarse (rows/chunks of matrix) | Fine (individual matrix elements) |

> **The Architecture-Algorithm Dependency:** An algorithm optimized for one architecture can be highly inefficient on another. The optimal parallelization method is not universal but architecture-specific—it answers: "What is the most effective way to map this computation onto the resources and constraints of this particular hardware?"

These contrasts demonstrate why portable performance across diverse hardware remains a fundamental challenge in parallel computing. The same algorithm requires radically different implementations to achieve efficiency on different architectures.

| Feature | Multi-Core CPU (with OpenMP) | Many-Core GPU (with CUDA) |
| :--- | :--- | :--- |
| **Programming Construct** | `#pragma omp parallel for` | `__global__` kernel launch `<<<...>>>` |
| **Logical Parallel Unit** | Loop Iteration (e.g., a matrix row) | Single Data Element (e.g., a matrix cell) |
| **Physical Execution Unit** | OS Thread mapped to a CPU Core | CUDA Thread, executed in a Warp of 32 |
| **Scheduling** | OS/Runtime scheduler (preemptive, work-stealing possible) | Hardware Warp Scheduler (non-preemptive, latency hiding) |
| **Key Memory Challenge** | Cache Coherence (MESI), NUMA locality | Global Memory Latency |
| **Optimization Goal** | Maximize cache hits, ensure NUMA-local access | Maximize use of Shared Memory, minimize global memory access |

## References
[^1]: Models for Parallel Computing : Review and Perspectives \- Semantic Scholar, accessed October 7, 2025, [https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29](https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29)
[^2]: Shared Versus Distributed Memory Multiprocessors \- ECMWF, accessed October 7, 2025, [https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf](https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf)
[^3]: Distributed memory \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Distributed\_memory](https://en.wikipedia.org/wiki/Distributed_memory)
[^4]: Insights on Parallel Programming Model \- Advanced Millennium Technologies, accessed October 7, 2025, [https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/](https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/)
[^5]: How Parallel Processing Shaped Modern Computing \- CelerData, accessed October 7, 2025, [https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing](https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing)
[^6]: Parallel programming model \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_programming\_model](https://en.wikipedia.org/wiki/Parallel_programming_model)
[^7]: The Evolution of Parallel Programming | by Tiwariabhinav | Medium, accessed October 7, 2025, [https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88](https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88)
[^8]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 7, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)
[^9]: Multithreaded Programming (POSIX pthreads Tutorial) \- randu.org, accessed October 7, 2025, [https://randu.org/tutorials/threads/](https://randu.org/tutorials/threads/)
[^10]: Parallel Computing at a Glance, accessed October 7, 2025, [http://www.buyya.com/microkernel/chap1.pdf](http://www.buyya.com/microkernel/chap1.pdf)
[^11]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 7, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)
[^12]: Scalable Parallel Processing: Architectural Models, Real-Time Programming, and Performance Evaluation \- MDPI, accessed October 7, 2025, [https://www.mdpi.com/2673-4591/104/1/60](https://www.mdpi.com/2673-4591/104/1/60)
[^13]: Task parallelism \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Task\_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)
[^14]: Parallel computing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_computing](https://en.wikipedia.org/wiki/Parallel_computing)
[^15]: What is a good example for task parallelismn in data processing? \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing](https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing)
[^16]: Using OpenMP with C — Research Computing University of ..., accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html](https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html)
[^17]: C++ Examples of Parallel Programming with OpenMP, accessed October 7, 2025, [https://people.math.sc.edu/burkardt/cpp\_src/openmp/openmp.html](https://people.math.sc.edu/burkardt/cpp_src/openmp/openmp.html)
[^18]: An introduction to OpenMP \- University College London, accessed October 7, 2025, [https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02\_intro\_openmp.html](https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02_intro_openmp.html)
[^19]: OpenMP Tutorial \- Scalable Parallel Computing Lab, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf](https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf)
[^20]: C++ Tutorial: Multi-Threaded Programming \- C++ Class Thread for Pthreads \- 2020 \- BogoToBogo, accessed October 7, 2025, [https://www.bogotobogo.com/cplusplus/multithreading\_pthread.php](https://www.bogotobogo.com/cplusplus/multithreading_pthread.php)
[^21]: Linux Tutorial: POSIX Threads, accessed October 7, 2025, [https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html](https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html)
[^22]: Getting Started with Intel® Threading Building Blocks (Intel® TBB), accessed October 7, 2025, [https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html](https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html)
[^23]: Threading Building Blocks \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Threading\_Building\_Blocks](https://en.wikipedia.org/wiki/Threading_Building_Blocks)
[^24]: Introduction to the Intel Threading Building Blocks — mcs572 0.7.8 documentation, accessed October 7, 2025, [http://homepages.math.uic.edu/\~jan/mcs572f16/mcs572notes/lec11.html](http://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec11.html)
[^25]: Threading Building Blocks \- Tools \- EBRAINS, accessed October 7, 2025, [https://www.ebrains.eu/tools/threading-building-blocks](https://www.ebrains.eu/tools/threading-building-blocks)
[^26]: TBB Tutorial \- cs.wisc.edu, accessed October 7, 2025, [https://pages.cs.wisc.edu/\~gibson/tbbTutorial.html](https://pages.cs.wisc.edu/~gibson/tbbTutorial.html)
[^27]: Introducing Intel Tbb \- Agenda INFN, accessed October 7, 2025, [https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb\_Introduction.pdf](https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb_Introduction.pdf)
[^28]: Intel(R) Threading Building Blocks Getting Started Guide, accessed October 7, 2025, [http://www.physics.ntua.gr/\~konstant/HetCluster/intel12.1/tbb/Getting\_Started.pdf](http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/tbb/Getting_Started.pdf)
[^29]: Work stealing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Work\_stealing](https://en.wikipedia.org/wiki/Work_stealing)
[^30]: Towards Efficient HPC: Exploring Overlap Strategies Using MPI Non ..., accessed October 7, 2025, [https://www.mdpi.com/2227-7390/13/11/1848](https://www.mdpi.com/2227-7390/13/11/1848)
[^31]: Tutorials · MPI Tutorial, accessed October 7, 2025, [https://mpitutorial.com/tutorials/](https://mpitutorial.com/tutorials/)
[^32]: MPI Tutorial – Part 1, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf](https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf)
[^33]: Using MPI with C \- CU Research Computing User Guide, accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/MPI-C.html](https://curc.readthedocs.io/en/latest/programming/MPI-C.html)
[^34]: (PDF) Review of Architecture and Model for Parallel Programming \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/372822306\_Review\_of\_Architecture\_and\_Model\_for\_Parallel\_Programming](https://www.researchgate.net/publication/372822306_Review_of_Architecture_and_Model_for_Parallel_Programming)
[^35]: mikeroyal/CUDA-Guide \- GitHub, accessed October 7, 2025, [https://github.com/mikeroyal/CUDA-Guide](https://github.com/mikeroyal/CUDA-Guide)
[^36]: CUDA C++ Programming Guide | NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/pdf/CUDA\_C\_Programming\_Guide.pdf](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
[^37]: CUDA Programming Guide: An Overview | by Zia Babar | Aug, 2025 \- Medium, accessed October 7, 2025, [https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8](https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8)
[^38]: Matrix Multiplication in CUDA \- Harshit Kumar, accessed October 7, 2025, [https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda](https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda)
[^39]: What is NVIDIA CUDA? What can it do for Deep Learning? \- Exxact Corporation, accessed October 7, 2025, [https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning](https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning)
[^40]: OpenCL \- The Open Standard for Parallel Programming of ..., accessed October 7, 2025, [https://www.khronos.org/opencl/](https://www.khronos.org/opencl/)
[^41]: An introduction to OpenCL \- Purdue Engineering, accessed October 7, 2025, [https://engineering.purdue.edu/\~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf](https://engineering.purdue.edu/~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf)
[^42]: OpenCL \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/OpenCL](https://en.wikipedia.org/wiki/OpenCL)
[^43]: OpenCL execution model \- Arm Developer, accessed October 7, 2025, [https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model](https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model)
[^44]: The OpenCL™ C Specification, accessed October 7, 2025, [https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL\_C.pdf](https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL_C.pdf)
[^45]: The OpenCL Specification \- IC-Unicamp, accessed October 7, 2025, [https://www.ic.unicamp.br/\~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf](https://www.ic.unicamp.br/~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf)
[^46]: Heterogeneous programming with SYCL documentation, accessed October 7, 2025, [https://enccs.github.io/sycl-workshop/](https://enccs.github.io/sycl-workshop/)
[^47]: Getting Started \- SYCL.tech, accessed October 7, 2025, [https://sycl.tech/getting-started/](https://sycl.tech/getting-started/)
[^48]: Compiler for Parallel Machines. Parallel computing is an approach ..., accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf)
[^49]: The Nexus Task-parallel Runtime System, accessed October 7, 2025, [https://marketing.globuscs.info/production/strapi/uploads/india\_paper\_ps\_40c2982c84.pdf](https://marketing.globuscs.info/production/strapi/uploads/india_paper_ps_40c2982c84.pdf)
[^50]: medium.com, accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf\#:\~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf#:~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.)
[^51]: en.wikipedia.org, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Automatic\_parallelization\_tool\#:\~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization\&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.](https://en.wikipedia.org/wiki/Automatic_parallelization_tool#:~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.)
[^52]: Auto-parallelization Overview, accessed October 7, 2025, [https://www.cita.utoronto.ca/\~merz/intel\_c10b/main\_cls/mergedProjects/optaps\_cls/common/optaps\_qpar\_par.htm](https://www.cita.utoronto.ca/~merz/intel_c10b/main_cls/mergedProjects/optaps_cls/common/optaps_qpar_par.htm)
[^53]: Compilers for Parallel Machines: A User-Friendly Guide | by Pranay Junghare | Medium, accessed October 7, 2025, [https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6](https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6)
[^54]: Pragma directives for OpenMP parallelization \- IBM, accessed October 7, 2025, [https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization)
[^55]: Directive Format \- OpenMP, accessed October 7, 2025, [https://www.openmp.org/spec-html/5.1/openmpse9.html](https://www.openmp.org/spec-html/5.1/openmpse9.html)
[^56]: What is parallel computing? \- IBM, accessed October 7, 2025, [https://www.ibm.com/think/topics/parallel-computing](https://www.ibm.com/think/topics/parallel-computing)
[^57]: Parallel Programming Models, Languages and Compilers \- csenotes, accessed October 7, 2025, [https://csenotes.github.io/pdf/mod5\_aca.pdf](https://csenotes.github.io/pdf/mod5_aca.pdf)
[^58]: (PDF) A Comparative Study and Evaluation of Parallel Programming ..., accessed October 7, 2025, [https://www.researchgate.net/publication/255791855\_A\_Comparative\_Study\_and\_Evaluation\_of\_Parallel\_Programming\_Models\_for\_Shared-Memory\_Parallel\_Architectures](https://www.researchgate.net/publication/255791855_A_Comparative_Study_and_Evaluation_of_Parallel_Programming_Models_for_Shared-Memory_Parallel_Architectures)
[^59]: Scheduling Multithreaded Computations by Work Stealing, accessed October 7, 2025, [https://www.csd.uwo.ca/\~mmorenom/CS433-CS9624/Resources/Scheduling\_multithreaded\_computations\_by\_work\_stealing.pdf](https://www.csd.uwo.ca/~mmorenom/CS433-CS9624/Resources/Scheduling_multithreaded_computations_by_work_stealing.pdf)
[^60]: Parallel Programming With OpenMP In C++ \- Matrix Multiplication ..., accessed October 7, 2025, [https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/](https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/)
[^61]: 5.2 Matrix Multiplication — Parallel Computing for Beginners \- Free Hands-On Materials for Learning PDC, accessed October 7, 2025, [https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html](https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html)
[^62]: Parallelized Blocked Matrix Multiplication using OpenMP | by Charith Pietersz \- Medium, accessed October 7, 2025, [https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47](https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47)
[^63]: OpenMP and cores/threads \- c++ \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/9292191/openmp-and-cores-threads](https://stackoverflow.com/questions/9292191/openmp-and-cores-threads)
[^64]: OpenMP thread mapping to physical cores \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores](https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores)
[^65]: Processor Affinity for OpenMP and MPI » ADMIN Magazine, accessed October 7, 2025, [https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI](https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI)
[^66]: How do I control threads and CPUs on Ookami using OpenMP? \- Stony Brook University, accessed October 7, 2025, [https://www.stonybrook.edu/commcms/ookami/support/faq/thread\_binding](https://www.stonybrook.edu/commcms/ookami/support/faq/thread_binding)
[^67]: Multicore processors and cache coherence | Intro to Computer Architecture Class Notes, accessed October 7, 2025, [https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH](https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH)
[^68]: Cache Coherence: How the MESI Protocol Keeps Multi-Core CPUs ..., accessed October 7, 2025, [https://dev.to/sachin\_tolay\_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j](https://dev.to/sachin_tolay_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j)
[^69]: Non-uniform memory access \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Non-uniform\_memory\_access](https://en.wikipedia.org/wiki/Non-uniform_memory_access)
[^70]: Improving Parallel System Performance with a NUMA-aware Load Balancer \- IDEALS, accessed October 7, 2025, [https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1](https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1)
[^71]: Mysteries of NUMA Memory Management Revealed \- Red Hat, accessed October 7, 2025, [https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed](https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed)
[^72]: CUDA C++ Programming Guide \- NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#hardware-implementation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)
[^73]: Matrix Multiplication in CUDA — A Simple Guide | by Charitha Saumya | Analytics Vidhya, accessed October 7, 2025, [https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab](https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab)
[^74]: MatrixMul sample \- CUDA Programming and Performance \- NVIDIA Developer Forums, accessed October 7, 2025, [https://forums.developer.nvidia.com/t/matrixmul-sample/58025](https://forums.developer.nvidia.com/t/matrixmul-sample/58025)