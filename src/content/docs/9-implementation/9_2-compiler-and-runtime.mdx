---
title: "9.2 The Role of the Compiler and Runtime"
description: "How compilers and runtime systems enable efficient parallel execution and optimization."
---
A parallel programming model, in isolation, is merely a specification—a set of rules and promises about how a programmer can express concurrency. The entities that transform this specification into a running parallel program are the **compiler** and the **runtime system**. These two components form a critical bridge between the programmer's high-level intent and the hardware's low-level capabilities. They can be viewed as partners that fulfill an "interface contract" defined by the programming model. The programmer writes code according to the model's rules, and in return, the compiler and runtime are responsible for generating and managing an efficient parallel execution. The sophistication of these tools directly determines the performance and scalability of the final application, and there is a clear historical trend of shifting the burden of managing parallel complexity from the programmer to these automated systems.[^48]

### **9.2.1 The Parallelizing Compiler: From Source Code to Parallel Instructions**

The compiler is the first agent in the process of realizing parallelism. Its primary role is to analyze the source code and transform it into an executable form that leverages the target hardware's parallel features.[^48] This process involves several key stages.

#### **Analysis: The Foundation of Parallelization**

Before any parallelization can occur, the compiler must first understand the program's structure and dependencies. The most critical step in this phase is **data dependency analysis**.[^48] The compiler meticulously examines the code to determine which operations are independent and can therefore be executed in any order or simultaneously, and which are dependent and must be executed in a specific sequence. For example, in a loop where each iteration calculates a value A\[i\] based only on B\[i\], the iterations are independent. However, if the calculation for A\[i\] depends on the value of A\[i-1\], a *loop-carried dependency* exists, which prevents straightforward parallelization. This analysis is the bedrock upon which all subsequent parallel transformations are built.[^48]

#### **Transformation: Creating Parallelism**

Once the compiler has identified opportunities for parallelism, it can transform the code to exploit them. This can happen in two primary ways: automatically or in response to programmer directives.

*   **Automatic Parallelization:** Many modern compilers, such as the Intel C++ Compiler, feature an "auto-parallelizer".[^51] When enabled, this feature attempts to automatically convert serial code into parallel code without any explicit guidance from the programmer.[^52] The compiler uses the results of its dependency analysis to identify loops that are safe to parallelize ("good worksharing candidates") and then generates the necessary multi-threaded code, effectively inserting the equivalent of OpenMP directives on the programmer's behalf.[^51] While powerful, automatic parallelization is typically effective only for simple, regularly structured loops with no complex dependencies.[^1]
*   **Vectorization (SIMD):** A related transformation is auto-vectorization, where the compiler identifies opportunities to use Single Instruction, Multiple Data (SIMD) instructions available in modern CPUs.[^48] Instead of a loop that adds two arrays element by element (a scalar operation), the compiler generates vector instructions that can add multiple elements (e.g., 4, 8, or 16\) in a single clock cycle. This exploits data-level parallelism at the instruction level, within a single core.[^12]

#### **Code Generation: Implementing Programmer Directives**

When a programmer uses an explicit parallel programming model like OpenMP, the compiler's role shifts from discovery to implementation. When the compiler encounters an OpenMP pragma, such as \#pragma omp parallel for, it recognizes this as a direct command.[^16] It then performs a series of code generation steps:

1.  It encapsulates the body of the loop into a separate function.
2.  It generates code that makes a call to the OpenMP runtime library. This call instructs the runtime to create a team of threads.
3.  It generates logic for each thread to calculate its assigned portion of the loop's iteration space.
4.  It handles the data environment according to the specified clauses (e.g., creating private copies of variables for each thread, setting up shared variables).[^16]

In this mode, the compiler acts as a translator, converting the high-level, abstract directive from the programmer into the low-level function calls and data structures that the runtime system understands and can act upon.[^55]

### **9.2.2 The Parallel Runtime: Managing Execution**

While the compiler prepares the code for parallel execution, the **runtime system** is the entity that manages the resources and orchestrates the process when the program is actually running.[^49] It is the dynamic, operational side of the parallel implementation.

#### **Resource Management and Abstraction**

The runtime system provides the fundamental abstractions of the parallel machine that the compiler targets.[^49] These abstractions typically include:

*   **Nodes:** Representations of physical processing resources, like a multi-core CPU or a node in a cluster.[^49]
*   **Contexts:** Address spaces in which computations execute.[^49]
*   **Threads:** The fundamental units of execution that perform the computational work.[^49]

The runtime is responsible for the entire lifecycle of these resources: creating threads or processes at the start of a parallel region, managing their state during execution, and destroying them when the parallel work is complete.[^56]

#### **Scheduling: Assigning Work to Processors**

One of the most critical functions of a parallel runtime is **scheduling**—the process of assigning units of work (tasks, loop iterations) to available threads or processors.[^57] The goal of a scheduler is to maximize processor utilization and minimize execution time, a task complicated by factors like varying task lengths and data locality.

*   **Static vs. Dynamic Scheduling:** Schedulers can operate statically or dynamically.[^57] In **static scheduling**, the work distribution is determined at compile-time. For example, a loop of 100 iterations on 4 threads might be scheduled by giving iterations 0-24 to the first thread, 25-49 to the second, and so on. This approach has very low runtime overhead but can perform poorly if the work per iteration is not uniform, leading to **load imbalance** where some threads finish early and sit idle while others are still working.[^58] **Dynamic scheduling** addresses this by assigning work at runtime. A common strategy is to use a central work queue; when a thread finishes a chunk of work, it requests another from the queue. This adapts to varying workloads but introduces contention and overhead for accessing the shared queue.[^18]
*   **Work-Stealing Schedulers:** A more advanced and highly effective dynamic scheduling strategy is **work-stealing**, which is central to runtimes like TBB and Cilk.[^24] In this decentralized model, each thread maintains its own local work queue, typically implemented as a double-ended queue (deque).[^29] A thread adds new tasks to and takes its own work from one end of its deque (e.g., the bottom). When a thread becomes idle, it turns into a "thief" and attempts to "steal" a task from the other end (e.g., the top) of a randomly chosen "victim" thread's deque.[^29] This strategy has several advantages: threads primarily operate on their local queues, minimizing contention; stealing only happens when necessary, reducing overhead; and stealing the oldest task (from the top of the deque) is more likely to yield a large chunk of work, further improving efficiency.[^29]

#### **Synchronization and Communication**

Finally, the runtime provides the concrete implementations of the synchronization and communication primitives exposed by the programming model. When a programmer calls MPI\_Send, it is the MPI runtime library that handles the low-level details of packetizing the data, interacting with the network interface card, and transmitting the message. Similarly, when an OpenMP program encounters a \#pragma omp barrier, the OpenMP runtime implements the logic that forces each thread to wait until all other threads in its team have reached that point.[^16]

### **9.2.3 The Implicit Model: The Ultimate Symbiosis**

The relationship between the compiler and runtime is most tightly coupled in **implicit parallelism**.[^6] In this paradigm, the programmer uses a domain-specific or functional language (like MATLAB or Haskell) and writes code that contains no explicit parallel directives or function calls.[^4] The responsibility for identifying and exploiting parallelism falls entirely on the compiler and runtime system.[^6]
For example, when a MATLAB user writes C \= A \* B for two large matrices, the MATLAB runtime can automatically interpret this high-level operation and dispatch it to a highly optimized, multi-threaded library function (like Intel MKL or OpenBLAS) without the user ever writing a parallel construct.[^53] In a functional language, the absence of side effects makes it mathematically straightforward for a compiler to prove that two function calls are independent and can be scheduled for concurrent execution.[^6] This model represents the ultimate fulfillment of the trend toward shifting the burden of parallelism away from the programmer and into the underlying tools, where sophisticated analysis and optimization can be applied automatically.

## References
[^1]: Models for Parallel Computing : Review and Perspectives \- Semantic Scholar, accessed October 7, 2025, [https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29](https://www.semanticscholar.org/paper/Models-for-Parallel-Computing-%3A-Review-and-Kessler-Keller/c924481fbb05bb807920c8f3f2f4d9234c9f1c29)
[^2]: Shared Versus Distributed Memory Multiprocessors \- ECMWF, accessed October 7, 2025, [https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf](https://www.ecmwf.int/sites/default/files/elibrary/1990/10302-shared-versus-distributed-memory-multiprocessors.pdf)
[^3]: Distributed memory \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Distributed\_memory](https://en.wikipedia.org/wiki/Distributed_memory)
[^4]: Insights on Parallel Programming Model \- Advanced Millennium Technologies, accessed October 7, 2025, [https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/](https://blog.amt.in/index.php/2023/01/17/insights-on-parallel-programming-model/)
[^5]: How Parallel Processing Shaped Modern Computing \- CelerData, accessed October 7, 2025, [https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing](https://celerdata.com/glossary/how-parallel-processing-shaped-modern-computing)
[^6]: Parallel programming model \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_programming\_model](https://en.wikipedia.org/wiki/Parallel_programming_model)
[^7]: The Evolution of Parallel Programming | by Tiwariabhinav | Medium, accessed October 7, 2025, [https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88](https://medium.com/@tiwariabhinav424/the-evolution-of-parallel-programming-d80665066b88)
[^8]: Introduction to Parallel Computing Tutorial \- | HPC @ LLNL, accessed October 7, 2025, [https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial)
[^9]: Multithreaded Programming (POSIX pthreads Tutorial) \- randu.org, accessed October 7, 2025, [https://randu.org/tutorials/threads/](https://randu.org/tutorials/threads/)
[^10]: Parallel Computing at a Glance, accessed October 7, 2025, [http://www.buyya.com/microkernel/chap1.pdf](http://www.buyya.com/microkernel/chap1.pdf)
[^11]: Data Parallel, Task Parallel, and Agent Actor Architectures \- bytewax, accessed October 7, 2025, [https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures](https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures)
[^12]: Scalable Parallel Processing: Architectural Models, Real-Time Programming, and Performance Evaluation \- MDPI, accessed October 7, 2025, [https://www.mdpi.com/2673-4591/104/1/60](https://www.mdpi.com/2673-4591/104/1/60)
[^13]: Task parallelism \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Task\_parallelism](https://en.wikipedia.org/wiki/Task_parallelism)
[^14]: Parallel computing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Parallel\_computing](https://en.wikipedia.org/wiki/Parallel_computing)
[^15]: What is a good example for task parallelismn in data processing? \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing](https://stackoverflow.com/questions/54072224/what-is-a-good-example-for-task-parallelismn-in-data-processing)
[^16]: Using OpenMP with C — Research Computing University of ..., accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html](https://curc.readthedocs.io/en/latest/programming/OpenMP-C.html)
[^17]: C++ Examples of Parallel Programming with OpenMP, accessed October 7, 2025, [https://people.math.sc.edu/burkardt/cpp\_src/openmp/openmp.html](https://people.math.sc.edu/burkardt/cpp_src/openmp/openmp.html)
[^18]: An introduction to OpenMP \- University College London, accessed October 7, 2025, [https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02\_intro\_openmp.html](https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02_intro_openmp.html)
[^19]: OpenMP Tutorial \- Scalable Parallel Computing Lab, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf](https://spcl.inf.ethz.ch/Teaching/2014-dphpc/assignments/openmp-tutorial.pdf)
[^20]: C++ Tutorial: Multi-Threaded Programming \- C++ Class Thread for Pthreads \- 2020 \- BogoToBogo, accessed October 7, 2025, [https://www.bogotobogo.com/cplusplus/multithreading\_pthread.php](https://www.bogotobogo.com/cplusplus/multithreading_pthread.php)
[^21]: Linux Tutorial: POSIX Threads, accessed October 7, 2025, [https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html](https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html)
[^22]: Getting Started with Intel® Threading Building Blocks (Intel® TBB), accessed October 7, 2025, [https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html](https://www.intel.com/content/www/us/en/developer/articles/guide/get-started-with-tbb.html)
[^23]: Threading Building Blocks \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Threading\_Building\_Blocks](https://en.wikipedia.org/wiki/Threading_Building_Blocks)
[^24]: Introduction to the Intel Threading Building Blocks — mcs572 0.7.8 documentation, accessed October 7, 2025, [http://homepages.math.uic.edu/\~jan/mcs572f16/mcs572notes/lec11.html](http://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec11.html)
[^25]: Threading Building Blocks \- Tools \- EBRAINS, accessed October 7, 2025, [https://www.ebrains.eu/tools/threading-building-blocks](https://www.ebrains.eu/tools/threading-building-blocks)
[^26]: TBB Tutorial \- cs.wisc.edu, accessed October 7, 2025, [https://pages.cs.wisc.edu/\~gibson/tbbTutorial.html](https://pages.cs.wisc.edu/~gibson/tbbTutorial.html)
[^27]: Introducing Intel Tbb \- Agenda INFN, accessed October 7, 2025, [https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb\_Introduction.pdf](https://agenda.infn.it/event/4107/contributions/50346/attachments/35473/41878/Tbb_Introduction.pdf)
[^28]: Intel(R) Threading Building Blocks Getting Started Guide, accessed October 7, 2025, [http://www.physics.ntua.gr/\~konstant/HetCluster/intel12.1/tbb/Getting\_Started.pdf](http://www.physics.ntua.gr/~konstant/HetCluster/intel12.1/tbb/Getting_Started.pdf)
[^29]: Work stealing \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Work\_stealing](https://en.wikipedia.org/wiki/Work_stealing)
[^30]: Towards Efficient HPC: Exploring Overlap Strategies Using MPI Non ..., accessed October 7, 2025, [https://www.mdpi.com/2227-7390/13/11/1848](https://www.mdpi.com/2227-7390/13/11/1848)
[^31]: Tutorials · MPI Tutorial, accessed October 7, 2025, [https://mpitutorial.com/tutorials/](https://mpitutorial.com/tutorials/)
[^32]: MPI Tutorial – Part 1, accessed October 7, 2025, [https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf](https://spcl.inf.ethz.ch/Teaching/2017-dphpc/recitation/mpi1.pdf)
[^33]: Using MPI with C \- CU Research Computing User Guide, accessed October 7, 2025, [https://curc.readthedocs.io/en/latest/programming/MPI-C.html](https://curc.readthedocs.io/en/latest/programming/MPI-C.html)
[^34]: (PDF) Review of Architecture and Model for Parallel Programming \- ResearchGate, accessed October 7, 2025, [https://www.researchgate.net/publication/372822306\_Review\_of\_Architecture\_and\_Model\_for\_Parallel\_Programming](https://www.researchgate.net/publication/372822306_Review_of_Architecture_and_Model_for_Parallel_Programming)
[^35]: mikeroyal/CUDA-Guide \- GitHub, accessed October 7, 2025, [https://github.com/mikeroyal/CUDA-Guide](https://github.com/mikeroyal/CUDA-Guide)
[^36]: CUDA C++ Programming Guide | NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/pdf/CUDA\_C\_Programming\_Guide.pdf](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
[^37]: CUDA Programming Guide: An Overview | by Zia Babar | Aug, 2025 \- Medium, accessed October 7, 2025, [https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8](https://medium.com/@zbabar/cuda-programming-guide-an-overview-84be487cb5a8)
[^38]: Matrix Multiplication in CUDA \- Harshit Kumar, accessed October 7, 2025, [https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda](https://kharshit.github.io/blog/2024/06/07/matrix-multiplication-cuda)
[^39]: What is NVIDIA CUDA? What can it do for Deep Learning? \- Exxact Corporation, accessed October 7, 2025, [https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning](https://www.exxactcorp.com/blog/Deep-Learning/NVIDIA-CUDA-in-AI-Deep-Learning)
[^40]: OpenCL \- The Open Standard for Parallel Programming of ..., accessed October 7, 2025, [https://www.khronos.org/opencl/](https://www.khronos.org/opencl/)
[^41]: An introduction to OpenCL \- Purdue Engineering, accessed October 7, 2025, [https://engineering.purdue.edu/\~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf](https://engineering.purdue.edu/~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod20OpenCL/3rd-Edition-AppendixA-intro-to-OpenCL.pdf)
[^42]: OpenCL \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/OpenCL](https://en.wikipedia.org/wiki/OpenCL)
[^43]: OpenCL execution model \- Arm Developer, accessed October 7, 2025, [https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model](https://developer.arm.com/documentation/dui0538/e/opencl-concepts/opencl-execution-model)
[^44]: The OpenCL™ C Specification, accessed October 7, 2025, [https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL\_C.pdf](https://bashbaug.github.io/OpenCL-Docs/pdf/OpenCL_C.pdf)
[^45]: The OpenCL Specification \- IC-Unicamp, accessed October 7, 2025, [https://www.ic.unicamp.br/\~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf](https://www.ic.unicamp.br/~edson/disciplinas/mo802/2016-1s/anexos/opencl-1.2.pdf)
[^46]: Heterogeneous programming with SYCL documentation, accessed October 7, 2025, [https://enccs.github.io/sycl-workshop/](https://enccs.github.io/sycl-workshop/)
[^47]: Getting Started \- SYCL.tech, accessed October 7, 2025, [https://sycl.tech/getting-started/](https://sycl.tech/getting-started/)
[^48]: Compiler for Parallel Machines. Parallel computing is an approach ..., accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf)
[^49]: The Nexus Task-parallel Runtime System, accessed October 7, 2025, [https://marketing.globuscs.info/production/strapi/uploads/india\_paper\_ps\_40c2982c84.pdf](https://marketing.globuscs.info/production/strapi/uploads/india_paper_ps_40c2982c84.pdf)
[^50]: medium.com, accessed October 7, 2025, [https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf\#:\~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.](https://medium.com/@omkar.patil20/compiler-for-parallel-machines-9df80e04d6bf#:~:text=Compilers%20play%20a%20crucial%20role,capabilities%20of%20the%20target%20machine.)
[^51]: en.wikipedia.org, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Automatic\_parallelization\_tool\#:\~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization\&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.](https://en.wikipedia.org/wiki/Automatic_parallelization_tool#:~:text=Intel%20C%2B%2B%20compiler,-The%20auto%2Dparallelization&text=Automatic%20parallelization%20determines%20the%20loops,in%20programming%20with%20OpenMP%20directives.)
[^52]: Auto-parallelization Overview, accessed October 7, 2025, [https://www.cita.utoronto.ca/\~merz/intel\_c10b/main\_cls/mergedProjects/optaps\_cls/common/optaps\_qpar\_par.htm](https://www.cita.utoronto.ca/~merz/intel_c10b/main_cls/mergedProjects/optaps_cls/common/optaps_qpar_par.htm)
[^53]: Compilers for Parallel Machines: A User-Friendly Guide | by Pranay Junghare | Medium, accessed October 7, 2025, [https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6](https://medium.com/@jungharepranay1509/compilers-for-parallel-machines-a-user-friendly-guide-0dd45ca6a9f6)
[^54]: Pragma directives for OpenMP parallelization \- IBM, accessed October 7, 2025, [https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization)
[^55]: Directive Format \- OpenMP, accessed October 7, 2025, [https://www.openmp.org/spec-html/5.1/openmpse9.html](https://www.openmp.org/spec-html/5.1/openmpse9.html)
[^56]: What is parallel computing? \- IBM, accessed October 7, 2025, [https://www.ibm.com/think/topics/parallel-computing](https://www.ibm.com/think/topics/parallel-computing)
[^57]: Parallel Programming Models, Languages and Compilers \- csenotes, accessed October 7, 2025, [https://csenotes.github.io/pdf/mod5\_aca.pdf](https://csenotes.github.io/pdf/mod5_aca.pdf)
[^58]: (PDF) A Comparative Study and Evaluation of Parallel Programming ..., accessed October 7, 2025, [https://www.researchgate.net/publication/255791855\_A\_Comparative\_Study\_and\_Evaluation\_of\_Parallel\_Programming\_Models\_for\_Shared-Memory\_Parallel\_Architectures](https://www.researchgate.net/publication/255791855_A_Comparative_Study_and_Evaluation_of_Parallel_Programming_Models_for_Shared-Memory_Parallel_Architectures)
[^59]: Scheduling Multithreaded Computations by Work Stealing, accessed October 7, 2025, [https://www.csd.uwo.ca/\~mmorenom/CS433-CS9624/Resources/Scheduling\_multithreaded\_computations\_by\_work\_stealing.pdf](https://www.csd.uwo.ca/~mmorenom/CS433-CS9624/Resources/Scheduling_multithreaded_computations_by_work_stealing.pdf)
[^60]: Parallel Programming With OpenMP In C++ \- Matrix Multiplication ..., accessed October 7, 2025, [https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/](https://www.c-sharpcorner.com/article/parallel-programming-with-openmp-in-cpp-matrix-multiplication-example/)
[^61]: 5.2 Matrix Multiplication — Parallel Computing for Beginners \- Free Hands-On Materials for Learning PDC, accessed October 7, 2025, [https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html](https://www.learnpdc.org/PDCBeginners/5-applications/matrix-multiply.html)
[^62]: Parallelized Blocked Matrix Multiplication using OpenMP | by Charith Pietersz \- Medium, accessed October 7, 2025, [https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47](https://medium.com/@cj.ptsz/parallelized-blocked-matrix-multiplication-using-openmp-97a4bc620a47)
[^63]: OpenMP and cores/threads \- c++ \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/9292191/openmp-and-cores-threads](https://stackoverflow.com/questions/9292191/openmp-and-cores-threads)
[^64]: OpenMP thread mapping to physical cores \- Stack Overflow, accessed October 7, 2025, [https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores](https://stackoverflow.com/questions/4717251/openmp-thread-mapping-to-physical-cores)
[^65]: Processor Affinity for OpenMP and MPI » ADMIN Magazine, accessed October 7, 2025, [https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI](https://www.admin-magazine.com/HPC/Articles/Processor-Affinity-for-OpenMP-and-MPI)
[^66]: How do I control threads and CPUs on Ookami using OpenMP? \- Stony Brook University, accessed October 7, 2025, [https://www.stonybrook.edu/commcms/ookami/support/faq/thread\_binding](https://www.stonybrook.edu/commcms/ookami/support/faq/thread_binding)
[^67]: Multicore processors and cache coherence | Intro to Computer Architecture Class Notes, accessed October 7, 2025, [https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH](https://fiveable.me/introduction-computer-architecture/unit-7/multicore-processors-cache-coherence/study-guide/0Fr1h83bSS0NcUMH)
[^68]: Cache Coherence: How the MESI Protocol Keeps Multi-Core CPUs ..., accessed October 7, 2025, [https://dev.to/sachin\_tolay\_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j](https://dev.to/sachin_tolay_052a7e539e57/cache-coherence-how-the-mesi-protocol-keeps-multi-core-cpus-consistent-170j)
[^69]: Non-uniform memory access \- Wikipedia, accessed October 7, 2025, [https://en.wikipedia.org/wiki/Non-uniform\_memory\_access](https://en.wikipedia.org/wiki/Non-uniform_memory_access)
[^70]: Improving Parallel System Performance with a NUMA-aware Load Balancer \- IDEALS, accessed October 7, 2025, [https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1](https://www.ideals.illinois.edu/items/26080/bitstreams/89289/object?dl=1)
[^71]: Mysteries of NUMA Memory Management Revealed \- Red Hat, accessed October 7, 2025, [https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed](https://www.redhat.com/en/blog/mysteries-numa-memory-management-revealed)
[^72]: CUDA C++ Programming Guide \- NVIDIA Docs, accessed October 7, 2025, [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#hardware-implementation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)
[^73]: Matrix Multiplication in CUDA — A Simple Guide | by Charitha Saumya | Analytics Vidhya, accessed October 7, 2025, [https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab](https://medium.com/analytics-vidhya/matrix-multiplication-in-cuda-a-simple-guide-bab44bc1f8ab)
[^74]: MatrixMul sample \- CUDA Programming and Performance \- NVIDIA Developer Forums, accessed October 7, 2025, [https://forums.developer.nvidia.com/t/matrixmul-sample/58025](https://forums.developer.nvidia.com/t/matrixmul-sample/58025)